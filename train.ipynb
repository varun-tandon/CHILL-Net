{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from torch_geometric) (2.28.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from torch_geometric) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from torch_geometric) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from torch_geometric) (1.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from torch_geometric) (1.24.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/site-packages (from torch_geometric) (5.9.4)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.14)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models.lightgcn import LightGCN\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use the first 10k rows of the data, set to None to use all data\n",
    "AMOUNT_TO_LOAD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1000/63111 [00:02<02:59, 345.62it/s]\n"
     ]
    }
   ],
   "source": [
    "user_reviews_dir = 'user_reviews'\n",
    "user_review_data = dict()\n",
    "\n",
    "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
    "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
    "        break\n",
    "    try:\n",
    "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Empty file: {filename}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
    "\n",
    "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asel82_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(list(user_review_data.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 884.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove all values with nan in the review column\n",
    "for key in tqdm(user_review_data.keys()):\n",
    "    user_review_data[key] = user_review_data[key].dropna(subset=['movie_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 340.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 396059\n",
      "Validation reviews: 12300\n",
      "Test reviews: 12300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = []\n",
    "validation_reviews = []\n",
    "test_reviews = []\n",
    "for user_id, reviews in tqdm(user_review_data.items()):\n",
    "    if len(reviews) > 50:\n",
    "        validation_review_data_df = reviews.sample(15, replace=False)\n",
    "        validation_review_data = validation_review_data_df.to_dict('records')\n",
    "        for review in validation_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        validation_reviews.extend(validation_review_data)\n",
    "        # remove the validation reviews from the training data\n",
    "        reviews = reviews.drop(validation_review_data_df.index)\n",
    "        test_review_data_df = reviews.sample(15, replace=False)\n",
    "        test_review_data = test_review_data_df.to_dict('records')\n",
    "        for review in test_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        test_reviews.extend(test_review_data)\n",
    "        # remove the test reviews from the training data\n",
    "        reviews = reviews.drop(test_review_data_df.index)\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "    else:\n",
    "        # if the user has less than 5 reviews, we will use all of them for training\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "\n",
    "print(f'Train reviews: {len(train_reviews)}')\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Now that we have the training data, let's construct the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train users: 1000\n",
      "Number of train items: 43225\n",
      "Number of nodes: 45001\n"
     ]
    }
   ],
   "source": [
    "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
    "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
    "num_total_items = len(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews]))\n",
    "num_nodes = num_train_users + num_total_items\n",
    "print(f'Number of train users: {num_train_users}')\n",
    "print(f'Number of train items: {num_train_items}')\n",
    "print(f'Number of nodes: {num_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val_users = len(set([review['user_id'] for review in validation_reviews]))\n",
    "num_val_items = len(set([review['movie_id'] for review in validation_reviews]))\n",
    "num_val_nodes = num_val_users + num_val_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map users to ids\n",
    "movie_id_to_movie_name = dict()\n",
    "for review in train_reviews + validation_reviews + test_reviews:\n",
    "    movie_id_to_movie_name[review['movie_id']] = review['movie_title']\n",
    "\n",
    "user_to_id = dict()\n",
    "for i, user_id in enumerate(set([review['user_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
    "    user_to_id[user_id] = i\n",
    "\n",
    "# Let's map movies to ids\n",
    "movie_to_id = dict()\n",
    "for i, movie_id in enumerate(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
    "    movie_to_id[movie_id] = i + num_train_users\n",
    "\n",
    "# Let's map ids to users\n",
    "id_to_user = dict()\n",
    "for user_id, index in user_to_id.items():\n",
    "    id_to_user[index] = user_id\n",
    "\n",
    "# Let's map ids to movies\n",
    "id_to_movie = dict()\n",
    "for movie_id, index in movie_to_id.items():\n",
    "    id_to_movie[index] = movie_id\n",
    "\n",
    "# Let's map movie names to movie ids\n",
    "movie_name_to_movie_id = dict()\n",
    "for movie_id, movie_name in movie_id_to_movie_name.items():\n",
    "    movie_name_to_movie_id[movie_name] = movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert_review_to_edge(review):\n",
    "    user_id = user_to_id[review['user_id']]\n",
    "    movie_id = movie_to_id[review['movie_id']]\n",
    "    edge_weight = review['movie_rating']\n",
    "    if (edge_weight < 3.5 and edge_weight > 2.5):\n",
    "        return None, None\n",
    "    edge = (user_id, movie_id)\n",
    "    edge_weight = review['movie_rating']\n",
    "    return edge, edge_weight\n",
    "\n",
    "def shuffle_edges_and_edge_weights(edges, edge_weights):\n",
    "    c = list(zip(edges, edge_weights))\n",
    "    random.shuffle(c)\n",
    "    return zip(*c)\n",
    "\n",
    "def convert_reviews_to_edges(reviews):\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    for review in tqdm(reviews):\n",
    "        edge, edge_weight = convert_review_to_edge(review)\n",
    "        if edge is not None:\n",
    "            edges.append(edge)\n",
    "            edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Reformat the edges to be a tensor\n",
    "    edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edges, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 396059/396059 [00:00<00:00, 704428.25it/s]\n",
      "100%|██████████| 12300/12300 [00:00<00:00, 605224.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 318166\n",
      "Validation edges: 10260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the edges between users and movies.\n",
    "# The id of the user will be the index of the user in the user_to_id dict\n",
    "# The id of the movie will be the index of the movie in the movie_to_id dict + the number of users\n",
    "\n",
    "train_edges, train_edge_weights = convert_reviews_to_edges(train_reviews)\n",
    "validation_edges, validation_edge_weights = convert_reviews_to_edges(validation_reviews)\n",
    "\n",
    "print(f'Train edges: {train_edges.shape[1]}')\n",
    "print(f'Validation edges: {validation_edges.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as data\n",
    "\n",
    "# create the graph\n",
    "train_graph = data.Data(\n",
    "    edge_index=train_edges,\n",
    "    edge_attr=torch.tensor(train_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "validation_graph = data.Data(\n",
    "    edge_index=validation_edges,\n",
    "    edge_attr=torch.tensor(validation_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.validate(raise_on_error=True)\n",
    "validation_graph.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some negative edges\n",
    "def resample_edges_for_user(user_positive_edges, user_negative_edges):\n",
    "    num_negative_edges_to_add = user_positive_edges.shape[1] * 3 - user_negative_edges.shape[1]\n",
    "    if (num_negative_edges_to_add <= 0):\n",
    "        num_negative_edges_to_remove = -num_negative_edges_to_add\n",
    "        # choose the negative edges to keep\n",
    "        negative_edges_to_keep = torch.randint(user_negative_edges.shape[1], (user_negative_edges.shape[1] - num_negative_edges_to_remove,))\n",
    "        # remove all the negative edges for this user\n",
    "        user_negative_edges = user_negative_edges[:, negative_edges_to_keep]\n",
    "    else:\n",
    "        # Create new negative edges\n",
    "        negative_edges_to_add = torch.tensor([[user_id] * num_negative_edges_to_add, torch.randint(num_train_users, num_train_items, (num_negative_edges_to_add,))], dtype=torch.long)\n",
    "        # Add the negative edges to the negative edges for this user\n",
    "        user_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
    "    return user_positive_edges, user_negative_edges\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute ndcg\n",
    "def compute_ndcg_at_k(relevances, k=5):\n",
    "    relevances = relevances[:k]\n",
    "    dcg = 0\n",
    "    for i, relevance in enumerate(relevances):\n",
    "        dcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
    "    idcg = 0\n",
    "    for i, relevance in enumerate(sorted(relevances, reverse=True)):\n",
    "        idcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def compute_recall_at_k(validation_graph, model, K):\n",
    "    # get positive edges in validation set\n",
    "    positive_edges = validation_graph.edge_index[:, validation_graph.edge_attr > 3.5]\n",
    "\n",
    "    # map users to positive edges\n",
    "    user_pos_items = get_user_positive_items(positive_edges)\n",
    "\n",
    "    # get users\n",
    "    users = positive_edges[0].unique()\n",
    "\n",
    "    users = users[torch.randint(users.shape[0], (min(200, len(users)),))]\n",
    "    # filter the validation edges to only the users we want to evaluate\n",
    "    user_validation_edges = []\n",
    "    for user in users:\n",
    "        user_validation_edges.append(validation_graph.edge_index[:, validation_graph.edge_index[0] == user])\n",
    "    user_validation_edges = torch.cat(user_validation_edges, dim=1)\n",
    "    print(user_validation_edges.shape)\n",
    "\n",
    "    first_user_id = users[0].item()\n",
    "    user_name = id_to_user[first_user_id]\n",
    "    print(f'User: {user_name}')\n",
    "\n",
    "    # get movies\n",
    "    movie_indices = torch.LongTensor([_ for _ in range(len(users) + 1, validation_graph.num_nodes)]).to(device)\n",
    "\n",
    "    # Get positive items for each user in validation set\n",
    "    truth_items = [set(user_pos_items[user.item()]) for user in users]\n",
    "\n",
    "    first_user_truth_items = truth_items[0]\n",
    "    first_user_truth_items = [id_to_movie[item] for item in first_user_truth_items]\n",
    "    first_user_truth_items = [movie_id_to_movie_name[item] for item in first_user_truth_items]\n",
    "    print(first_user_truth_items)\n",
    "\n",
    "    training_edges = train_graph.edge_index\n",
    "\n",
    "    # Get top-K recommended items for each user in validation set\n",
    "    total_recall = 0\n",
    "    print(\"Computing recommendations for {} users\".format(len(users)))\n",
    "    for user_index, user_id in tqdm(enumerate(users), total=len(users)):\n",
    "        tick = time.time()\n",
    "        all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "        recommendations = model.recommend(all_edges.to(device), src_index=torch.tensor([user_id]), dst_index=torch.tensor([x for x in range(num_train_users + 1, num_train_items)]), k=3 * K)[0]\n",
    "        tock = time.time()\n",
    "        train_edges_for_user = training_edges[:, training_edges[0] == user_id]\n",
    "        # remove all the recommendations that are in the training set\n",
    "        recommendations = recommendations[~torch.isin(recommendations, train_edges_for_user[1])][:K]\n",
    "        if (len(recommendations) < K):\n",
    "            print(\"Not enough recommendations for user {}\".format(user_id))\n",
    "        if (user_id == first_user_id):\n",
    "            first_user_recommended_items = recommendations\n",
    "            first_user_recommended_items = [id_to_movie[item.item()] for item in first_user_recommended_items if item.item() > num_train_users]\n",
    "            first_user_recommended_items = [movie_id_to_movie_name[item] for item in first_user_recommended_items if item in movie_id_to_movie_name]\n",
    "            print(first_user_recommended_items)\n",
    "        # num_intersect = 0\n",
    "        truth_items_for_user = truth_items[user_index]\n",
    "        # for item in recommendations:\n",
    "        #     item = item.item()\n",
    "        #     if item in truth_items_for_user:\n",
    "        #         num_intersect += 1\n",
    "        # print(num_intersect)\n",
    "        num_intersect = len(set([item.item() for item in recommendations]).intersection(truth_items[user_index]))\n",
    "        recall = num_intersect / len(truth_items_for_user)\n",
    "        total_recall += recall\n",
    "    return total_recall / len(users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adapted from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/lightgcn.html\"\"\"\n",
    "class CustomLightGCN(torch.nn.Module):\n",
    "    \"\"\"From the <https://arxiv.org/abs/2002.02126>` paper.\n",
    "\n",
    "    Args:\n",
    "        num_nodes (int): The number of nodes in the graph.\n",
    "        embedding_dim (int): The dimensionality of node embeddings.\n",
    "        num_layers (int): The number of layers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        embedding_dim: int,\n",
    "        num_layers: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "        self.alpha = torch.tensor([1. / (num_layers + 1)] * (num_layers + 1))\n",
    "        self.convs = ModuleList([GATConv(embedding_dim, embedding_dim, heads=8, dropout=0.6) for _ in range(num_layers)])\n",
    "        self.linears = ModuleList([Linear(embedding_dim * 8, embedding_dim) for _ in range(num_layers)])\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def get_embedding(self, edge_index):\n",
    "        x = self.embedding.weight\n",
    "        out = x * self.alpha[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.linears[i](x.view(-1, self.embedding_dim * 8))\n",
    "            out = out + x * self.alpha[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        edge_label_index = edge_index\n",
    "        out = self.get_embedding(edge_index)\n",
    "        user = out[edge_label_index[0]]\n",
    "        movie = out[edge_label_index[1]]\n",
    "        return (user * movie).sum(dim=-1)\n",
    "\n",
    "\n",
    "    def predict_link(self, edge_index, edge_label_index):\n",
    "        \"Predict links between nodes specified in edge_label_index.\"\"\"\n",
    "        pred = self(edge_index, edge_label_index).sigmoid()\n",
    "        return pred.round()\n",
    "\n",
    "\n",
    "    def recommend(self, edge_index, k):\n",
    "        \"\"\"Get top-k recommendations for nodes in src_index.\"\"\"\n",
    "        out_user = self.get_embedding(edge_index)\n",
    "        out_movie = self.get_embedding(edge_index)\n",
    "        pred = out_user @ out_movie.t()\n",
    "        top_index = pred.topk(k, dim=-1).indices\n",
    "        return top_index\n",
    "\n",
    "\n",
    "    def link_pred_loss(self, pred, edge_label):\n",
    "        \"\"\"Computes the model loss for a link prediction using torch.nn.BCEWithLogitsLoss.\n",
    "        \n",
    "        Args:\n",
    "            pred (torch.Tensor): The predictions.\n",
    "            edge_label (torch.Tensor): The ground-truth edge labels.\n",
    "        \"\"\"\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "\n",
    "    def recommendation_loss(self, pos_edge_rank, neg_edge_rank,\n",
    "                            lambda_reg: float = 1e-4):\n",
    "        \"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "        Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "            pos_edge_rank (torch.Tensor): Positive edge rankings.\n",
    "            neg_edge_rank (torch.Tensor): Negative edge rankings.\n",
    "            lambda_reg (int, optional): The L2 regularization strength\n",
    "                of the Bayesian Personalized Ranking (BPR) loss.\n",
    "        \"\"\"\n",
    "        loss_fn = BPRLoss(lambda_reg)\n",
    "        return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is verbatim from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/lightgcn.html. \"\"\"\n",
    "class BPRLoss(_Loss):\n",
    "    \"\"\"The Bayesian Personalized Ranking (BPR) loss.\"\"\"\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = 0\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        \"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).mean()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_hard_negative_edges_for_user(user_positive_edges, user_negative_edges, model, num_train_items, epoch):\n",
    "    # Select hard negative edges based on current model parameters\n",
    "    user_positive_items = user_positive_edges[1, :]\n",
    "    \n",
    "    # get the rankings for this user\n",
    "    all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "    with torch.no_grad():\n",
    "        user_rankings = model.forward(all_edges) # this is of shape (42263) -- each index is the prediction for that index's movie\n",
    "    mask = torch.ones(num_train_items - num_train_users, dtype=torch.bool) # gets indices of all the movies\n",
    "    pos_items_mask = user_positive_items < num_train_items - num_train_users\n",
    "    filtered_pos_items = user_positive_items[pos_items_mask]\n",
    "    mask[filtered_pos_items] = True\n",
    "\n",
    "    # get the rankings for negative items\n",
    "    negative_rankings = user_rankings[mask]\n",
    "    \n",
    "    # select the top k negative items for this user\n",
    "    if epoch != 0:\n",
    "        k = min(epoch - 1, negative_rankings.shape[0])\n",
    "    else:\n",
    "        k = 0\n",
    "\n",
    "    _, topk_items = torch.topk(negative_rankings, k)\n",
    "    negative_items = torch.nonzero(mask).flatten()[topk_items]\n",
    "\n",
    "    # create the new negative edges\n",
    "    negative_edges_to_add = torch.tensor([[user_id] * k, negative_items], dtype=torch.long)\n",
    "    new_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
    "\n",
    "    return user_positive_edges, new_negative_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m     user_negative_edges \u001b[39m=\u001b[39m user_negative_edges[:, :\u001b[39m15000\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[39m# sample hard negative edges\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m user_positive_edges, user_negative_edges \u001b[39m=\u001b[39m resample_hard_negative_edges_for_user(user_positive_edges, user_negative_edges, model, num_train_items, epoch)\n\u001b[1;32m     52\u001b[0m \u001b[39m# resample negative edges to make sure we have enough\u001b[39;00m\n\u001b[1;32m     53\u001b[0m user_positive_edges, user_negative_edges \u001b[39m=\u001b[39m resample_edges_for_user(user_positive_edges, user_negative_edges)\n",
      "Cell \u001b[0;32mIn[165], line 7\u001b[0m, in \u001b[0;36mresample_hard_negative_edges_for_user\u001b[0;34m(user_positive_edges, user_negative_edges, model, num_train_items, epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m user_positive_items \u001b[39m=\u001b[39m user_positive_edges[\u001b[39m1\u001b[39m, :]\n\u001b[1;32m      6\u001b[0m \u001b[39m# get the rankings for this user\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m all_edges \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([(user_id, item_id) \u001b[39mfor\u001b[39;49;00m item_id \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(num_train_users, num_train_items)], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong)\u001b[39m.\u001b[39mt()\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     user_rankings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(all_edges) \u001b[39m# this is of shape (42263) -- each index is the prediction for that index's movie\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "LR = 5e-5\n",
    "BATCH_SIZE = min(128, len(user_review_data))\n",
    "EMBEDDING_DIM = 64\n",
    "K = 10\n",
    "model = LightGCN(num_nodes=num_nodes, embedding_dim=EMBEDDING_DIM, num_layers=NUM_LAYERS)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Running on device: {}\".format(device))\n",
    "print(EMBEDDING_DIM)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[100, 200, 300, 400], gamma=0.5)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=100)\n",
    "\n",
    "train_positive_edges = train_graph.edge_index[:, train_graph.edge_attr >= 3.5]\n",
    "train_negative_edges = train_graph.edge_index[:, train_graph.edge_attr <= 2.5]\n",
    "\n",
    "validation_df = pd.DataFrame.from_dict(validation_reviews)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment=f'LightGCN_{EMBEDDING_DIM}_layers_{NUM_LAYERS}_batch_size_{BATCH_SIZE}_lr_{LR}_num_train_users_{num_train_users}_num_train_items_{num_train_items}_recall_{K}')\n",
    "\n",
    "for epoch in range(10):\n",
    "    # we are using BPR so we go by user\n",
    "    average_loss = 0\n",
    "    # We'll proceed in batches of users\n",
    "    for start_idx in tqdm(range(0, num_train_users, BATCH_SIZE)):\n",
    "        model.train()\n",
    "        loss = torch.tensor(0.0, requires_grad=True)\n",
    "        # randomly select a batch of users\n",
    "        users_in_batch = torch.randperm(num_train_users)[start_idx:start_idx + BATCH_SIZE]\n",
    "        for user_id in users_in_batch:\n",
    "            # get all the edges specific to this user\n",
    "            user_positive_edges = train_positive_edges[:, train_positive_edges[0] == user_id]\n",
    "            user_negative_edges = train_negative_edges[:, train_negative_edges[0] == user_id]\n",
    "            if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
    "                continue\n",
    "            # limit the number of positive edges to 5000\n",
    "            if (user_positive_edges.shape[1] > 5000):\n",
    "                user_positive_edges = user_positive_edges[:, :5000]\n",
    "            # Get at most 15000 negative edges\n",
    "            if (user_negative_edges.shape[1] > 15000):\n",
    "                user_negative_edges = user_negative_edges[:, :15000]\n",
    "            # sample hard negative edges\n",
    "            user_positive_edges, user_negative_edges = resample_hard_negative_edges_for_user(user_positive_edges, user_negative_edges, model, num_train_items, epoch)\n",
    "            # resample negative edges to make sure we have enough\n",
    "            user_positive_edges, user_negative_edges = resample_edges_for_user(user_positive_edges, user_negative_edges)\n",
    "            # concatenate the positive and negative edges\n",
    "            user_edges = torch.cat([user_positive_edges, user_negative_edges], dim=1)\n",
    "            # get the rankings for this user\n",
    "            user_edges = user_edges.to(device)\n",
    "            user_rankings = model(user_edges)\n",
    "            # divide the rankings into positive and negative rankings\n",
    "            user_positive_rankings = user_rankings[:user_positive_edges.shape[1]]\n",
    "            user_negative_rankings = user_rankings[user_positive_edges.shape[1]:]\n",
    "            # create all pairs of positive and negative rankings\n",
    "            user_positive_rankings = user_positive_rankings.unsqueeze(1).repeat(1, user_negative_rankings.shape[0])\n",
    "            user_negative_rankings = user_negative_rankings.unsqueeze(0).repeat(user_positive_rankings.shape[0], 1)\n",
    "            # get the user loss\n",
    "            user_loss = model.recommendation_loss(user_positive_rankings, user_negative_rankings, 1e-4)\n",
    "            # add the user loss to the total loss\n",
    "            loss = loss + user_loss\n",
    "        # divide the loss by the number of users\n",
    "        loss = loss / BATCH_SIZE\n",
    "        # log the loss\n",
    "        # backprop\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "        print(epoch * BATCH_SIZE + start_idx // BATCH_SIZE)\n",
    "        average_loss = 0\n",
    "        if (epoch * BATCH_SIZE + start_idx // BATCH_SIZE) % 100 == 0:\n",
    "            # evaluate the model\n",
    "            model.eval()\n",
    "            # iterate over all users in the validation set\n",
    "            validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "            # randomly select 1000 of the users\n",
    "            validation_users = random.sample(validation_users, min(len(validation_users), 500))\n",
    "            mean_ndcg = 0\n",
    "            ndcg_scores = []\n",
    "            for user in tqdm(validation_users):\n",
    "                user_id = id_to_user[user]\n",
    "                relevant_reviews = validation_df[validation_df['user_id'] == user_id]\n",
    "                user_validation_edges = validation_edges[:, validation_edges[0] == user]\n",
    "                user_validation_edges = user_validation_edges.to(device)\n",
    "                user_rankings = model(user_validation_edges)\n",
    "                edges_sorted = list(user_validation_edges[1, user_rankings.argsort(descending=True)])\n",
    "                # use validation_df to get the relevances via the movie_id column and the movie_rating column\n",
    "                relevances = []\n",
    "                for edge in edges_sorted:\n",
    "                    movie_id = id_to_movie[int(edge)]\n",
    "                    if (movie_id in relevant_reviews['movie_id'].values):\n",
    "                        relevances.append(relevant_reviews[relevant_reviews['movie_id'] == movie_id]['movie_rating'].values[0])\n",
    "                    else:\n",
    "                        relevances.append(0)\n",
    "                # calculate the ndcg\n",
    "                ndcg = compute_ndcg_at_k(relevances)\n",
    "                if (math.isnan(ndcg)):\n",
    "                    print(relevant_reviews)\n",
    "                    input()\n",
    "                mean_ndcg += ndcg\n",
    "                ndcg_scores.append(ndcg)\n",
    "            mean_ndcg = mean_ndcg / len(validation_users)\n",
    "            print(\"Standard Deviation: {}\".format(np.std(ndcg_scores)))\n",
    "            # create a histogram of the ndcg scores, make bins for each 0.1\n",
    "            ndcg_scores = np.array(ndcg_scores).squeeze()\n",
    "            writer.add_histogram(\"hist_NDCG/val\", ndcg_scores, epoch)\n",
    "            # also make a histogram in matplotlib and save as png\n",
    "            plt.hist(ndcg_scores, bins=np.arange(0, 1.1, 0.1))\n",
    "            plt.suptitle(\"Validation NDCG Histogram\")\n",
    "            # write information about the model to the histogram\n",
    "            plt.title(f\"Model: LightGCN, Embedding Dim: {EMBEDDING_DIM}, Num Layers: {NUM_LAYERS}, Batch Size: {BATCH_SIZE}, LR: {LR}, Num Train Users: {num_train_users}, Num Train Items: {num_train_items}\", fontsize=8, wrap=True)\n",
    "            plt.xlabel(\"NDCG\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            # save the figure in the hist_NDCG folder, with the title having the model information and the epoch number\n",
    "            plt.savefig(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.png\")\n",
    "            plt.close()\n",
    "            # Also save the raw NDCG scores to a csv file, with the model information in the title, and the epoch number\n",
    "            np.savetxt(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.csv\", ndcg_scores, delimiter=\",\")\n",
    "            print(mean_ndcg)\n",
    "            writer.add_scalar(\"NDCG/val\", mean_ndcg, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "            recall_at_k = compute_recall_at_k(validation_graph, model, K)\n",
    "            writer.add_scalar(\"Recall@K/val\", recall_at_k, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "            print(\"Epoch: {}, NDCG: {}, Recall@{}: {}\".format(epoch, mean_ndcg, K, recall_at_k))\n",
    "            average_number_of_matches = 0\n",
    "            for user_id in validation_users:\n",
    "                all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "                recommendations = model.recommend(all_edges.to(device), src_index=torch.tensor([user_id]), dst_index=torch.tensor([x for x in range(num_train_users + 1, num_train_items)]), k=10)[0]\n",
    "                movie_names = [movie_id_to_movie_name[id_to_movie[int(recommendation)]] for recommendation in recommendations]\n",
    "                true_user_reviews = user_review_data[id_to_user[user_id]]\n",
    "                matches = 0\n",
    "                for movie_name in movie_names:\n",
    "                    if movie_name in true_user_reviews['movie_title'].values:\n",
    "                        matches += 1\n",
    "                average_number_of_matches += matches\n",
    "            average_number_of_matches = average_number_of_matches / len(validation_users)\n",
    "            print(\"Average number of matches: {}\".format(average_number_of_matches))\n",
    "            writer.add_scalar(\"Average number of matches\", average_number_of_matches, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "            print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "validation_df[validation_df.user_id == id_to_user[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_edges[:, validation_edges[0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
