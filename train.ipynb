{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from torch_geometric) (2.28.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from torch_geometric) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from torch_geometric) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from torch_geometric) (1.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from torch_geometric) (1.24.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/site-packages (from torch_geometric) (5.9.4)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.14)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models.lightgcn import LightGCN\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use the first 10k rows of the data, set to None to use all data\n",
    "AMOUNT_TO_LOAD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1000/63111 [00:02<02:59, 345.62it/s]\n"
     ]
    }
   ],
   "source": [
    "user_reviews_dir = 'user_reviews'\n",
    "user_review_data = dict()\n",
    "\n",
    "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
    "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
    "        break\n",
    "    try:\n",
    "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Empty file: {filename}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
    "\n",
    "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asel82_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(list(user_review_data.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 884.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove all values with nan in the review column\n",
    "for key in tqdm(user_review_data.keys()):\n",
    "    user_review_data[key] = user_review_data[key].dropna(subset=['movie_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 340.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 396059\n",
      "Validation reviews: 12300\n",
      "Test reviews: 12300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = []\n",
    "validation_reviews = []\n",
    "test_reviews = []\n",
    "for user_id, reviews in tqdm(user_review_data.items()):\n",
    "    if len(reviews) > 50:\n",
    "        validation_review_data_df = reviews.sample(15, replace=False)\n",
    "        validation_review_data = validation_review_data_df.to_dict('records')\n",
    "        for review in validation_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        validation_reviews.extend(validation_review_data)\n",
    "        # remove the validation reviews from the training data\n",
    "        reviews = reviews.drop(validation_review_data_df.index)\n",
    "        test_review_data_df = reviews.sample(15, replace=False)\n",
    "        test_review_data = test_review_data_df.to_dict('records')\n",
    "        for review in test_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        test_reviews.extend(test_review_data)\n",
    "        # remove the test reviews from the training data\n",
    "        reviews = reviews.drop(test_review_data_df.index)\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "    else:\n",
    "        # if the user has less than 5 reviews, we will use all of them for training\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "\n",
    "print(f'Train reviews: {len(train_reviews)}')\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Now that we have the training data, let's construct the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train users: 1000\n",
      "Number of train items: 43225\n",
      "Number of nodes: 45001\n"
     ]
    }
   ],
   "source": [
    "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
    "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
    "num_total_items = len(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews]))\n",
    "num_nodes = num_train_users + num_total_items\n",
    "print(f'Number of train users: {num_train_users}')\n",
    "print(f'Number of train items: {num_train_items}')\n",
    "print(f'Number of nodes: {num_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val_users = len(set([review['user_id'] for review in validation_reviews]))\n",
    "num_val_items = len(set([review['movie_id'] for review in validation_reviews]))\n",
    "num_val_nodes = num_val_users + num_val_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map users to ids\n",
    "movie_id_to_movie_name = dict()\n",
    "for review in train_reviews + validation_reviews + test_reviews:\n",
    "    movie_id_to_movie_name[review['movie_id']] = review['movie_title']\n",
    "\n",
    "user_to_id = dict()\n",
    "for i, user_id in enumerate(set([review['user_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
    "    user_to_id[user_id] = i\n",
    "\n",
    "# Let's map movies to ids\n",
    "movie_to_id = dict()\n",
    "for i, movie_id in enumerate(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
    "    movie_to_id[movie_id] = i + num_train_users\n",
    "\n",
    "# Let's map ids to users\n",
    "id_to_user = dict()\n",
    "for user_id, index in user_to_id.items():\n",
    "    id_to_user[index] = user_id\n",
    "\n",
    "# Let's map ids to movies\n",
    "id_to_movie = dict()\n",
    "for movie_id, index in movie_to_id.items():\n",
    "    id_to_movie[index] = movie_id\n",
    "\n",
    "# Let's map movie names to movie ids\n",
    "movie_name_to_movie_id = dict()\n",
    "for movie_id, movie_name in movie_id_to_movie_name.items():\n",
    "    movie_name_to_movie_id[movie_name] = movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert_review_to_edge(review):\n",
    "    user_id = user_to_id[review['user_id']]\n",
    "    movie_id = movie_to_id[review['movie_id']]\n",
    "    edge_weight = review['movie_rating']\n",
    "    if (edge_weight < 3.5 and edge_weight > 2.5):\n",
    "        return None, None\n",
    "    edge = (user_id, movie_id)\n",
    "    edge_weight = review['movie_rating']\n",
    "    return edge, edge_weight\n",
    "\n",
    "def shuffle_edges_and_edge_weights(edges, edge_weights):\n",
    "    c = list(zip(edges, edge_weights))\n",
    "    random.shuffle(c)\n",
    "    return zip(*c)\n",
    "\n",
    "def convert_reviews_to_edges(reviews):\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    for review in tqdm(reviews):\n",
    "        edge, edge_weight = convert_review_to_edge(review)\n",
    "        if edge is not None:\n",
    "            edges.append(edge)\n",
    "            edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Reformat the edges to be a tensor\n",
    "    edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edges, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 396059/396059 [00:00<00:00, 704428.25it/s]\n",
      "100%|██████████| 12300/12300 [00:00<00:00, 605224.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 318166\n",
      "Validation edges: 10260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the edges between users and movies.\n",
    "# The id of the user will be the index of the user in the user_to_id dict\n",
    "# The id of the movie will be the index of the movie in the movie_to_id dict + the number of users\n",
    "\n",
    "train_edges, train_edge_weights = convert_reviews_to_edges(train_reviews)\n",
    "validation_edges, validation_edge_weights = convert_reviews_to_edges(validation_reviews)\n",
    "\n",
    "print(f'Train edges: {train_edges.shape[1]}')\n",
    "print(f'Validation edges: {validation_edges.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as data\n",
    "\n",
    "# create the graph\n",
    "train_graph = data.Data(\n",
    "    edge_index=train_edges,\n",
    "    edge_attr=torch.tensor(train_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "validation_graph = data.Data(\n",
    "    edge_index=validation_edges,\n",
    "    edge_attr=torch.tensor(validation_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.validate(raise_on_error=True)\n",
    "validation_graph.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some negative edges\n",
    "def resample_edges_for_user(user_positive_edges, user_negative_edges):\n",
    "    num_negative_edges_to_add = user_positive_edges.shape[1] * 3 - user_negative_edges.shape[1]\n",
    "    if (num_negative_edges_to_add <= 0):\n",
    "        num_negative_edges_to_remove = -num_negative_edges_to_add\n",
    "        # choose the negative edges to keep\n",
    "        negative_edges_to_keep = torch.randint(user_negative_edges.shape[1], (user_negative_edges.shape[1] - num_negative_edges_to_remove,))\n",
    "        # remove all the negative edges for this user\n",
    "        user_negative_edges = user_negative_edges[:, negative_edges_to_keep]\n",
    "    else:\n",
    "        # Create new negative edges\n",
    "        negative_edges_to_add = torch.tensor([[user_id] * num_negative_edges_to_add, torch.randint(num_train_users, num_train_items, (num_negative_edges_to_add,))], dtype=torch.long)\n",
    "        # Add the negative edges to the negative edges for this user\n",
    "        user_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
    "    return user_positive_edges, user_negative_edges\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute ndcg\n",
    "def compute_ndcg_at_k(relevances, k=5):\n",
    "    relevances = relevances[:k]\n",
    "    dcg = 0\n",
    "    for i, relevance in enumerate(relevances):\n",
    "        dcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
    "    idcg = 0\n",
    "    for i, relevance in enumerate(sorted(relevances, reverse=True)):\n",
    "        idcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def compute_recall_at_k(validation_graph, model, K):\n",
    "    # get positive edges in validation set\n",
    "    positive_edges = validation_graph.edge_index[:, validation_graph.edge_attr > 3.5]\n",
    "\n",
    "    # map users to positive edges\n",
    "    user_pos_items = get_user_positive_items(positive_edges)\n",
    "\n",
    "    # get users\n",
    "    users = positive_edges[0].unique()\n",
    "\n",
    "    users = users[torch.randint(users.shape[0], (min(200, len(users)),))]\n",
    "    # filter the validation edges to only the users we want to evaluate\n",
    "    user_validation_edges = []\n",
    "    for user in users:\n",
    "        user_validation_edges.append(validation_graph.edge_index[:, validation_graph.edge_index[0] == user])\n",
    "    user_validation_edges = torch.cat(user_validation_edges, dim=1)\n",
    "    print(user_validation_edges.shape)\n",
    "\n",
    "    first_user_id = users[0].item()\n",
    "    user_name = id_to_user[first_user_id]\n",
    "    print(f'User: {user_name}')\n",
    "\n",
    "    # get movies\n",
    "    movie_indices = torch.LongTensor([_ for _ in range(len(users) + 1, validation_graph.num_nodes)]).to(device)\n",
    "\n",
    "    # Get positive items for each user in validation set\n",
    "    truth_items = [set(user_pos_items[user.item()]) for user in users]\n",
    "\n",
    "    first_user_truth_items = truth_items[0]\n",
    "    first_user_truth_items = [id_to_movie[item] for item in first_user_truth_items]\n",
    "    first_user_truth_items = [movie_id_to_movie_name[item] for item in first_user_truth_items]\n",
    "    print(first_user_truth_items)\n",
    "\n",
    "    training_edges = train_graph.edge_index\n",
    "\n",
    "    # Get top-K recommended items for each user in validation set\n",
    "    total_recall = 0\n",
    "    print(\"Computing recommendations for {} users\".format(len(users)))\n",
    "    for user_index, user_id in tqdm(enumerate(users), total=len(users)):\n",
    "        tick = time.time()\n",
    "        all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "        recommendations = model.recommend(all_edges.to(device), src_index=torch.tensor([user_id]), dst_index=torch.tensor([x for x in range(num_train_users + 1, num_train_items)]), k=3 * K)[0]\n",
    "        tock = time.time()\n",
    "        train_edges_for_user = training_edges[:, training_edges[0] == user_id]\n",
    "        # remove all the recommendations that are in the training set\n",
    "        recommendations = recommendations[~torch.isin(recommendations, train_edges_for_user[1])][:K]\n",
    "        if (len(recommendations) < K):\n",
    "            print(\"Not enough recommendations for user {}\".format(user_id))\n",
    "        if (user_id == first_user_id):\n",
    "            first_user_recommended_items = recommendations\n",
    "            first_user_recommended_items = [id_to_movie[item.item()] for item in first_user_recommended_items if item.item() > num_train_users]\n",
    "            first_user_recommended_items = [movie_id_to_movie_name[item] for item in first_user_recommended_items if item in movie_id_to_movie_name]\n",
    "            print(first_user_recommended_items)\n",
    "        # num_intersect = 0\n",
    "        truth_items_for_user = truth_items[user_index]\n",
    "        # for item in recommendations:\n",
    "        #     item = item.item()\n",
    "        #     if item in truth_items_for_user:\n",
    "        #         num_intersect += 1\n",
    "        # print(num_intersect)\n",
    "        num_intersect = len(set([item.item() for item in recommendations]).intersection(truth_items[user_index]))\n",
    "        recall = num_intersect / len(truth_items_for_user)\n",
    "        total_recall += recall\n",
    "    return total_recall / len(users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adapted from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/lightgcn.html\"\"\"\n",
    "class CustomLightGCN(torch.nn.Module):\n",
    "    \"\"\"From the <https://arxiv.org/abs/2002.02126>` paper.\n",
    "\n",
    "    Args:\n",
    "        num_nodes (int): The number of nodes in the graph.\n",
    "        embedding_dim (int): The dimensionality of node embeddings.\n",
    "        num_layers (int): The number of layers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        embedding_dim: int,\n",
    "        num_layers: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "        self.alpha = torch.tensor([1. / (num_layers + 1)] * (num_layers + 1))\n",
    "        self.convs = ModuleList([GATConv(embedding_dim, embedding_dim, heads=8, dropout=0.6) for _ in range(num_layers)])\n",
    "        self.linears = ModuleList([Linear(embedding_dim * 8, embedding_dim) for _ in range(num_layers)])\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def get_embedding(self, edge_index):\n",
    "        x = self.embedding.weight\n",
    "        out = x * self.alpha[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.linears[i](x.view(-1, self.embedding_dim * 8))\n",
    "            out = out + x * self.alpha[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        edge_label_index = edge_index\n",
    "        out = self.get_embedding(edge_index)\n",
    "        user = out[edge_label_index[0]]\n",
    "        movie = out[edge_label_index[1]]\n",
    "        return (user * movie).sum(dim=-1)\n",
    "\n",
    "\n",
    "    def predict_link(self, edge_index, edge_label_index):\n",
    "        \"Predict links between nodes specified in edge_label_index.\"\"\"\n",
    "        pred = self(edge_index, edge_label_index).sigmoid()\n",
    "        return pred.round()\n",
    "\n",
    "\n",
    "    def recommend(self, edge_index, k):\n",
    "        \"\"\"Get top-k recommendations for nodes in src_index.\"\"\"\n",
    "        out_user = self.get_embedding(edge_index)\n",
    "        out_movie = self.get_embedding(edge_index)\n",
    "        pred = out_user @ out_movie.t()\n",
    "        top_index = pred.topk(k, dim=-1).indices\n",
    "        return top_index\n",
    "\n",
    "\n",
    "    def link_pred_loss(self, pred, edge_label):\n",
    "        \"\"\"Computes the model loss for a link prediction using torch.nn.BCEWithLogitsLoss.\n",
    "        \n",
    "        Args:\n",
    "            pred (torch.Tensor): The predictions.\n",
    "            edge_label (torch.Tensor): The ground-truth edge labels.\n",
    "        \"\"\"\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "\n",
    "    def recommendation_loss(self, pos_edge_rank, neg_edge_rank,\n",
    "                            lambda_reg: float = 1e-4):\n",
    "        \"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "        Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "            pos_edge_rank (torch.Tensor): Positive edge rankings.\n",
    "            neg_edge_rank (torch.Tensor): Negative edge rankings.\n",
    "            lambda_reg (int, optional): The L2 regularization strength\n",
    "                of the Bayesian Personalized Ranking (BPR) loss.\n",
    "        \"\"\"\n",
    "        loss_fn = BPRLoss(lambda_reg)\n",
    "        return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is verbatim from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/lightgcn.html. \"\"\"\n",
    "class BPRLoss(_Loss):\n",
    "    \"\"\"The Bayesian Personalized Ranking (BPR) loss.\"\"\"\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = 0\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        \"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).mean()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_hard_negative_edges_for_user(user_positive_edges, user_negative_edges, model, num_train_items, epoch):\n",
    "    # Select hard negative edges based on current model parameters\n",
    "    user_positive_items = user_positive_edges[1, :]\n",
    "    \n",
    "    # get the rankings for this user\n",
    "    all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "    with torch.no_grad():\n",
    "        user_rankings = model.forward(all_edges) # this is of shape (42263) -- each index is the prediction for that index's movie\n",
    "    mask = torch.ones(num_train_items - num_train_users, dtype=torch.bool) # gets indices of all the movies\n",
    "    pos_items_mask = user_positive_items < num_train_items - num_train_users\n",
    "    filtered_pos_items = user_positive_items[pos_items_mask]\n",
    "    mask[filtered_pos_items] = True\n",
    "\n",
    "    # get the rankings for negative items\n",
    "    negative_rankings = user_rankings[mask]\n",
    "    \n",
    "    # select the top k negative items for this user\n",
    "    if epoch != 0:\n",
    "        k = min(epoch - 1, negative_rankings.shape[0])\n",
    "    else:\n",
    "        k = 0\n",
    "\n",
    "    _, topk_items = torch.topk(negative_rankings, k)\n",
    "    negative_items = torch.nonzero(mask).flatten()[topk_items]\n",
    "\n",
    "    # create the new negative edges\n",
    "    negative_edges_to_add = torch.tensor([[user_id] * k, negative_items], dtype=torch.long)\n",
    "    new_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
    "\n",
    "    return user_positive_edges, new_negative_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m     user_negative_edges \u001b[39m=\u001b[39m user_negative_edges[:, :\u001b[39m15000\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[39m# sample hard negative edges\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m user_positive_edges, user_negative_edges \u001b[39m=\u001b[39m resample_hard_negative_edges_for_user(user_positive_edges, user_negative_edges, model, num_train_items, epoch)\n\u001b[1;32m     52\u001b[0m \u001b[39m# resample negative edges to make sure we have enough\u001b[39;00m\n\u001b[1;32m     53\u001b[0m user_positive_edges, user_negative_edges \u001b[39m=\u001b[39m resample_edges_for_user(user_positive_edges, user_negative_edges)\n",
      "Cell \u001b[0;32mIn[165], line 7\u001b[0m, in \u001b[0;36mresample_hard_negative_edges_for_user\u001b[0;34m(user_positive_edges, user_negative_edges, model, num_train_items, epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m user_positive_items \u001b[39m=\u001b[39m user_positive_edges[\u001b[39m1\u001b[39m, :]\n\u001b[1;32m      6\u001b[0m \u001b[39m# get the rankings for this user\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m all_edges \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([(user_id, item_id) \u001b[39mfor\u001b[39;49;00m item_id \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(num_train_users, num_train_items)], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong)\u001b[39m.\u001b[39mt()\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     user_rankings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(all_edges) \u001b[39m# this is of shape (42263) -- each index is the prediction for that index's movie\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "LR = 5e-5\n",
    "BATCH_SIZE = min(128, len(user_review_data))\n",
    "EMBEDDING_DIM = 64\n",
    "K = 10\n",
    "model = LightGCN(num_nodes=num_nodes, embedding_dim=EMBEDDING_DIM, num_layers=NUM_LAYERS)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Running on device: {}\".format(device))\n",
    "print(EMBEDDING_DIM)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[100, 200, 300, 400], gamma=0.5)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=100)\n",
    "\n",
    "train_positive_edges = train_graph.edge_index[:, train_graph.edge_attr >= 3.5]\n",
    "train_negative_edges = train_graph.edge_index[:, train_graph.edge_attr <= 2.5]\n",
    "\n",
    "validation_df = pd.DataFrame.from_dict(validation_reviews)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment=f'LightGCN_{EMBEDDING_DIM}_layers_{NUM_LAYERS}_batch_size_{BATCH_SIZE}_lr_{LR}_num_train_users_{num_train_users}_num_train_items_{num_train_items}_recall_{K}')\n",
    "\n",
    "for epoch in range(10):\n",
    "    # we are using BPR so we go by user\n",
    "    average_loss = 0\n",
    "    # We'll proceed in batches of users\n",
    "    for start_idx in tqdm(range(0, num_train_users, BATCH_SIZE)):\n",
    "        model.train()\n",
    "        loss = torch.tensor(0.0, requires_grad=True)\n",
    "        # randomly select a batch of users\n",
    "        users_in_batch = torch.randperm(num_train_users)[start_idx:start_idx + BATCH_SIZE]\n",
    "        for user_id in users_in_batch:\n",
    "            # get all the edges specific to this user\n",
    "            user_positive_edges = train_positive_edges[:, train_positive_edges[0] == user_id]\n",
    "            user_negative_edges = train_negative_edges[:, train_negative_edges[0] == user_id]\n",
    "            if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
    "                continue\n",
    "            # limit the number of positive edges to 5000\n",
    "            if (user_positive_edges.shape[1] > 5000):\n",
    "                user_positive_edges = user_positive_edges[:, :5000]\n",
    "            # Get at most 15000 negative edges\n",
    "            if (user_negative_edges.shape[1] > 15000):\n",
    "                user_negative_edges = user_negative_edges[:, :15000]\n",
    "            # sample hard negative edges\n",
    "            user_positive_edges, user_negative_edges = resample_hard_negative_edges_for_user(user_positive_edges, user_negative_edges, model, num_train_items, epoch)\n",
    "            # resample negative edges to make sure we have enough\n",
    "            user_positive_edges, user_negative_edges = resample_edges_for_user(user_positive_edges, user_negative_edges)\n",
    "            # concatenate the positive and negative edges\n",
    "            user_edges = torch.cat([user_positive_edges, user_negative_edges], dim=1)\n",
    "            # get the rankings for this user\n",
    "            user_edges = user_edges.to(device)\n",
    "            user_rankings = model(user_edges)\n",
    "            # divide the rankings into positive and negative rankings\n",
    "            user_positive_rankings = user_rankings[:user_positive_edges.shape[1]]\n",
    "            user_negative_rankings = user_rankings[user_positive_edges.shape[1]:]\n",
    "            # create all pairs of positive and negative rankings\n",
    "            user_positive_rankings = user_positive_rankings.unsqueeze(1).repeat(1, user_negative_rankings.shape[0])\n",
    "            user_negative_rankings = user_negative_rankings.unsqueeze(0).repeat(user_positive_rankings.shape[0], 1)\n",
    "            # get the user loss\n",
    "            user_loss = model.recommendation_loss(user_positive_rankings, user_negative_rankings, 1e-4)\n",
    "            # add the user loss to the total loss\n",
    "            loss = loss + user_loss\n",
    "        # divide the loss by the number of users\n",
    "        loss = loss / BATCH_SIZE\n",
    "        # log the loss\n",
    "        # backprop\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "        print(epoch * BATCH_SIZE + start_idx // BATCH_SIZE)\n",
    "        average_loss = 0\n",
    "        if (epoch * BATCH_SIZE + start_idx // BATCH_SIZE) % 100 == 0:\n",
    "            # evaluate the model\n",
    "            model.eval()\n",
    "            # iterate over all users in the validation set\n",
    "            validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "            # randomly select 1000 of the users\n",
    "            validation_users = random.sample(validation_users, min(len(validation_users), 500))\n",
    "            mean_ndcg = 0\n",
    "            ndcg_scores = []\n",
    "            for user in tqdm(validation_users):\n",
    "                user_id = id_to_user[user]\n",
    "                relevant_reviews = validation_df[validation_df['user_id'] == user_id]\n",
    "                user_validation_edges = validation_edges[:, validation_edges[0] == user]\n",
    "                user_validation_edges = user_validation_edges.to(device)\n",
    "                user_rankings = model(user_validation_edges)\n",
    "                edges_sorted = list(user_validation_edges[1, user_rankings.argsort(descending=True)])\n",
    "                # use validation_df to get the relevances via the movie_id column and the movie_rating column\n",
    "                relevances = []\n",
    "                for edge in edges_sorted:\n",
    "                    movie_id = id_to_movie[int(edge)]\n",
    "                    if (movie_id in relevant_reviews['movie_id'].values):\n",
    "                        relevances.append(relevant_reviews[relevant_reviews['movie_id'] == movie_id]['movie_rating'].values[0])\n",
    "                    else:\n",
    "                        relevances.append(0)\n",
    "                # calculate the ndcg\n",
    "                ndcg = compute_ndcg_at_k(relevances)\n",
    "                if (math.isnan(ndcg)):\n",
    "                    print(relevant_reviews)\n",
    "                    input()\n",
    "                mean_ndcg += ndcg\n",
    "                ndcg_scores.append(ndcg)\n",
    "            mean_ndcg = mean_ndcg / len(validation_users)\n",
    "            print(\"Standard Deviation: {}\".format(np.std(ndcg_scores)))\n",
    "            # create a histogram of the ndcg scores, make bins for each 0.1\n",
    "            ndcg_scores = np.array(ndcg_scores).squeeze()\n",
    "            writer.add_histogram(\"hist_NDCG/val\", ndcg_scores, epoch)\n",
    "            # also make a histogram in matplotlib and save as png\n",
    "            plt.hist(ndcg_scores, bins=np.arange(0, 1.1, 0.1))\n",
    "            plt.suptitle(\"Validation NDCG Histogram\")\n",
    "            # write information about the model to the histogram\n",
    "            plt.title(f\"Model: LightGCN, Embedding Dim: {EMBEDDING_DIM}, Num Layers: {NUM_LAYERS}, Batch Size: {BATCH_SIZE}, LR: {LR}, Num Train Users: {num_train_users}, Num Train Items: {num_train_items}\", fontsize=8, wrap=True)\n",
    "            plt.xlabel(\"NDCG\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            # save the figure in the hist_NDCG folder, with the title having the model information and the epoch number\n",
    "            plt.savefig(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.png\")\n",
    "            plt.close()\n",
    "            # Also save the raw NDCG scores to a csv file, with the model information in the title, and the epoch number\n",
    "            np.savetxt(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.csv\", ndcg_scores, delimiter=\",\")\n",
    "            print(mean_ndcg)\n",
    "            writer.add_scalar(\"NDCG/val\", mean_ndcg, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "            recall_at_k = compute_recall_at_k(validation_graph, model, K)\n",
    "            writer.add_scalar(\"Recall@K/val\", recall_at_k, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "            print(\"Epoch: {}, NDCG: {}, Recall@{}: {}\".format(epoch, mean_ndcg, K, recall_at_k))\n",
    "            average_number_of_matches = 0\n",
    "            for user_id in validation_users:\n",
    "                all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "                recommendations = model.recommend(all_edges.to(device), src_index=torch.tensor([user_id]), dst_index=torch.tensor([x for x in range(num_train_users + 1, num_train_items)]), k=10)[0]\n",
    "                movie_names = [movie_id_to_movie_name[id_to_movie[int(recommendation)]] for recommendation in recommendations]\n",
    "                true_user_reviews = user_review_data[id_to_user[user_id]]\n",
    "                matches = 0\n",
    "                for movie_name in movie_names:\n",
    "                    if movie_name in true_user_reviews['movie_title'].values:\n",
    "                        matches += 1\n",
    "                average_number_of_matches += matches\n",
    "            average_number_of_matches = average_number_of_matches / len(validation_users)\n",
    "            print(\"Average number of matches: {}\".format(average_number_of_matches))\n",
    "            writer.add_scalar(\"Average number of matches\", average_number_of_matches, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "            print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "validation_df[validation_df.user_id == id_to_user[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_edges[:, validation_edges[0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
=======
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UQK_3l0ugoHn",
        "outputId": "06185120-bd6e-4284-bf8b-81260d240b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (2.25.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.2.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (1.26.14)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (1.1.1)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=7e1ec35eced56cd8f50def3342c013bdebc149a8fb31878693444c2b02543711\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: psutil, torch_geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch_geometric-2.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.0+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.16%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.16+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch_geometric\n",
        "%pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
        "%pip install torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
        "%pip install torch\n",
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "A5yY1RZQgoHn"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn.models.lightgcn import LightGCN\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ3JhgijgoHo"
      },
      "source": [
        "## Load Data\n",
        "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "CFhM-0EJgoHp"
      },
      "outputs": [],
      "source": [
        "# for now we will use the first 10k rows of the data, set to None to use all data\n",
        "AMOUNT_TO_LOAD = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy5HrpxfgoHp",
        "outputId": "c8810b49-1ccd-4cd5-f9dd-44bd867f97c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# We also need to mount in a google drive \n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSZsmDIBgoHp",
        "outputId": "c182227e-9a73-4fe9-9c4a-71cf8dcac613"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1024/63111 [00:01<01:36, 643.38it/s]\n"
          ]
        }
      ],
      "source": [
        "user_reviews_dir = 'user_reviews'\n",
        "\n",
        "# We are in the google drive setting then\n",
        "if not os.path.exists(user_reviews_dir):\n",
        "  user_reviews_dir = 'gdrive/MyDrive/user_reviews'\n",
        "\n",
        "user_review_data = dict()\n",
        "\n",
        "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
        "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
        "        break\n",
        "    try:\n",
        "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f'Empty file: {filename}')\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-4dNzbKgoHp"
      },
      "source": [
        "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
        "\n",
        "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXjbPr6rgoHp",
        "outputId": "769b1df5-edd2-4996-a708-e1753ba323ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "asel82_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "print(list(user_review_data.keys())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfsfB1AcgoHp",
        "outputId": "5e70fe39-0a8a-4bef-d5aa-a2a2191ea00c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1024/1024 [00:00<00:00, 2065.58it/s]\n"
          ]
        }
      ],
      "source": [
        "# remove all values with nan in the review column\n",
        "for key in tqdm(user_review_data.keys()):\n",
        "    user_review_data[key] = user_review_data[key].dropna(subset=['movie_rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMoYYcHNgoHp",
        "outputId": "4386c5f6-8ca3-48b4-f1b6-b15c1b54910a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1024/1024 [00:01<00:00, 648.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train reviews: 403979\n",
            "Validation reviews: 12600\n",
            "Test reviews: 12600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_reviews = []\n",
        "validation_reviews = []\n",
        "test_reviews = []\n",
        "for user_id, reviews in tqdm(user_review_data.items()):\n",
        "    if len(reviews) > 50:\n",
        "        validation_review_data_df = reviews.sample(15, replace=False)\n",
        "        validation_review_data = validation_review_data_df.to_dict('records')\n",
        "        for review in validation_review_data:\n",
        "            review['user_id'] = user_id\n",
        "        validation_reviews.extend(validation_review_data)\n",
        "        # remove the validation reviews from the training data\n",
        "        reviews = reviews.drop(validation_review_data_df.index)\n",
        "        test_review_data_df = reviews.sample(15, replace=False)\n",
        "        test_review_data = test_review_data_df.to_dict('records')\n",
        "        for review in test_review_data:\n",
        "            review['user_id'] = user_id\n",
        "        test_reviews.extend(test_review_data)\n",
        "        # remove the test reviews from the training data\n",
        "        reviews = reviews.drop(test_review_data_df.index)\n",
        "        train_review_data = reviews.to_dict('records')\n",
        "        for review in train_review_data:\n",
        "            review['user_id'] = user_id\n",
        "        train_reviews.extend(train_review_data)\n",
        "    else:\n",
        "        # if the user has less than 5 reviews, we will use all of them for training\n",
        "        train_review_data = reviews.to_dict('records')\n",
        "        for review in train_review_data:\n",
        "            review['user_id'] = user_id\n",
        "        train_reviews.extend(train_review_data)\n",
        "\n",
        "print(f'Train reviews: {len(train_reviews)}')\n",
        "print(f'Validation reviews: {len(validation_reviews)}')\n",
        "print(f'Test reviews: {len(test_reviews)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9njzZIogoHq"
      },
      "source": [
        "## Build the Model\n",
        "Now that we have the training data, let's construct the model to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvJEs2EigoHq",
        "outputId": "16471754-a6c5-44a7-dac7-7f1e546d5bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of train users: 1024\n",
            "Number of train items: 43530\n",
            "Number of nodes: 45291\n"
          ]
        }
      ],
      "source": [
        "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
        "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
        "num_total_items = len(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews]))\n",
        "num_nodes = num_train_users + num_total_items\n",
        "print(f'Number of train users: {num_train_users}')\n",
        "print(f'Number of train items: {num_train_items}')\n",
        "print(f'Number of nodes: {num_nodes}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "VvhZUsTYgoHq"
      },
      "outputs": [],
      "source": [
        "num_val_users = len(set([review['user_id'] for review in validation_reviews]))\n",
        "num_val_items = len(set([review['movie_id'] for review in validation_reviews]))\n",
        "num_val_nodes = num_val_users + num_val_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "Om2OylcjgoHq"
      },
      "outputs": [],
      "source": [
        "# Let's map users to ids\n",
        "movie_id_to_movie_name = dict()\n",
        "for review in train_reviews + validation_reviews + test_reviews:\n",
        "    movie_id_to_movie_name[review['movie_id']] = review['movie_title']\n",
        "\n",
        "user_to_id = dict()\n",
        "for i, user_id in enumerate(set([review['user_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
        "    user_to_id[user_id] = i\n",
        "\n",
        "# Let's map movies to ids\n",
        "movie_to_id = dict()\n",
        "for i, movie_id in enumerate(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
        "    movie_to_id[movie_id] = i + num_train_users\n",
        "\n",
        "# Let's map ids to users\n",
        "id_to_user = dict()\n",
        "for user_id, index in user_to_id.items():\n",
        "    id_to_user[index] = user_id\n",
        "\n",
        "# Let's map ids to movies\n",
        "id_to_movie = dict()\n",
        "for movie_id, index in movie_to_id.items():\n",
        "    id_to_movie[index] = movie_id\n",
        "\n",
        "# Let's map movie names to movie ids\n",
        "movie_name_to_movie_id = dict()\n",
        "for movie_id, movie_name in movie_id_to_movie_name.items():\n",
        "    movie_name_to_movie_id[movie_name] = movie_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "dlhAlhSGgoHq"
      },
      "outputs": [],
      "source": [
        "# Let's add nodes that are in our validation and test sets but not in our training set\n",
        "for review in validation_reviews:\n",
        "    if review['user_id'] not in user_to_id:\n",
        "        user_to_id[review['user_id']] = len(user_to_id)\n",
        "    if review['movie_id'] not in movie_to_id:\n",
        "        movie_to_id[review['movie_id']] = len(movie_to_id) + num_train_users\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "OOH3cvkqgoHq"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def convert_review_to_edge(review):\n",
        "    user_id = user_to_id[review['user_id']]\n",
        "    movie_id = movie_to_id[review['movie_id']]\n",
        "    edge_weight = review['movie_rating']\n",
        "    if (edge_weight < 3.5 and edge_weight > 2.5):\n",
        "        return None, None\n",
        "    edge = (user_id, movie_id)\n",
        "    edge_weight = review['movie_rating']\n",
        "    return edge, edge_weight\n",
        "\n",
        "def shuffle_edges_and_edge_weights(edges, edge_weights):\n",
        "    c = list(zip(edges, edge_weights))\n",
        "    random.shuffle(c)\n",
        "    return zip(*c)\n",
        "\n",
        "def convert_reviews_to_edges(reviews):\n",
        "    edges = []\n",
        "    edge_weights = []\n",
        "    for review in tqdm(reviews):\n",
        "        edge, edge_weight = convert_review_to_edge(review)\n",
        "        if edge is not None:\n",
        "            edges.append(edge)\n",
        "            edge_weights.append(edge_weight)\n",
        "    \n",
        "    # Reformat the edges to be a tensor\n",
        "    edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    return edges, edge_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQvDO7zAgoHq",
        "outputId": "ad3410df-2200-4c3d-f70f-3ec8c616972b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 403979/403979 [00:00<00:00, 1683968.45it/s]\n",
            "100%|██████████| 12600/12600 [00:00<00:00, 1326578.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train edges: 324568\n",
            "Validation edges: 10471\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Now let's create the edges between users and movies.\n",
        "# The id of the user will be the index of the user in the user_to_id dict\n",
        "# The id of the movie will be the index of the movie in the movie_to_id dict + the number of users\n",
        "\n",
        "train_edges, train_edge_weights = convert_reviews_to_edges(train_reviews)\n",
        "validation_edges, validation_edge_weights = convert_reviews_to_edges(validation_reviews)\n",
        "\n",
        "print(f'Train edges: {train_edges.shape[1]}')\n",
        "print(f'Validation edges: {validation_edges.shape[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "4LZaF28AgoHq"
      },
      "outputs": [],
      "source": [
        "import torch_geometric.data as data\n",
        "\n",
        "# create the graph\n",
        "train_graph = data.Data(\n",
        "    edge_index=train_edges,\n",
        "    edge_attr=torch.tensor(train_edge_weights),\n",
        "    num_nodes=num_nodes\n",
        ").to(torch.device(device))\n",
        "\n",
        "validation_graph = data.Data(\n",
        "    edge_index=validation_edges,\n",
        "    edge_attr=torch.tensor(validation_edge_weights),\n",
        "    num_nodes=num_nodes\n",
        ").to(torch.device(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDaHcSLVgoHq",
        "outputId": "b00c9953-3f83-474a-afa9-70ccf740af18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 307,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_graph.validate(raise_on_error=True)\n",
        "validation_graph.validate(raise_on_error=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "IyZJ4UTKgoHr"
      },
      "outputs": [],
      "source": [
        "# Let's create some negative edges\n",
        "def resample_edges_for_user(user_positive_edges, user_negative_edges):\n",
        "    num_negative_edges_to_add = user_positive_edges.shape[1] * 3 - user_negative_edges.shape[1]\n",
        "    if (num_negative_edges_to_add <= 0):\n",
        "        num_negative_edges_to_remove = -num_negative_edges_to_add\n",
        "        # choose the negative edges to keep\n",
        "        negative_edges_to_keep = torch.randint(user_negative_edges.shape[1], (user_negative_edges.shape[1] - num_negative_edges_to_remove,))\n",
        "        # remove all the negative edges for this user\n",
        "        user_negative_edges = user_negative_edges[:, negative_edges_to_keep]\n",
        "    else:\n",
        "        # Create new negative edges\n",
        "        negative_edges_to_add = torch.tensor([[user_id] * num_negative_edges_to_add, torch.randint(num_train_users, num_train_items, (num_negative_edges_to_add,))], dtype=torch.long)\n",
        "        # Add the negative edges to the negative edges for this user\n",
        "        user_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
        "    return user_positive_edges, user_negative_edges\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "__Dp5M-TgoHr"
      },
      "outputs": [],
      "source": [
        "# let's compute ndcg\n",
        "def compute_ndcg_at_k(relevances, k=5):\n",
        "    relevances = relevances[:k]\n",
        "    dcg = 0\n",
        "    for i, relevance in enumerate(relevances):\n",
        "        dcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
        "    idcg = 0\n",
        "    for i, relevance in enumerate(sorted(relevances, reverse=True)):\n",
        "        idcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
        "    return dcg / idcg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "YuG0pr4qgoHr"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Embedding, ModuleList\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "from torch_geometric.nn.conv import LGConv\n",
        "from torch_geometric.typing import Adj, OptTensor, SparseTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "p0YFj4l3goHr"
      },
      "outputs": [],
      "source": [
        "def generate_negative_edges(user_negative_edges, minibatch_size):\n",
        "    num_negative_edges_to_add = minibatch_size - user_negative_edges.shape[1]\n",
        "    # Create new negative edges\n",
        "    negative_edges_to_add = torch.tensor([[user_id] * num_negative_edges_to_add, torch.randint(num_train_users, num_train_items, (num_negative_edges_to_add,))], dtype=torch.long).to(torch.device(device))\n",
        "    # Add the negative edges to the negative edges for this user\n",
        "    user_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
        "    return user_negative_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "ZkrQRQuSgoHr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def get_user_positive_items(edge_index):\n",
        "    \"\"\"Generates dictionary of positive items for each user\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        dict: dictionary of positive items for each user\n",
        "    \"\"\"\n",
        "    user_pos_items = {}\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        user = edge_index[0][i].item()\n",
        "        item = edge_index[1][i].item()\n",
        "        if user not in user_pos_items:\n",
        "            user_pos_items[user] = []\n",
        "        user_pos_items[user].append(item)\n",
        "    return user_pos_items\n",
        "\n",
        "\n",
        "def compute_recall_at_k(validation_graph, model, K):\n",
        "    # get positive edges in validation set\n",
        "    positive_edges = validation_graph.edge_index[:, validation_graph.edge_attr > 3.5]\n",
        "\n",
        "    # map users to positive edges\n",
        "    user_pos_items = get_user_positive_items(positive_edges)\n",
        "\n",
        "    # get users\n",
        "    users = positive_edges[0].unique()\n",
        "\n",
        "    users = users[torch.randint(users.shape[0], (min(100, len(users)),))]\n",
        "    # filter the validation edges to only the users we want to evaluate\n",
        "    user_validation_edges = []\n",
        "    for user in users:\n",
        "        user_validation_edges.append(validation_graph.edge_index[:, validation_graph.edge_index[0] == user])\n",
        "    user_validation_edges = torch.cat(user_validation_edges, dim=1)\n",
        "    # print(user_validation_edges.shape)\n",
        "\n",
        "    first_user_id = users[0].item()\n",
        "    user_name = id_to_user[first_user_id]\n",
        "    # print(f'User: {user_name}')\n",
        "\n",
        "    # get movies\n",
        "    movie_indices = torch.LongTensor([_ for _ in range(len(users) + 1, validation_graph.num_nodes)]).to(device)\n",
        "\n",
        "    # Get positive items for each user in validation set\n",
        "    truth_items = [set(user_pos_items[user.item()]) for user in users]\n",
        "\n",
        "    first_user_truth_items = truth_items[0]\n",
        "    first_user_truth_items = [id_to_movie[item] for item in first_user_truth_items]\n",
        "    first_user_truth_items = [movie_id_to_movie_name[item] for item in first_user_truth_items]\n",
        "    # print(first_user_truth_items)\n",
        "\n",
        "    training_edges = train_graph.edge_index\n",
        "\n",
        "    # Get top-K recommended items for each user in validation set\n",
        "    total_recall = 0\n",
        "    # print(\"Computing recommendations for {} users\".format(len(users)))\n",
        "    for user_index, user_id in tqdm(enumerate(users), total=len(users)):\n",
        "        all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
        "        recommendations = model.recommend(all_edges.to(device), src_index=torch.tensor([user_id]), dst_index=torch.tensor([x for x in range(num_train_users + 1, num_train_items)]).to(device), k=3 * K)[0]\n",
        "\n",
        "        train_edges_for_user = training_edges[:, training_edges[0] == user_id]\n",
        "        # remove all the recommendations that are in the training set\n",
        "        recommendations = recommendations[~torch.isin(recommendations, train_edges_for_user[1])][:K]\n",
        "        if (len(recommendations) < K):\n",
        "            pass\n",
        "            # print(\"Not enough recommendations for user {}\".format(user_id))\n",
        "        if (user_id == first_user_id):\n",
        "            first_user_recommended_items = recommendations\n",
        "            first_user_recommended_items = [id_to_movie[item.item()] for item in first_user_recommended_items if item.item() > num_train_users]\n",
        "            first_user_recommended_items = [movie_id_to_movie_name[item] for item in first_user_recommended_items if item in movie_id_to_movie_name]\n",
        "            # print(first_user_recommended_items)\n",
        "        # num_intersect = 0\n",
        "        truth_items_for_user = truth_items[user_index]\n",
        "        # for item in recommendations:\n",
        "        #     item = item.item()\n",
        "        #     if item in truth_items_for_user:\n",
        "        #         num_intersect += 1\n",
        "        # print(num_intersect)\n",
        "        num_intersect = len(set([item.item() for item in recommendations]).intersection(truth_items[user_index]))\n",
        "        recall = num_intersect / len(truth_items_for_user)\n",
        "        total_recall += recall\n",
        "    return total_recall / len(users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "e1wf4zwJk5Dn"
      },
      "outputs": [],
      "source": [
        "# these functions taken from https://github.com/NegatioN/WARP-Pytorch\n",
        "def num_tries_gt_zero(scores, batch_size, max_trials, max_num, device):\n",
        "    '''\n",
        "    scores: [batch_size x N] float scores\n",
        "    returns: [batch_size x 1] the lowest indice per row where scores were first greater than 0. plus 1\n",
        "    '''\n",
        "    tmp = scores.gt(0).nonzero().t()\n",
        "    # We offset these values by 1 to look for unset values (zeros) later\n",
        "    values = tmp[1] + 1\n",
        "    # Sparse tensors can't be moved with .to() or .cuda() if you want to send in cuda variables first\n",
        "    if device.type == 'cuda':\n",
        "        t = torch.cuda.sparse.LongTensor(tmp, values, torch.Size((batch_size, max_trials+1))).to_dense()\n",
        "    else:\n",
        "        t = torch.sparse.LongTensor(tmp, values, torch.Size((batch_size, max_trials+1))).to_dense()\n",
        "    t[(t == 0)] += max_num # set all unused indices to be max possible number so its not picked by min() call\n",
        "\n",
        "    tries = torch.min(t, dim=1)[0]\n",
        "    return tries\n",
        "\n",
        "\n",
        "def warp_loss(positive_predictions, negative_predictions, num_labels, device):\n",
        "    '''\n",
        "    positive_predictions: [batch_size x 1] floats between -1 to 1\n",
        "    negative_predictions: [batch_size x N] floats between -1 to 1\n",
        "    num_labels: int total number of labels in dataset (not just the subset you're using for the batch)\n",
        "    device: pytorch.device\n",
        "    '''\n",
        "    batch_size, max_trials = negative_predictions.size(0), negative_predictions.size(1)\n",
        "\n",
        "    offsets, ones, max_num = (torch.arange(0, batch_size, 1).long().to(device) * (max_trials + 1),\n",
        "                              torch.ones(batch_size, 1).float().to(device),\n",
        "                              batch_size * (max_trials + 1) )\n",
        "\n",
        "    sample_scores = (1 + negative_predictions - positive_predictions)\n",
        "    # Add column of ones so we know when we used all our attempts, This is used for indexing and computing should_count_loss if no real value is above 0\n",
        "    sample_scores, negative_predictions = (torch.cat([sample_scores, ones], dim=1),\n",
        "                                           torch.cat([negative_predictions, ones], dim=1))\n",
        "\n",
        "    tries = num_tries_gt_zero(sample_scores, batch_size, max_trials, max_num, device)\n",
        "    attempts, trial_offset = tries.float(), (tries - 1) + offsets\n",
        "    loss_weights, should_count_loss = ( torch.log(torch.floor((num_labels - 1) / attempts)),\n",
        "                                        (attempts <= max_trials).float()) #Don't count loss if we used max number of attempts\n",
        "\n",
        "    losses = loss_weights * ((1 - positive_predictions.view(-1)) + negative_predictions.view(-1)[trial_offset]) * should_count_loss\n",
        "\n",
        "    return losses.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DxgB1IGVgoHr",
        "outputId": "58bd6e91-9fbf-4c99-f61a-1588135958a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on device: cpu\n",
            "8.990080958988983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:12<00:00, 15.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_train ndcg time 12.95704874995863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 840/840 [00:04<00:00, 174.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_test ndcg time 4.8195535419508815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 92/100 [00:05<00:00, 18.66it/s]"
          ]
        }
      ],
      "source": [
        "import json \n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import timeit\n",
        "\n",
        "NUM_LAYERS = 1\n",
        "LR = 1e-4\n",
        "BATCH_SIZE = min(256, len(user_review_data))\n",
        "EMBEDDING_DIM = 64\n",
        "K = 20\n",
        "NUM_EPOCHS = 100\n",
        "MINIBATCH_SIZE = 128\n",
        "BPR_LAMBDA = 1e-6\n",
        "WARP_LOSS = False\n",
        "\n",
        "metadata = {\n",
        "    'num_layers': NUM_LAYERS,\n",
        "    'lr': LR,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'minibatch_size': MINIBATCH_SIZE,\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'num_epochs': NUM_EPOCHS,\n",
        "    'k': K,\n",
        "    'num_train_users': num_train_users,\n",
        "    'num_train_items': num_train_items,\n",
        "    'loss_function': 'BPR',\n",
        "}\n",
        "\n",
        "# make the output directory\n",
        "output_path = f\"models/{int(time.time())}/\"\n",
        "os.makedirs(output_path)\n",
        "\n",
        "json.dump(metadata, open(f\"{output_path}metadata.json\", 'w'))\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "writer = SummaryWriter(comment=f'LightGCN_{EMBEDDING_DIM}_layers_{NUM_LAYERS}_batch_size_{BATCH_SIZE}_lr_{LR}_num_train_users_{num_train_users}_num_train_items_{num_train_items}_recall_{K}_loss_BPR')\n",
        "model = LightGCN(num_nodes=num_nodes, embedding_dim=EMBEDDING_DIM, num_layers=NUM_LAYERS)\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Running on device: {}\".format(device))\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95)\n",
        "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[100, 200, 300, 400], gamma=0.5)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=100)\n",
        "\n",
        "train_positive_edges = train_graph.edge_index[:, train_graph.edge_attr >= 3.5]\n",
        "train_negative_edges = train_graph.edge_index[:, train_graph.edge_attr <= 2.5]\n",
        "\n",
        "# get all of the users in the validation set\n",
        "validation_users_total = list(set([int(x) for x in validation_edges[0, :]]))\n",
        "train_users_total = list(set([int(x) for x in train_edges[0, :]]))\n",
        "\n",
        "validation_df = pd.DataFrame.from_dict(validation_reviews)\n",
        "train_df = pd.DataFrame.from_dict(train_reviews)\n",
        "\n",
        "loss_run = []\n",
        "train_ndcg = []\n",
        "test_ndcg = []\n",
        "train_recall = []\n",
        "test_recall = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # We are using BPR so we go by user, proceeding in batches\n",
        "    loss_over_epoch = 0\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for start_idx in range(0, num_train_users, BATCH_SIZE):\n",
        "        model.train()\n",
        "        loss = torch.tensor(0.0, requires_grad=True)\n",
        "        # randomly select a batch of users\n",
        "        users_in_batch = torch.randperm(num_train_users)[start_idx:start_idx + BATCH_SIZE]\n",
        "\n",
        "        user_positive_rankings_batch = torch.zeros(size=(BATCH_SIZE, MINIBATCH_SIZE), device=device)\n",
        "        user_negative_rankings_batch = torch.zeros(size=(BATCH_SIZE, MINIBATCH_SIZE), device=device)\n",
        "\n",
        "        for user_id in users_in_batch:\n",
        "            # get all the edges specific to this user\n",
        "            user_positive_edges = train_positive_edges[:, train_positive_edges[0] == user_id]\n",
        "            user_negative_edges = train_negative_edges[:, train_negative_edges[0] == user_id]\n",
        "            if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
        "                continue\n",
        "            \n",
        "            # sample 1 positive edge\n",
        "            user_positive_edges = user_positive_edges[:, :1]\n",
        "            # sample minibatch number of negative edges\n",
        "            if (user_negative_edges.shape[1] > MINIBATCH_SIZE):\n",
        "                user_negative_edges = user_negative_edges[:, :MINIBATCH_SIZE]\n",
        "            else:\n",
        "                # resample the negative edges if we don't have enough\n",
        "                user_negative_edges = generate_negative_edges(user_negative_edges, MINIBATCH_SIZE)\n",
        "\n",
        "            # concatenate the positive and negative edges\n",
        "            user_edges = torch.cat([user_positive_edges, user_negative_edges], dim=1)\n",
        "\n",
        "            # get the rankings for this user\n",
        "            user_edges = user_edges.to(device)\n",
        "            user_rankings = model(user_edges)\n",
        "\n",
        "            # divide the rankings into positive and negative rankings\n",
        "            user_positive_rankings = user_rankings[:1]\n",
        "            user_negative_rankings = user_rankings[1:]\n",
        "\n",
        "            # Get the loss, we have to duplicate the positive edge to make them the same size!\n",
        "            user_positive_rankings = user_positive_rankings.repeat(user_negative_rankings.shape[0])\n",
        "\n",
        "            user_positive_rankings_batch = torch.cat([user_positive_rankings_batch, user_positive_rankings.unsqueeze(0)], dim=0)\n",
        "            user_negative_rankings_batch = torch.cat([user_negative_rankings_batch, user_negative_rankings.unsqueeze(0)], dim=0)\n",
        "\n",
        "        if WARP_LOSS:\n",
        "            # We should only pass in the singular positive ranking for each user\n",
        "            user_positive_rankings_batch = user_positive_rankings_batch[:, : 1]\n",
        "            sample_scores = (1 + user_negative_rankings_batch - user_positive_rankings_batch)\n",
        "\n",
        "            user_loss = warp_loss(user_positive_rankings_batch, user_negative_rankings_batch, num_labels=100, device=device)\n",
        "        else:\n",
        "            user_loss = model.recommendation_loss(user_negative_rankings_batch, user_negative_rankings_batch, BPR_LAMBDA)\n",
        "            \n",
        "        # add the user loss to the total loss\n",
        "        loss = loss + user_loss\n",
        "        # divide the loss by the number of users\n",
        "        loss = loss / BATCH_SIZE\n",
        "        loss_over_epoch += loss.item()\n",
        "        # backprop\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "        # record our losses\n",
        "    writer.add_scalar(\"Loss/train\", loss_over_epoch, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
        "    loss_run.append(loss_over_epoch)\n",
        "\n",
        "    elapsed = timeit.default_timer() - start_time\n",
        "    print(elapsed)\n",
        "\n",
        "    if (epoch % 4 == 0):\n",
        "        # evaluate the model\n",
        "        model.eval()\n",
        "\n",
        "        # get the ndcg values\n",
        "        start_time = timeit.default_timer()\n",
        "        mean_ndcg_train = get_train_ndcg(validation_users_total, model, train_edges, train_df, id_to_user, id_to_movie)\n",
        "        elapsed_time = timeit.default_timer() - start_time\n",
        "        print(f\"get_train ndcg time {elapsed_time}\")\n",
        "        \n",
        "        start_time = timeit.default_timer()\n",
        "        mean_ndcg_test = get_test_ndcg(validation_users_total, model, validation_edges, validation_df, id_to_user, id_to_movie)\n",
        "        elapsed_time = timeit.default_timer() - start_time\n",
        "        print(f\"get_test ndcg time {elapsed_time}\")\n",
        "        \n",
        "        train_ndcg.append(mean_ndcg_train)\n",
        "        test_ndcg.append(mean_ndcg_test)\n",
        "        \n",
        "        # get the recall@k values\n",
        "        start_time = timeit.default_timer()\n",
        "        recall_at_k_train = compute_recall_at_k(train_graph, model, K)\n",
        "        elapsed_time = timeit.default_timer() - start_time\n",
        "        print(f\"compute_train_recall ndcg time {elapsed_time}\")\n",
        "\n",
        "        start_time = timeit.default_timer()\n",
        "        recall_at_k_test = compute_recall_at_k(validation_graph, model, K)\n",
        "        elapsed_time = timeit.default_timer() - start_time\n",
        "        print(f\"compute test recall ndcg time {elapsed_time}\")\n",
        "        \n",
        "        train_recall.append(recall_at_k_train)\n",
        "        test_recall.append(recall_at_k_test)\n",
        "\n",
        "        global_time = epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE\n",
        "        writer.add_scalar(\"NDCG/val train\", mean_ndcg_train, global_time)\n",
        "        writer.add_scalar(\"NDCG/val test\", mean_ndcg_test, global_time)\n",
        "\n",
        "        writer.add_scalar(\"Recall@K/val train\", recall_at_k_train, global_time)\n",
        "        writer.add_scalar(\"Recall@K/val test\", recall_at_k_test, global_time)\n",
        "\n",
        "    if (epoch % 10 == 0):\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "        # save the model and the graphs so far\n",
        "        np.save(f\"{output_path}train_ndcg.npy\", train_ndcg)\n",
        "        np.save(f\"{output_path}test_ndcg.npy\", test_ndcg)\n",
        "\n",
        "        np.save(f\"{output_path}train_recall.npy\", train_recall)\n",
        "        np.save(f\"{output_path}test_recall.npy\", test_recall)\n",
        "\n",
        "        torch.save(model.state_dict(), f\"{output_path}model.pt\")\n",
        "\n",
        "        plt.plot(np.arange(0, len(loss_run)), loss_run)\n",
        "        plt.title(\"Loss\")\n",
        "        plt.savefig(f\"{output_path}/loss.png\")\n",
        "        plt.show()\n",
        "\n",
        "        # print(train_ndcg)\n",
        "        # print(test_ndcg)\n",
        "        plt.plot(np.arange(0, len(train_ndcg)), train_ndcg)\n",
        "        plt.plot(np.arange(0, len(test_ndcg)), test_ndcg)\n",
        "        plt.title(\"NDCG\")\n",
        "        plt.savefig(f\"{output_path}/ndcg.png\")\n",
        "        plt.show()\n",
        "\n",
        "        # print(train_recall)\n",
        "        # print(test_recall)\n",
        "        plt.plot(np.arange(0, len(test_recall)), test_recall)\n",
        "        plt.plot(np.arange(0, len(train_recall)), train_recall)\n",
        "        plt.title(f\"Recall@{K}\")\n",
        "        plt.savefig(f\"{output_path}/recall.png\")\n",
        "        plt.show()\n",
        "\n",
        "# do a final evaluation and save\n",
        "model.eval()\n",
        "\n",
        "# get the ndcg values\n",
        "mean_ndcg_train = get_train_ndcg(validation_users_total, model, train_edges, train_df, id_to_user, id_to_movie)\n",
        "mean_ndcg_test = get_test_ndcg(validation_users_total, model, validation_edges, validation_df, id_to_user, id_to_movie)\n",
        "\n",
        "train_ndcg.append(mean_ndcg_train)\n",
        "test_ndcg.append(mean_ndcg_test)\n",
        "\n",
        "# get the recall@k values\n",
        "recall_at_k_train = compute_recall_at_k(train_graph, model, K)\n",
        "recall_at_k_test = compute_recall_at_k(validation_graph, model, K)\n",
        "train_recall.append(recall_at_k_train)\n",
        "test_recall.append(recall_at_k_test)\n",
        "\n",
        "global_time = epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE\n",
        "writer.add_scalar(\"NDCG/val train\", mean_ndcg_train, global_time)\n",
        "writer.add_scalar(\"NDCG/val test\", mean_ndcg_test, global_time)\n",
        "\n",
        "writer.add_scalar(\"Recall@K/val train\", recall_at_k_train, global_time)\n",
        "writer.add_scalar(\"Recall@K/val test\", recall_at_k_test, global_time)\n",
        "\n",
        "# save the model and the graphs so far\n",
        "np.save(f\"{output_path}train_ndcg.npy\", train_ndcg)\n",
        "np.save(f\"{output_path}test_ndcg.npy\", test_ndcg)\n",
        "\n",
        "np.save(f\"{output_path}train_recall.npy\", train_recall)\n",
        "np.save(f\"{output_path}test_recall.npy\", test_recall)\n",
        "\n",
        "torch.save(model.state_dict(), f\"{output_path}model.pt\")\n",
        "\n",
        "# print(loss_run)\n",
        "print(f\"Best loss: {np.min(loss_run)}\")\n",
        "print(f\"Best NDCG train: {np.max(train_ndcg)}\")\n",
        "print(f\"Best NDCG test: {np.max(test_ndcg)}\")\n",
        "print(f\"Best Recall@{K} train: {np.max(train_recall)}\")\n",
        "print(f\"Best Recall@{K} test: {np.max(test_recall)}\")\n",
        "\n",
        "plt.plot(np.arange(0, len(loss_run)), loss_run)\n",
        "plt.title(\"Loss\")\n",
        "plt.savefig(f\"{output_path}/loss.png\")\n",
        "plt.show()\n",
        "\n",
        "# print(train_ndcg)\n",
        "# print(test_ndcg)\n",
        "plt.plot(np.arange(0, len(train_ndcg)), train_ndcg)\n",
        "plt.plot(np.arange(0, len(test_ndcg)), test_ndcg)\n",
        "plt.title(\"NDCG\")\n",
        "plt.savefig(f\"{output_path}/ndcg.png\")\n",
        "plt.show()\n",
        "\n",
        "# print(train_recall)\n",
        "# print(test_recall)\n",
        "plt.plot(np.arange(0, len(test_recall)), test_recall)\n",
        "plt.plot(np.arange(0, len(train_recall)), train_recall)\n",
        "plt.title(f\"Recall@{K}\")\n",
        "plt.savefig(f\"{output_path}/recall.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "xHqSzAzEgoHs"
      },
      "outputs": [],
      "source": [
        "def get_test_ndcg(validation_users_total, model, validation_edges, validation_df, id_to_user, id_to_movie):\n",
        "    validation_users = random.sample(validation_users_total, min(1000, len(validation_users_total)))\n",
        "\n",
        "    mean_ndcg = 0\n",
        "    ndcg_scores = []\n",
        "    for user in tqdm(validation_users):\n",
        "        user_id = id_to_user[user]\n",
        "        relevant_reviews = validation_df[validation_df['user_id'] == user_id]\n",
        "        user_validation_edges = validation_edges[:, validation_edges[0] == user]\n",
        "        user_validation_edges = user_validation_edges.to(device)\n",
        "        user_rankings = model(user_validation_edges)\n",
        "        edges_sorted = list(user_validation_edges[1, user_rankings.argsort(descending=True)])\n",
        "        # use validation_df to get the relevances via the movie_id column and the movie_rating column\n",
        "        relevances = []\n",
        "        for edge in edges_sorted:\n",
        "            movie_id = id_to_movie[int(edge)]\n",
        "            if (movie_id in relevant_reviews['movie_id'].values):\n",
        "                relevances.append(relevant_reviews[relevant_reviews['movie_id'] == movie_id]['movie_rating'].values[0])\n",
        "            else:\n",
        "                relevances.append(0)\n",
        "        # calculate the ndcg\n",
        "        ndcg = compute_ndcg_at_k(relevances)\n",
        "        if (math.isnan(ndcg)):\n",
        "            print(relevant_reviews)\n",
        "            input()\n",
        "        mean_ndcg += ndcg\n",
        "        ndcg_scores.append(ndcg)\n",
        "    mean_ndcg = mean_ndcg / len(validation_users)\n",
        "    return mean_ndcg\n",
        "\n",
        "def get_train_ndcg(train_users_total, model, train_edges, train_df, id_to_user, id_to_movie):\n",
        "    train_users = random.sample(train_users_total, min(200, len(train_users_total)))\n",
        "\n",
        "    mean_ndcg = 0\n",
        "    ndcg_scores = []\n",
        "    for user in tqdm(train_users):\n",
        "        user_id = id_to_user[user]\n",
        "        relevant_reviews = train_df[train_df['user_id'] == user_id]\n",
        "        user_train_edges = train_edges[:, train_edges[0] == user]\n",
        "        user_train_edges = user_train_edges.to(device)\n",
        "        user_rankings = model(user_train_edges)\n",
        "        edges_sorted = list(user_train_edges[1, user_rankings.argsort(descending=True)])\n",
        "        # use validation_df to get the relevances via the movie_id column and the movie_rating column\n",
        "\n",
        "        relevances = []\n",
        "        for edge in edges_sorted:\n",
        "            movie_id = id_to_movie[int(edge)]\n",
        "            if (movie_id in relevant_reviews['movie_id'].values):\n",
        "                relevances.append(relevant_reviews[relevant_reviews['movie_id'] == movie_id]['movie_rating'].values[0])\n",
        "            else:\n",
        "                relevances.append(0)\n",
        "\n",
        "        # calculate the ndcg\n",
        "        ndcg = compute_ndcg_at_k(relevances)\n",
        "        if (math.isnan(ndcg)):\n",
        "            print(relevant_reviews)\n",
        "            input()\n",
        "        mean_ndcg += ndcg\n",
        "        ndcg_scores.append(ndcg)\n",
        "    mean_ndcg = mean_ndcg / len(train_users)\n",
        "    return mean_ndcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BJw6t4KNgoHs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXjMltfSiUI5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "chill",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
>>>>>>> f1d9c89 (pls)
}
