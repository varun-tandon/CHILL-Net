{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (4.64.1)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.1-cp310-cp310-macosx_12_0_arm64.whl (8.4 MB)\n",
      "Requirement already satisfied: requests in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (2.28.1)\n",
      "Requirement already satisfied: scipy in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (1.10.0)\n",
      "Collecting psutil>=5.8.0\n",
      "  Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB)\n",
      "Requirement already satisfied: numpy in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (1.24.2)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: pyparsing in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.2-cp310-cp310-macosx_10_9_universal2.whl (17 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, psutil, MarkupSafe, joblib, scikit-learn, jinja2, torch_geometric\n",
      "Successfully installed MarkupSafe-2.1.2 jinja2-3.1.2 joblib-1.2.0 psutil-5.9.4 scikit-learn-1.2.1 threadpoolctl-3.1.0 torch_geometric-2.2.0\n",
      "Collecting torch\n",
      "  Using cached torch-1.13.1-cp310-none-macosx_11_0_arm64.whl (53.2 MB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "Successfully installed torch-1.13.1 typing-extensions-4.5.0\n",
      "Collecting torch_sparse\n",
      "  Using cached torch_sparse-0.6.16-cp310-cp310-macosx_11_0_arm64.whl\n",
      "Requirement already satisfied: scipy in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_sparse) (1.10.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/varun/miniconda3/lib/python3.10/site-packages (from scipy->torch_sparse) (1.24.2)\n",
      "Installing collected packages: torch_sparse\n",
      "Successfully installed torch_sparse-0.6.16\n",
      "Collecting torch_scatter\n",
      "  Using cached torch_scatter-2.1.0.tar.gz (106 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: torch_scatter\n",
      "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l-^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install torch\n",
    "!pip install torch_sparse\n",
    "!pip install torch_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.models.lightgcn import LightGCN\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use the first 10k rows of the data, set to None to use all data\n",
    "AMOUNT_TO_LOAD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1000/63111 [00:01<01:10, 882.12it/s]\n"
     ]
    }
   ],
   "source": [
    "user_reviews_dir = 'user_reviews'\n",
    "user_review_data = dict()\n",
    "\n",
    "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
    "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
    "        break\n",
    "    try:\n",
    "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Empty file: {filename}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
    "\n",
    "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asel82_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(list(user_review_data.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 862.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 525094\n",
      "Validation reviews: 988\n",
      "Test reviews: 988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = []\n",
    "validation_reviews = []\n",
    "test_reviews = []\n",
    "for user_id, reviews in tqdm(user_review_data.items()):\n",
    "    if len(reviews) > 5:\n",
    "        # randomly remove one review from the user's reviews for the test set and one for the validation set\n",
    "        reviews_to_remove = reviews.sample(2)\n",
    "        # test data\n",
    "        test_review_data = reviews_to_remove.iloc[0].to_dict()\n",
    "        test_review_data['user_id'] = user_id\n",
    "        test_reviews.append(test_review_data)\n",
    "        # validation data\n",
    "        validation_review_data = reviews_to_remove.iloc[1].to_dict()\n",
    "        validation_review_data['user_id'] = user_id\n",
    "        validation_reviews.append(validation_review_data)\n",
    "        # train data\n",
    "        train_review_data = reviews.drop(reviews_to_remove.index).to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "    else:\n",
    "        # if the user has less than 5 reviews, we will use all of them for training\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "\n",
    "print(f'Train reviews: {len(train_reviews)}')\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_title': 'All Too Well: The Short Film',\n",
       " 'movie_rating': 4.0,\n",
       " 'movie_id': 807762,\n",
       " 'film_slug': '/film/all-too-well-the-short-film/',\n",
       " 'user_id': 'asel82_reviews.csv'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Now that we have the training data, let's construct the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train users: 1000\n",
      "Number of train items: 50800\n",
      "Number of nodes: 51800\n"
     ]
    }
   ],
   "source": [
    "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
    "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
    "num_nodes = num_train_users + num_train_items\n",
    "print(f'Number of train users: {num_train_users}')\n",
    "print(f'Number of train items: {num_train_items}')\n",
    "print(f'Number of nodes: {num_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map users to ids\n",
    "user_to_id = dict()\n",
    "for i, user_id in enumerate(set([review['user_id'] for review in train_reviews])):\n",
    "    user_to_id[user_id] = i\n",
    "\n",
    "# Let's map movies to ids\n",
    "movie_to_id = dict()\n",
    "for i, movie_id in enumerate(set([review['movie_id'] for review in train_reviews])):\n",
    "    movie_to_id[movie_id] = i + num_train_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation reviews: 988\n",
      "Test reviews: 988\n",
      "Validation reviews: 961\n",
      "Test reviews: 965\n"
     ]
    }
   ],
   "source": [
    "# Let's remove any data in our validation and test sets that have ids that are not in our training set\n",
    "# Before removal:\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')\n",
    "\n",
    "# Removal\n",
    "validation_reviews = [review for review in validation_reviews if review['user_id'] in user_to_id and review['movie_id'] in movie_to_id]\n",
    "test_reviews = [review for review in test_reviews if review['user_id'] in user_to_id and review['movie_id'] in movie_to_id]\n",
    "\n",
    "# After removal:\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert_review_to_edge(review):\n",
    "    user_id = user_to_id[review['user_id']]\n",
    "    movie_id = movie_to_id[review['movie_id']]\n",
    "    edge_weight = review['movie_rating']\n",
    "    if (edge_weight < 3.5 and edge_weight > 2.5):\n",
    "        return None, None\n",
    "    edge = (user_id, movie_id)\n",
    "    edge_weight = review['movie_rating']\n",
    "    return edge, edge_weight\n",
    "\n",
    "def shuffle_edges_and_edge_weights(edges, edge_weights):\n",
    "    c = list(zip(edges, edge_weights))\n",
    "    random.shuffle(c)\n",
    "    return zip(*c)\n",
    "\n",
    "def convert_reviews_to_edges(reviews):\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    for review in tqdm(reviews):\n",
    "        edge, edge_weight = convert_review_to_edge(review)\n",
    "        if edge is not None:\n",
    "            edges.append(edge)\n",
    "            edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Reformat the edges to be a tensor\n",
    "    edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edges, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525094/525094 [00:00<00:00, 2028102.64it/s]\n",
      "100%|██████████| 961/961 [00:00<00:00, 315912.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 443343\n",
      "Validation edges: 840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the edges between users and movies.\n",
    "# The id of the user will be the index of the user in the user_to_id dict\n",
    "# The id of the movie will be the index of the movie in the movie_to_id dict + the number of users\n",
    "\n",
    "train_edges, train_edge_weights = convert_reviews_to_edges(train_reviews)\n",
    "validation_edges, validation_edge_weights = convert_reviews_to_edges(validation_reviews)\n",
    "\n",
    "print(f'Train edges: {train_edges.shape[1]}')\n",
    "print(f'Validation edges: {validation_edges.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as data\n",
    "\n",
    "# create the graph\n",
    "train_graph = data.Data(\n",
    "    edge_index=train_edges,\n",
    "    edge_attr=torch.tensor(train_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "validation_graph = data.Data(\n",
    "    edge_index=validation_edges,\n",
    "    edge_attr=torch.tensor(validation_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.validate(raise_on_error=True)\n",
    "validation_graph.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_edges(positive_edges, negative_edges):\n",
    "    \"\"\"If the positive edges and negative edges are not the same length, resample the one that has more edges\"\"\"\n",
    "    if positive_edges.shape[1] > negative_edges.shape[1]:\n",
    "        positive_edges = positive_edges[:, torch.randperm(positive_edges.shape[1])[:negative_edges.shape[1]]]\n",
    "    elif negative_edges.shape[1] > positive_edges.shape[1]:\n",
    "        negative_edges = negative_edges[:, torch.randperm(negative_edges.shape[1])[:positive_edges.shape[1]]]\n",
    "    return positive_edges, negative_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put this on tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 7.340400316024898e-06\n",
      "Epoch: 0, Validation Loss: 0.005429787095636129\n",
      "Epoch: 1, Train Loss: 7.3398482527409215e-06\n",
      "Epoch: 1, Validation Loss: 0.005429391749203205\n",
      "Epoch: 2, Train Loss: 7.3392984631937e-06\n",
      "Epoch: 2, Validation Loss: 0.005428981501609087\n",
      "Epoch: 3, Train Loss: 7.338750947383232e-06\n",
      "Epoch: 3, Validation Loss: 0.005428567994385958\n",
      "Epoch: 4, Train Loss: 7.3382057053095195e-06\n",
      "Epoch: 4, Validation Loss: 0.00542816985398531\n",
      "Epoch: 5, Train Loss: 7.3376609179831576e-06\n",
      "Epoch: 5, Validation Loss: 0.005427775904536247\n",
      "Epoch: 6, Train Loss: 7.337119313888252e-06\n",
      "Epoch: 6, Validation Loss: 0.005427377298474312\n",
      "Epoch: 7, Train Loss: 7.336578619288048e-06\n",
      "Epoch: 7, Validation Loss: 0.0054269772954285145\n",
      "Epoch: 8, Train Loss: 7.336039743677247e-06\n",
      "Epoch: 8, Validation Loss: 0.005426575429737568\n",
      "Epoch: 9, Train Loss: 7.33550268705585e-06\n",
      "Epoch: 9, Validation Loss: 0.005426190793514252\n",
      "Epoch: 10, Train Loss: 7.334967904171208e-06\n",
      "Epoch: 10, Validation Loss: 0.005425796844065189\n",
      "Epoch: 11, Train Loss: 7.33443539502332e-06\n",
      "Epoch: 11, Validation Loss: 0.005425402894616127\n",
      "Epoch: 12, Train Loss: 7.333903340622783e-06\n",
      "Epoch: 12, Validation Loss: 0.00542500289157033\n",
      "Epoch: 13, Train Loss: 7.333374469453702e-06\n",
      "Epoch: 13, Validation Loss: 0.005424617789685726\n",
      "Epoch: 14, Train Loss: 7.3328469625266735e-06\n",
      "Epoch: 14, Validation Loss: 0.005424229893833399\n",
      "Epoch: 15, Train Loss: 7.3323221840837505e-06\n",
      "Epoch: 15, Validation Loss: 0.0054238466545939445\n",
      "Epoch: 16, Train Loss: 7.331797860388178e-06\n",
      "Epoch: 16, Validation Loss: 0.005423457361757755\n",
      "Epoch: 17, Train Loss: 7.331277174671413e-06\n",
      "Epoch: 17, Validation Loss: 0.005423073656857014\n",
      "Epoch: 18, Train Loss: 7.330755124712596e-06\n",
      "Epoch: 18, Validation Loss: 0.005422686226665974\n",
      "Epoch: 19, Train Loss: 7.330240350711392e-06\n",
      "Epoch: 19, Validation Loss: 0.005422299262136221\n",
      "Epoch: 20, Train Loss: 7.3297228482260834e-06\n",
      "Epoch: 20, Validation Loss: 0.005421927664428949\n",
      "Epoch: 21, Train Loss: 7.329208983719582e-06\n",
      "Epoch: 21, Validation Loss: 0.005421547684818506\n",
      "Epoch: 22, Train Loss: 7.328696938202484e-06\n",
      "Epoch: 22, Validation Loss: 0.005421176087111235\n",
      "Epoch: 23, Train Loss: 7.328187621169491e-06\n",
      "Epoch: 23, Validation Loss: 0.005420795641839504\n",
      "Epoch: 24, Train Loss: 7.3276792136312e-06\n",
      "Epoch: 24, Validation Loss: 0.005420419853180647\n",
      "Epoch: 25, Train Loss: 7.327173534577014e-06\n",
      "Epoch: 25, Validation Loss: 0.005420043598860502\n",
      "Epoch: 26, Train Loss: 7.326669219764881e-06\n",
      "Epoch: 26, Validation Loss: 0.005419673398137093\n",
      "Epoch: 27, Train Loss: 7.326167633436853e-06\n",
      "Epoch: 27, Validation Loss: 0.005419308319687843\n",
      "Epoch: 28, Train Loss: 7.325667866098229e-06\n",
      "Epoch: 28, Validation Loss: 0.005418939981609583\n",
      "Epoch: 29, Train Loss: 7.3251685535069555e-06\n",
      "Epoch: 29, Validation Loss: 0.0054185763001441956\n",
      "Epoch: 30, Train Loss: 7.32467333364184e-06\n",
      "Epoch: 30, Validation Loss: 0.005418201442807913\n",
      "Epoch: 31, Train Loss: 7.324178568524076e-06\n",
      "Epoch: 31, Validation Loss: 0.005417839623987675\n",
      "Epoch: 32, Train Loss: 7.323687441385118e-06\n",
      "Epoch: 32, Validation Loss: 0.005417480133473873\n",
      "Epoch: 33, Train Loss: 7.323196768993512e-06\n",
      "Epoch: 33, Validation Loss: 0.0054171159863471985\n",
      "Epoch: 34, Train Loss: 7.32270837033866e-06\n",
      "Epoch: 34, Validation Loss: 0.0054167588241398335\n",
      "Epoch: 35, Train Loss: 7.322222700167913e-06\n",
      "Epoch: 35, Validation Loss: 0.005416400730609894\n",
      "Epoch: 36, Train Loss: 7.321737939491868e-06\n",
      "Epoch: 36, Validation Loss: 0.00541603984311223\n",
      "Epoch: 37, Train Loss: 7.321256362047279e-06\n",
      "Epoch: 37, Validation Loss: 0.005415697116404772\n",
      "Epoch: 38, Train Loss: 7.320776148844743e-06\n",
      "Epoch: 38, Validation Loss: 0.005415328312665224\n",
      "Epoch: 39, Train Loss: 7.320297754631611e-06\n",
      "Epoch: 39, Validation Loss: 0.005414990708231926\n",
      "Epoch: 40, Train Loss: 7.3198216341552325e-06\n",
      "Epoch: 40, Validation Loss: 0.00541462330147624\n",
      "Epoch: 41, Train Loss: 7.319347332668258e-06\n",
      "Epoch: 41, Validation Loss: 0.005414280109107494\n",
      "Epoch: 42, Train Loss: 7.318873940675985e-06\n",
      "Epoch: 42, Validation Loss: 0.005413942039012909\n",
      "Epoch: 43, Train Loss: 7.318404186662519e-06\n",
      "Epoch: 43, Validation Loss: 0.005413586273789406\n",
      "Epoch: 44, Train Loss: 7.317935796891106e-06\n",
      "Epoch: 44, Validation Loss: 0.005413238424807787\n",
      "Epoch: 45, Train Loss: 7.317468771361746e-06\n",
      "Epoch: 45, Validation Loss: 0.005412900820374489\n",
      "Epoch: 46, Train Loss: 7.317004929063842e-06\n",
      "Epoch: 46, Validation Loss: 0.005412558559328318\n",
      "Epoch: 47, Train Loss: 7.316541541513288e-06\n",
      "Epoch: 47, Validation Loss: 0.0054122088477015495\n",
      "Epoch: 48, Train Loss: 7.3160804276994895e-06\n",
      "Epoch: 48, Validation Loss: 0.005411874037235975\n",
      "Epoch: 49, Train Loss: 7.315622042369796e-06\n",
      "Epoch: 49, Validation Loss: 0.0054115294478833675\n",
      "Epoch: 50, Train Loss: 7.315165021282155e-06\n",
      "Epoch: 50, Validation Loss: 0.005411196034401655\n",
      "Epoch: 51, Train Loss: 7.314710273931269e-06\n",
      "Epoch: 51, Validation Loss: 0.0054108635522425175\n",
      "Epoch: 52, Train Loss: 7.314256436075084e-06\n",
      "Epoch: 52, Validation Loss: 0.005410525016486645\n",
      "Epoch: 53, Train Loss: 7.3138057814503554e-06\n",
      "Epoch: 53, Validation Loss: 0.005410199053585529\n",
      "Epoch: 54, Train Loss: 7.313356036320329e-06\n",
      "Epoch: 54, Validation Loss: 0.005409866105765104\n",
      "Epoch: 55, Train Loss: 7.3129085649270564e-06\n",
      "Epoch: 55, Validation Loss: 0.005409544333815575\n",
      "Epoch: 56, Train Loss: 7.312463367270539e-06\n",
      "Epoch: 56, Validation Loss: 0.005409207660704851\n",
      "Epoch: 57, Train Loss: 7.3120190791087225e-06\n",
      "Epoch: 57, Validation Loss: 0.005408890545368195\n",
      "Epoch: 58, Train Loss: 7.311577064683661e-06\n",
      "Epoch: 58, Validation Loss: 0.005408561322838068\n",
      "Epoch: 59, Train Loss: 7.311137323995354e-06\n",
      "Epoch: 59, Validation Loss: 0.005408237222582102\n",
      "Epoch: 60, Train Loss: 7.310698947549099e-06\n",
      "Epoch: 60, Validation Loss: 0.005407906137406826\n",
      "Epoch: 61, Train Loss: 7.31026329958695e-06\n",
      "Epoch: 61, Validation Loss: 0.005407592747360468\n",
      "Epoch: 62, Train Loss: 7.309829470614204e-06\n",
      "Epoch: 62, Validation Loss: 0.005407280288636684\n",
      "Epoch: 63, Train Loss: 7.309397460630862e-06\n",
      "Epoch: 63, Validation Loss: 0.005406948737800121\n",
      "Epoch: 64, Train Loss: 7.308963631658116e-06\n",
      "Epoch: 64, Validation Loss: 0.005406639073044062\n",
      "Epoch: 65, Train Loss: 7.308536623895634e-06\n",
      "Epoch: 65, Validation Loss: 0.005406314507126808\n",
      "Epoch: 66, Train Loss: 7.308109161385801e-06\n",
      "Epoch: 66, Validation Loss: 0.0054060001857578754\n",
      "Epoch: 67, Train Loss: 7.307683972612722e-06\n",
      "Epoch: 67, Validation Loss: 0.005405687727034092\n",
      "Epoch: 68, Train Loss: 7.3072610575763974e-06\n",
      "Epoch: 68, Validation Loss: 0.005405377130955458\n",
      "Epoch: 69, Train Loss: 7.306838597287424e-06\n",
      "Epoch: 69, Validation Loss: 0.005405065603554249\n",
      "Epoch: 70, Train Loss: 7.306418865482556e-06\n",
      "Epoch: 70, Validation Loss: 0.005404764786362648\n",
      "Epoch: 71, Train Loss: 7.30600049791974e-06\n",
      "Epoch: 71, Validation Loss: 0.005404444877058268\n",
      "Epoch: 72, Train Loss: 7.305583494598977e-06\n",
      "Epoch: 72, Validation Loss: 0.00540414173156023\n",
      "Epoch: 73, Train Loss: 7.30516921976232e-06\n",
      "Epoch: 73, Validation Loss: 0.005403839983046055\n",
      "Epoch: 74, Train Loss: 7.304756309167715e-06\n",
      "Epoch: 74, Validation Loss: 0.005403534509241581\n",
      "Epoch: 75, Train Loss: 7.3043452175625134e-06\n",
      "Epoch: 75, Validation Loss: 0.005403231829404831\n",
      "Epoch: 76, Train Loss: 7.3039363996940665e-06\n",
      "Epoch: 76, Validation Loss: 0.005402929615229368\n",
      "Epoch: 77, Train Loss: 7.3035275818256196e-06\n",
      "Epoch: 77, Validation Loss: 0.005402625072747469\n",
      "Epoch: 78, Train Loss: 7.303121947188629e-06\n",
      "Epoch: 78, Validation Loss: 0.005402327515184879\n",
      "Epoch: 79, Train Loss: 7.30271722204634e-06\n",
      "Epoch: 79, Validation Loss: 0.005402030423283577\n",
      "Epoch: 80, Train Loss: 7.302314770640805e-06\n",
      "Epoch: 80, Validation Loss: 0.005401735659688711\n",
      "Epoch: 81, Train Loss: 7.301914138224674e-06\n",
      "Epoch: 81, Validation Loss: 0.005401443224400282\n",
      "Epoch: 82, Train Loss: 7.301515779545298e-06\n",
      "Epoch: 82, Validation Loss: 0.005401144735515118\n",
      "Epoch: 83, Train Loss: 7.30111696611857e-06\n",
      "Epoch: 83, Validation Loss: 0.00540085369721055\n",
      "Epoch: 84, Train Loss: 7.30072179067065e-06\n",
      "Epoch: 84, Validation Loss: 0.005400563590228558\n",
      "Epoch: 85, Train Loss: 7.300327069970081e-06\n",
      "Epoch: 85, Validation Loss: 0.005400268826633692\n",
      "Epoch: 86, Train Loss: 7.299934623006266e-06\n",
      "Epoch: 86, Validation Loss: 0.0053999824449419975\n",
      "Epoch: 87, Train Loss: 7.299543995031854e-06\n",
      "Epoch: 87, Validation Loss: 0.0053996918722987175\n",
      "Epoch: 88, Train Loss: 7.2991542765521444e-06\n",
      "Epoch: 88, Validation Loss: 0.005399405490607023\n",
      "Epoch: 89, Train Loss: 7.298766831809189e-06\n",
      "Epoch: 89, Validation Loss: 0.005399120040237904\n",
      "Epoch: 90, Train Loss: 7.298382115550339e-06\n",
      "Epoch: 90, Validation Loss: 0.005398829001933336\n",
      "Epoch: 91, Train Loss: 7.297996944544138e-06\n",
      "Epoch: 91, Validation Loss: 0.005398547276854515\n",
      "Epoch: 92, Train Loss: 7.297614047274692e-06\n",
      "Epoch: 92, Validation Loss: 0.005398266017436981\n",
      "Epoch: 93, Train Loss: 7.297233423742e-06\n",
      "Epoch: 93, Validation Loss: 0.005397988483309746\n",
      "Epoch: 94, Train Loss: 7.29685370970401e-06\n",
      "Epoch: 94, Validation Loss: 0.005397710017859936\n",
      "Epoch: 95, Train Loss: 7.296476269402774e-06\n",
      "Epoch: 95, Validation Loss: 0.005397433880716562\n",
      "Epoch: 96, Train Loss: 7.296100193343591e-06\n",
      "Epoch: 96, Validation Loss: 0.0053971512243151665\n",
      "Epoch: 97, Train Loss: 7.29572502677911e-06\n",
      "Epoch: 97, Validation Loss: 0.005396874621510506\n",
      "Epoch: 98, Train Loss: 7.295352588698734e-06\n",
      "Epoch: 98, Validation Loss: 0.005396605934947729\n",
      "Epoch: 99, Train Loss: 7.294980150618358e-06\n",
      "Epoch: 99, Validation Loss: 0.0053963311947882175\n",
      "Epoch: 100, Train Loss: 7.294609531527385e-06\n",
      "Epoch: 100, Validation Loss: 0.005396069493144751\n",
      "Epoch: 101, Train Loss: 7.294243005162571e-06\n",
      "Epoch: 101, Validation Loss: 0.005395791493356228\n",
      "Epoch: 102, Train Loss: 7.293875569303054e-06\n",
      "Epoch: 102, Validation Loss: 0.005395511165261269\n",
      "Epoch: 103, Train Loss: 7.293509952432942e-06\n",
      "Epoch: 103, Validation Loss: 0.005395246669650078\n",
      "Epoch: 104, Train Loss: 7.293146609299583e-06\n",
      "Epoch: 104, Validation Loss: 0.005394983571022749\n",
      "Epoch: 105, Train Loss: 7.292784630408278e-06\n",
      "Epoch: 105, Validation Loss: 0.005394710227847099\n",
      "Epoch: 106, Train Loss: 7.292422651516972e-06\n",
      "Epoch: 106, Validation Loss: 0.00539444899186492\n",
      "Epoch: 107, Train Loss: 7.2920647653518245e-06\n",
      "Epoch: 107, Validation Loss: 0.005394181702286005\n",
      "Epoch: 108, Train Loss: 7.291706879186677e-06\n",
      "Epoch: 108, Validation Loss: 0.005393912550061941\n",
      "Epoch: 109, Train Loss: 7.291351266758284e-06\n",
      "Epoch: 109, Validation Loss: 0.00539366016164422\n",
      "Epoch: 110, Train Loss: 7.290997473319294e-06\n",
      "Epoch: 110, Validation Loss: 0.005393398460000753\n",
      "Epoch: 111, Train Loss: 7.290644134627655e-06\n",
      "Epoch: 111, Validation Loss: 0.0053931367583572865\n",
      "Epoch: 112, Train Loss: 7.290291250683367e-06\n",
      "Epoch: 112, Validation Loss: 0.005392875988036394\n",
      "Epoch: 113, Train Loss: 7.2899420047178864e-06\n",
      "Epoch: 113, Validation Loss: 0.0053926194086670876\n",
      "Epoch: 114, Train Loss: 7.289593668247107e-06\n",
      "Epoch: 114, Validation Loss: 0.005392370745539665\n",
      "Epoch: 115, Train Loss: 7.28924624127103e-06\n",
      "Epoch: 115, Validation Loss: 0.005392108578234911\n",
      "Epoch: 116, Train Loss: 7.2889006332843564e-06\n",
      "Epoch: 116, Validation Loss: 0.005391851998865604\n",
      "Epoch: 117, Train Loss: 7.288556389539735e-06\n",
      "Epoch: 117, Validation Loss: 0.005391599610447884\n",
      "Epoch: 118, Train Loss: 7.288213964784518e-06\n",
      "Epoch: 118, Validation Loss: 0.005391344428062439\n",
      "Epoch: 119, Train Loss: 7.287872449524002e-06\n",
      "Epoch: 119, Validation Loss: 0.0053910925053060055\n",
      "Epoch: 120, Train Loss: 7.287533208000241e-06\n",
      "Epoch: 120, Validation Loss: 0.005390851758420467\n",
      "Epoch: 121, Train Loss: 7.2871944212238304e-06\n",
      "Epoch: 121, Validation Loss: 0.00539059191942215\n",
      "Epoch: 122, Train Loss: 7.286856998689473e-06\n",
      "Epoch: 122, Validation Loss: 0.005390344187617302\n",
      "Epoch: 123, Train Loss: 7.286521849891869e-06\n",
      "Epoch: 123, Validation Loss: 0.005390097852796316\n",
      "Epoch: 124, Train Loss: 7.286187610588968e-06\n",
      "Epoch: 124, Validation Loss: 0.0053898547776043415\n",
      "Epoch: 125, Train Loss: 7.285854280780768e-06\n",
      "Epoch: 125, Validation Loss: 0.005389607045799494\n",
      "Epoch: 126, Train Loss: 7.2855241342040244e-06\n",
      "Epoch: 126, Validation Loss: 0.005389355588704348\n",
      "Epoch: 127, Train Loss: 7.28519353287993e-06\n",
      "Epoch: 127, Validation Loss: 0.005389124155044556\n",
      "Epoch: 128, Train Loss: 7.28486520529259e-06\n",
      "Epoch: 128, Validation Loss: 0.005388879682868719\n",
      "Epoch: 129, Train Loss: 7.284537787199952e-06\n",
      "Epoch: 129, Validation Loss: 0.005388637538999319\n",
      "Epoch: 130, Train Loss: 7.284212642844068e-06\n",
      "Epoch: 130, Validation Loss: 0.0053883944638073444\n",
      "Epoch: 131, Train Loss: 7.283887953235535e-06\n",
      "Epoch: 131, Validation Loss: 0.005388162564486265\n",
      "Epoch: 132, Train Loss: 7.2835655373637564e-06\n",
      "Epoch: 132, Validation Loss: 0.005387918092310429\n",
      "Epoch: 133, Train Loss: 7.28324403098668e-06\n",
      "Epoch: 133, Validation Loss: 0.005387682002037764\n",
      "Epoch: 134, Train Loss: 7.282922979356954e-06\n",
      "Epoch: 134, Validation Loss: 0.005387448240071535\n",
      "Epoch: 135, Train Loss: 7.2826042014639825e-06\n",
      "Epoch: 135, Validation Loss: 0.005387214012444019\n",
      "Epoch: 136, Train Loss: 7.282286787813064e-06\n",
      "Epoch: 136, Validation Loss: 0.005386972799897194\n",
      "Epoch: 137, Train Loss: 7.281970283656847e-06\n",
      "Epoch: 137, Validation Loss: 0.00538674695417285\n",
      "Epoch: 138, Train Loss: 7.2816546889953315e-06\n",
      "Epoch: 138, Validation Loss: 0.005386508069932461\n",
      "Epoch: 139, Train Loss: 7.281341822817922e-06\n",
      "Epoch: 139, Validation Loss: 0.005386285949498415\n",
      "Epoch: 140, Train Loss: 7.281029411387863e-06\n",
      "Epoch: 140, Validation Loss: 0.005386053584516048\n",
      "Epoch: 141, Train Loss: 7.280718818947207e-06\n",
      "Epoch: 141, Validation Loss: 0.00538582494482398\n",
      "Epoch: 142, Train Loss: 7.2804091360012535e-06\n",
      "Epoch: 142, Validation Loss: 0.005385595373809338\n",
      "Epoch: 143, Train Loss: 7.2801008172973525e-06\n",
      "Epoch: 143, Validation Loss: 0.005385373719036579\n",
      "Epoch: 144, Train Loss: 7.279794317582855e-06\n",
      "Epoch: 144, Validation Loss: 0.005385139491409063\n",
      "Epoch: 145, Train Loss: 7.2794878178683575e-06\n",
      "Epoch: 145, Validation Loss: 0.00538492389023304\n",
      "Epoch: 146, Train Loss: 7.2791835918906145e-06\n",
      "Epoch: 146, Validation Loss: 0.0053846887312829494\n",
      "Epoch: 147, Train Loss: 7.278880730154924e-06\n",
      "Epoch: 147, Validation Loss: 0.005384472664445639\n",
      "Epoch: 148, Train Loss: 7.278579232661286e-06\n",
      "Epoch: 148, Validation Loss: 0.005384248681366444\n",
      "Epoch: 149, Train Loss: 7.27827864466235e-06\n",
      "Epoch: 149, Validation Loss: 0.0053840321488678455\n",
      "Epoch: 150, Train Loss: 7.277979420905467e-06\n",
      "Epoch: 150, Validation Loss: 0.0053838035091757774\n",
      "Epoch: 151, Train Loss: 7.277682016137987e-06\n",
      "Epoch: 151, Validation Loss: 0.005383588373661041\n",
      "Epoch: 152, Train Loss: 7.2773850661178585e-06\n",
      "Epoch: 152, Validation Loss: 0.00538336718454957\n",
      "Epoch: 153, Train Loss: 7.277089480339782e-06\n",
      "Epoch: 153, Validation Loss: 0.005383150652050972\n",
      "Epoch: 154, Train Loss: 7.276794804056408e-06\n",
      "Epoch: 154, Validation Loss: 0.005382936913520098\n",
      "Epoch: 155, Train Loss: 7.276501946762437e-06\n",
      "Epoch: 155, Validation Loss: 0.005382719449698925\n",
      "Epoch: 156, Train Loss: 7.276209998963168e-06\n",
      "Epoch: 156, Validation Loss: 0.005382501054555178\n",
      "Epoch: 157, Train Loss: 7.275919870153302e-06\n",
      "Epoch: 157, Validation Loss: 0.0053822873160243034\n",
      "Epoch: 158, Train Loss: 7.275630196090788e-06\n",
      "Epoch: 158, Validation Loss: 0.0053820800967514515\n",
      "Epoch: 159, Train Loss: 7.2753423410176765e-06\n",
      "Epoch: 159, Validation Loss: 0.00538186589255929\n",
      "Epoch: 160, Train Loss: 7.275055395439267e-06\n",
      "Epoch: 160, Validation Loss: 0.005381652619689703\n",
      "Epoch: 161, Train Loss: 7.2747693593555596e-06\n",
      "Epoch: 161, Validation Loss: 0.005381444469094276\n",
      "Epoch: 162, Train Loss: 7.274485597008606e-06\n",
      "Epoch: 162, Validation Loss: 0.005381230264902115\n",
      "Epoch: 163, Train Loss: 7.274201834661653e-06\n",
      "Epoch: 163, Validation Loss: 0.005381018854677677\n",
      "Epoch: 164, Train Loss: 7.273921255546156e-06\n",
      "Epoch: 164, Validation Loss: 0.005380816757678986\n",
      "Epoch: 165, Train Loss: 7.273638857441256e-06\n",
      "Epoch: 165, Validation Loss: 0.005380605347454548\n",
      "Epoch: 166, Train Loss: 7.273360097315162e-06\n",
      "Epoch: 166, Validation Loss: 0.005380404181778431\n",
      "Epoch: 167, Train Loss: 7.273081337189069e-06\n",
      "Epoch: 167, Validation Loss: 0.005380195565521717\n",
      "Epoch: 168, Train Loss: 7.272803486557677e-06\n",
      "Epoch: 168, Validation Loss: 0.005379996728152037\n",
      "Epoch: 169, Train Loss: 7.27252790966304e-06\n",
      "Epoch: 169, Validation Loss: 0.005379789043217897\n",
      "Epoch: 170, Train Loss: 7.272252787515754e-06\n",
      "Epoch: 170, Validation Loss: 0.0053795878775417805\n",
      "Epoch: 171, Train Loss: 7.27197902961052e-06\n",
      "Epoch: 171, Validation Loss: 0.005379383452236652\n",
      "Epoch: 172, Train Loss: 7.271706635947339e-06\n",
      "Epoch: 172, Validation Loss: 0.005379186477512121\n",
      "Epoch: 173, Train Loss: 7.271434242284158e-06\n",
      "Epoch: 173, Validation Loss: 0.005378983914852142\n",
      "Epoch: 174, Train Loss: 7.271164122357732e-06\n",
      "Epoch: 174, Validation Loss: 0.005378785543143749\n",
      "Epoch: 175, Train Loss: 7.270895366673358e-06\n",
      "Epoch: 175, Validation Loss: 0.005378588102757931\n",
      "Epoch: 176, Train Loss: 7.270627520483686e-06\n",
      "Epoch: 176, Validation Loss: 0.005378386937081814\n",
      "Epoch: 177, Train Loss: 7.2703596742940135e-06\n",
      "Epoch: 177, Validation Loss: 0.005378193687647581\n",
      "Epoch: 178, Train Loss: 7.270094556588447e-06\n",
      "Epoch: 178, Validation Loss: 0.005377996247261763\n",
      "Epoch: 179, Train Loss: 7.26982943888288e-06\n",
      "Epoch: 179, Validation Loss: 0.005377800669521093\n",
      "Epoch: 180, Train Loss: 7.269565685419366e-06\n",
      "Epoch: 180, Validation Loss: 0.005377613473683596\n",
      "Epoch: 181, Train Loss: 7.269303750945255e-06\n",
      "Epoch: 181, Validation Loss: 0.005377419292926788\n",
      "Epoch: 182, Train Loss: 7.2690422712184954e-06\n",
      "Epoch: 182, Validation Loss: 0.00537722697481513\n",
      "Epoch: 183, Train Loss: 7.2687817009864375e-06\n",
      "Epoch: 183, Validation Loss: 0.005377032328397036\n",
      "Epoch: 184, Train Loss: 7.268522040249081e-06\n",
      "Epoch: 184, Validation Loss: 0.005376838613301516\n",
      "Epoch: 185, Train Loss: 7.2682651079958305e-06\n",
      "Epoch: 185, Validation Loss: 0.005376649089157581\n",
      "Epoch: 186, Train Loss: 7.268007266247878e-06\n",
      "Epoch: 186, Validation Loss: 0.005376462824642658\n",
      "Epoch: 187, Train Loss: 7.26775169823668e-06\n",
      "Epoch: 187, Validation Loss: 0.005376270506531\n",
      "Epoch: 188, Train Loss: 7.2674961302254815e-06\n",
      "Epoch: 188, Validation Loss: 0.0053760819137096405\n",
      "Epoch: 189, Train Loss: 7.267242835951038e-06\n",
      "Epoch: 189, Validation Loss: 0.0053758989088237286\n",
      "Epoch: 190, Train Loss: 7.266990451171296e-06\n",
      "Epoch: 190, Validation Loss: 0.0053757065907120705\n",
      "Epoch: 191, Train Loss: 7.266739430633606e-06\n",
      "Epoch: 191, Validation Loss: 0.005375525448471308\n",
      "Epoch: 192, Train Loss: 7.266488410095917e-06\n",
      "Epoch: 192, Validation Loss: 0.005375345703214407\n",
      "Epoch: 193, Train Loss: 7.26623875380028e-06\n",
      "Epoch: 193, Validation Loss: 0.0053751603700220585\n",
      "Epoch: 194, Train Loss: 7.265990916494047e-06\n",
      "Epoch: 194, Validation Loss: 0.005374977830797434\n",
      "Epoch: 195, Train Loss: 7.265743079187814e-06\n",
      "Epoch: 195, Validation Loss: 0.005374792497605085\n",
      "Epoch: 196, Train Loss: 7.2654970608709846e-06\n",
      "Epoch: 196, Validation Loss: 0.005374607630074024\n",
      "Epoch: 197, Train Loss: 7.265251497301506e-06\n",
      "Epoch: 197, Validation Loss: 0.005374428816139698\n",
      "Epoch: 198, Train Loss: 7.265007752721431e-06\n",
      "Epoch: 198, Validation Loss: 0.005374250002205372\n",
      "Epoch: 199, Train Loss: 7.264764917636057e-06\n",
      "Epoch: 199, Validation Loss: 0.005374070256948471\n",
      "Epoch: 200, Train Loss: 7.264522537298035e-06\n",
      "Epoch: 200, Validation Loss: 0.00537389749661088\n",
      "Epoch: 201, Train Loss: 7.264281521202065e-06\n",
      "Epoch: 201, Validation Loss: 0.005373714957386255\n",
      "Epoch: 202, Train Loss: 7.264040959853446e-06\n",
      "Epoch: 202, Validation Loss: 0.005373538006097078\n",
      "Epoch: 203, Train Loss: 7.26380176274688e-06\n",
      "Epoch: 203, Validation Loss: 0.005373366642743349\n",
      "Epoch: 204, Train Loss: 7.263563929882366e-06\n",
      "Epoch: 204, Validation Loss: 0.0053731841035187244\n",
      "Epoch: 205, Train Loss: 7.2633265517652035e-06\n",
      "Epoch: 205, Validation Loss: 0.005373011808842421\n",
      "Epoch: 206, Train Loss: 7.2630900831427425e-06\n",
      "Epoch: 206, Validation Loss: 0.005372838582843542\n",
      "Epoch: 207, Train Loss: 7.262855888257036e-06\n",
      "Epoch: 207, Validation Loss: 0.005372661165893078\n",
      "Epoch: 208, Train Loss: 7.2626216933713295e-06\n",
      "Epoch: 208, Validation Loss: 0.0053724925965070724\n",
      "Epoch: 209, Train Loss: 7.262388862727676e-06\n",
      "Epoch: 209, Validation Loss: 0.005372321233153343\n",
      "Epoch: 210, Train Loss: 7.262156486831373e-06\n",
      "Epoch: 210, Validation Loss: 0.005372145213186741\n",
      "Epoch: 211, Train Loss: 7.261925020429771e-06\n",
      "Epoch: 211, Validation Loss: 0.0053719826973974705\n",
      "Epoch: 212, Train Loss: 7.261694463522872e-06\n",
      "Epoch: 212, Validation Loss: 0.005371809471398592\n",
      "Epoch: 213, Train Loss: 7.261465725605376e-06\n",
      "Epoch: 213, Validation Loss: 0.005371642764657736\n",
      "Epoch: 214, Train Loss: 7.261237897182582e-06\n",
      "Epoch: 214, Validation Loss: 0.005371476523578167\n",
      "Epoch: 215, Train Loss: 7.261010523507139e-06\n",
      "Epoch: 215, Validation Loss: 0.005371303763240576\n",
      "Epoch: 216, Train Loss: 7.260784059326397e-06\n",
      "Epoch: 216, Validation Loss: 0.0053711398504674435\n",
      "Epoch: 217, Train Loss: 7.2605585046403576e-06\n",
      "Epoch: 217, Validation Loss: 0.0053709726780653\n",
      "Epoch: 218, Train Loss: 7.2603343141963705e-06\n",
      "Epoch: 218, Validation Loss: 0.005370810627937317\n",
      "Epoch: 219, Train Loss: 7.260111033247085e-06\n",
      "Epoch: 219, Validation Loss: 0.005370643455535173\n",
      "Epoch: 220, Train Loss: 7.259888661792502e-06\n",
      "Epoch: 220, Validation Loss: 0.005370480008423328\n",
      "Epoch: 221, Train Loss: 7.2596658355905674e-06\n",
      "Epoch: 221, Validation Loss: 0.005370319355279207\n",
      "Epoch: 222, Train Loss: 7.25944664736744e-06\n",
      "Epoch: 222, Validation Loss: 0.005370154045522213\n",
      "Epoch: 223, Train Loss: 7.25922609490226e-06\n",
      "Epoch: 223, Validation Loss: 0.005369989201426506\n",
      "Epoch: 224, Train Loss: 7.259007816173835e-06\n",
      "Epoch: 224, Validation Loss: 0.0053698280826210976\n",
      "Epoch: 225, Train Loss: 7.258790446940111e-06\n",
      "Epoch: 225, Validation Loss: 0.005369666963815689\n",
      "Epoch: 226, Train Loss: 7.258573532453738e-06\n",
      "Epoch: 226, Validation Loss: 0.005369511898607016\n",
      "Epoch: 227, Train Loss: 7.258356617967365e-06\n",
      "Epoch: 227, Validation Loss: 0.005369353108108044\n",
      "Epoch: 228, Train Loss: 7.258141522470396e-06\n",
      "Epoch: 228, Validation Loss: 0.005369189195334911\n",
      "Epoch: 229, Train Loss: 7.25792824596283e-06\n",
      "Epoch: 229, Validation Loss: 0.005369035992771387\n",
      "Epoch: 230, Train Loss: 7.257714514707914e-06\n",
      "Epoch: 230, Validation Loss: 0.005368875805288553\n",
      "Epoch: 231, Train Loss: 7.25750214769505e-06\n",
      "Epoch: 231, Validation Loss: 0.005368721205741167\n",
      "Epoch: 232, Train Loss: 7.257291144924238e-06\n",
      "Epoch: 232, Validation Loss: 0.00536856846883893\n",
      "Epoch: 233, Train Loss: 7.257080142153427e-06\n",
      "Epoch: 233, Validation Loss: 0.005368407815694809\n",
      "Epoch: 234, Train Loss: 7.256870958372019e-06\n",
      "Epoch: 234, Validation Loss: 0.005368260201066732\n",
      "Epoch: 235, Train Loss: 7.2566617745906115e-06\n",
      "Epoch: 235, Validation Loss: 0.005368104204535484\n",
      "Epoch: 236, Train Loss: 7.2564535003039055e-06\n",
      "Epoch: 236, Validation Loss: 0.005367949604988098\n",
      "Epoch: 237, Train Loss: 7.256247045006603e-06\n",
      "Epoch: 237, Validation Loss: 0.005367795936763287\n",
      "Epoch: 238, Train Loss: 7.2560410444566514e-06\n",
      "Epoch: 238, Validation Loss: 0.005367643665522337\n",
      "Epoch: 239, Train Loss: 7.255835953401402e-06\n",
      "Epoch: 239, Validation Loss: 0.005367497447878122\n",
      "Epoch: 240, Train Loss: 7.255631317093503e-06\n",
      "Epoch: 240, Validation Loss: 0.005367343779653311\n",
      "Epoch: 241, Train Loss: 7.255427135532955e-06\n",
      "Epoch: 241, Validation Loss: 0.0053671980276703835\n",
      "Epoch: 242, Train Loss: 7.255225227709161e-06\n",
      "Epoch: 242, Validation Loss: 0.005367044825106859\n",
      "Epoch: 243, Train Loss: 7.2550233198853675e-06\n",
      "Epoch: 243, Validation Loss: 0.00536689255386591\n",
      "Epoch: 244, Train Loss: 7.254821866808925e-06\n",
      "Epoch: 244, Validation Loss: 0.005366744473576546\n",
      "Epoch: 245, Train Loss: 7.2546226874692366e-06\n",
      "Epoch: 245, Validation Loss: 0.005366599652916193\n",
      "Epoch: 246, Train Loss: 7.2544225986348465e-06\n",
      "Epoch: 246, Validation Loss: 0.00536645483225584\n",
      "Epoch: 247, Train Loss: 7.25422432878986e-06\n",
      "Epoch: 247, Validation Loss: 0.005366306286305189\n",
      "Epoch: 248, Train Loss: 7.254026513692224e-06\n",
      "Epoch: 248, Validation Loss: 0.005366160534322262\n",
      "Epoch: 249, Train Loss: 7.2538296080892906e-06\n",
      "Epoch: 249, Validation Loss: 0.005366019904613495\n",
      "Epoch: 250, Train Loss: 7.253634066728409e-06\n",
      "Epoch: 250, Validation Loss: 0.00536587368696928\n",
      "Epoch: 251, Train Loss: 7.253438980114879e-06\n",
      "Epoch: 251, Validation Loss: 0.005365731660276651\n",
      "Epoch: 252, Train Loss: 7.2532452577434015e-06\n",
      "Epoch: 252, Validation Loss: 0.0053655835799872875\n",
      "Epoch: 253, Train Loss: 7.253051080624573e-06\n",
      "Epoch: 253, Validation Loss: 0.005365442950278521\n",
      "Epoch: 254, Train Loss: 7.252858722495148e-06\n",
      "Epoch: 254, Validation Loss: 0.005365300923585892\n",
      "Epoch: 255, Train Loss: 7.252666819113074e-06\n",
      "Epoch: 255, Validation Loss: 0.00536515936255455\n",
      "Epoch: 256, Train Loss: 7.2524758252257016e-06\n",
      "Epoch: 256, Validation Loss: 0.005365020129829645\n",
      "Epoch: 257, Train Loss: 7.252285740833031e-06\n",
      "Epoch: 257, Validation Loss: 0.00536488089710474\n",
      "Epoch: 258, Train Loss: 7.252096565935062e-06\n",
      "Epoch: 258, Validation Loss: 0.005364737939089537\n",
      "Epoch: 259, Train Loss: 7.2519073910370935e-06\n",
      "Epoch: 259, Validation Loss: 0.005364602897316217\n",
      "Epoch: 260, Train Loss: 7.251720489875879e-06\n",
      "Epoch: 260, Validation Loss: 0.005364460404962301\n",
      "Epoch: 261, Train Loss: 7.251533133967314e-06\n",
      "Epoch: 261, Validation Loss: 0.005364323500543833\n",
      "Epoch: 262, Train Loss: 7.2513466875534505e-06\n",
      "Epoch: 262, Validation Loss: 0.005364187527447939\n",
      "Epoch: 263, Train Loss: 7.251161150634289e-06\n",
      "Epoch: 263, Validation Loss: 0.005364051088690758\n",
      "Epoch: 264, Train Loss: 7.250976523209829e-06\n",
      "Epoch: 264, Validation Loss: 0.005363916512578726\n",
      "Epoch: 265, Train Loss: 7.250793260027422e-06\n",
      "Epoch: 265, Validation Loss: 0.0053637768141925335\n",
      "Epoch: 266, Train Loss: 7.2506095420976635e-06\n",
      "Epoch: 266, Validation Loss: 0.005363641772419214\n",
      "Epoch: 267, Train Loss: 7.250427643157309e-06\n",
      "Epoch: 267, Validation Loss: 0.00536351278424263\n",
      "Epoch: 268, Train Loss: 7.250245744216954e-06\n",
      "Epoch: 268, Validation Loss: 0.0053633772768080235\n",
      "Epoch: 269, Train Loss: 7.2500647547713015e-06\n",
      "Epoch: 269, Validation Loss: 0.005363245494663715\n",
      "Epoch: 270, Train Loss: 7.249885129567701e-06\n",
      "Epoch: 270, Validation Loss: 0.005363107658922672\n",
      "Epoch: 271, Train Loss: 7.249705959111452e-06\n",
      "Epoch: 271, Validation Loss: 0.005362977739423513\n",
      "Epoch: 272, Train Loss: 7.249527698149905e-06\n",
      "Epoch: 272, Validation Loss: 0.005362846422940493\n",
      "Epoch: 273, Train Loss: 7.249349891935708e-06\n",
      "Epoch: 273, Validation Loss: 0.005362715106457472\n",
      "Epoch: 274, Train Loss: 7.249172995216213e-06\n",
      "Epoch: 274, Validation Loss: 0.0053625856526196\n",
      "Epoch: 275, Train Loss: 7.248996553244069e-06\n",
      "Epoch: 275, Validation Loss: 0.005362454801797867\n",
      "Epoch: 276, Train Loss: 7.248821020766627e-06\n",
      "Epoch: 276, Validation Loss: 0.005362330004572868\n",
      "Epoch: 277, Train Loss: 7.248647307278588e-06\n",
      "Epoch: 277, Validation Loss: 0.005362196825444698\n",
      "Epoch: 278, Train Loss: 7.248472684295848e-06\n",
      "Epoch: 278, Validation Loss: 0.005362073425203562\n",
      "Epoch: 279, Train Loss: 7.248298970807809e-06\n",
      "Epoch: 279, Validation Loss: 0.005361942574381828\n",
      "Epoch: 280, Train Loss: 7.248127531056525e-06\n",
      "Epoch: 280, Validation Loss: 0.005361813586205244\n",
      "Epoch: 281, Train Loss: 7.247955181810539e-06\n",
      "Epoch: 281, Validation Loss: 0.005361692514270544\n",
      "Epoch: 282, Train Loss: 7.247784651553957e-06\n",
      "Epoch: 282, Validation Loss: 0.0053615630604326725\n",
      "Epoch: 283, Train Loss: 7.2476136665500235e-06\n",
      "Epoch: 283, Validation Loss: 0.005361437331885099\n",
      "Epoch: 284, Train Loss: 7.247444500535494e-06\n",
      "Epoch: 284, Validation Loss: 0.005361310672014952\n",
      "Epoch: 285, Train Loss: 7.247275334520964e-06\n",
      "Epoch: 285, Validation Loss: 0.005361189600080252\n",
      "Epoch: 286, Train Loss: 7.247107532748487e-06\n",
      "Epoch: 286, Validation Loss: 0.0053610652685165405\n",
      "Epoch: 287, Train Loss: 7.246940185723361e-06\n",
      "Epoch: 287, Validation Loss: 0.005360942799597979\n",
      "Epoch: 288, Train Loss: 7.246773293445585e-06\n",
      "Epoch: 288, Validation Loss: 0.005360818933695555\n",
      "Epoch: 289, Train Loss: 7.24660640116781e-06\n",
      "Epoch: 289, Validation Loss: 0.00536069693043828\n",
      "Epoch: 290, Train Loss: 7.246441782626789e-06\n",
      "Epoch: 290, Validation Loss: 0.005360572133213282\n",
      "Epoch: 291, Train Loss: 7.246276709338417e-06\n",
      "Epoch: 291, Validation Loss: 0.005360455252230167\n",
      "Epoch: 292, Train Loss: 7.246113000292098e-06\n",
      "Epoch: 292, Validation Loss: 0.0053603313863277435\n",
      "Epoch: 293, Train Loss: 7.24594974599313e-06\n",
      "Epoch: 293, Validation Loss: 0.005360214505344629\n",
      "Epoch: 294, Train Loss: 7.2457869464415126e-06\n",
      "Epoch: 294, Validation Loss: 0.005360092036426067\n",
      "Epoch: 295, Train Loss: 7.245625511131948e-06\n",
      "Epoch: 295, Validation Loss: 0.0053599742241203785\n",
      "Epoch: 296, Train Loss: 7.245464075822383e-06\n",
      "Epoch: 296, Validation Loss: 0.005359852220863104\n",
      "Epoch: 297, Train Loss: 7.245304459502222e-06\n",
      "Epoch: 297, Validation Loss: 0.005359734874218702\n",
      "Epoch: 298, Train Loss: 7.24514438843471e-06\n",
      "Epoch: 298, Validation Loss: 0.005359615199267864\n",
      "Epoch: 299, Train Loss: 7.2449852268619e-06\n",
      "Epoch: 299, Validation Loss: 0.005359502509236336\n",
      "Epoch: 300, Train Loss: 7.24482652003644e-06\n",
      "Epoch: 300, Validation Loss: 0.005359382834285498\n",
      "Epoch: 301, Train Loss: 7.244668267958332e-06\n",
      "Epoch: 301, Validation Loss: 0.005359266418963671\n",
      "Epoch: 302, Train Loss: 7.244510925374925e-06\n",
      "Epoch: 302, Validation Loss: 0.005359153263270855\n",
      "Epoch: 303, Train Loss: 7.244355401780922e-06\n",
      "Epoch: 303, Validation Loss: 0.005359034985303879\n",
      "Epoch: 304, Train Loss: 7.244198513944866e-06\n",
      "Epoch: 304, Validation Loss: 0.005358921363949776\n",
      "Epoch: 305, Train Loss: 7.244043899845565e-06\n",
      "Epoch: 305, Validation Loss: 0.005358811002224684\n",
      "Epoch: 306, Train Loss: 7.243889285746263e-06\n",
      "Epoch: 306, Validation Loss: 0.005358690861612558\n",
      "Epoch: 307, Train Loss: 7.243735581141664e-06\n",
      "Epoch: 307, Validation Loss: 0.005358580965548754\n",
      "Epoch: 308, Train Loss: 7.243581876537064e-06\n",
      "Epoch: 308, Validation Loss: 0.0053584701381623745\n",
      "Epoch: 309, Train Loss: 7.243429536174517e-06\n",
      "Epoch: 309, Validation Loss: 0.00535835325717926\n",
      "Epoch: 310, Train Loss: 7.243277650559321e-06\n",
      "Epoch: 310, Validation Loss: 0.005358241498470306\n",
      "Epoch: 311, Train Loss: 7.243126219691476e-06\n",
      "Epoch: 311, Validation Loss: 0.005358134862035513\n",
      "Epoch: 312, Train Loss: 7.2429756983183324e-06\n",
      "Epoch: 312, Validation Loss: 0.005358021706342697\n",
      "Epoch: 313, Train Loss: 7.24282563169254e-06\n",
      "Epoch: 313, Validation Loss: 0.005357910413295031\n",
      "Epoch: 314, Train Loss: 7.242676474561449e-06\n",
      "Epoch: 314, Validation Loss: 0.005357797257602215\n",
      "Epoch: 315, Train Loss: 7.24252822692506e-06\n",
      "Epoch: 315, Validation Loss: 0.005357690621167421\n",
      "Epoch: 316, Train Loss: 7.24237952454132e-06\n",
      "Epoch: 316, Validation Loss: 0.005357581190764904\n",
      "Epoch: 317, Train Loss: 7.242232186399633e-06\n",
      "Epoch: 317, Validation Loss: 0.005357473157346249\n",
      "Epoch: 318, Train Loss: 7.242085303005297e-06\n",
      "Epoch: 318, Validation Loss: 0.005357364658266306\n",
      "Epoch: 319, Train Loss: 7.241938874358311e-06\n",
      "Epoch: 319, Validation Loss: 0.005357257090508938\n",
      "Epoch: 320, Train Loss: 7.241793355206028e-06\n",
      "Epoch: 320, Validation Loss: 0.005357151851058006\n",
      "Epoch: 321, Train Loss: 7.241648745548446e-06\n",
      "Epoch: 321, Validation Loss: 0.005357041954994202\n",
      "Epoch: 322, Train Loss: 7.241505045385566e-06\n",
      "Epoch: 322, Validation Loss: 0.005356936249881983\n",
      "Epoch: 323, Train Loss: 7.241360890475335e-06\n",
      "Epoch: 323, Validation Loss: 0.005356831010431051\n",
      "Epoch: 324, Train Loss: 7.241217645059805e-06\n",
      "Epoch: 324, Validation Loss: 0.00535672577098012\n",
      "Epoch: 325, Train Loss: 7.241074399644276e-06\n",
      "Epoch: 325, Validation Loss: 0.0053566209971904755\n",
      "Epoch: 326, Train Loss: 7.2409329732181504e-06\n",
      "Epoch: 326, Validation Loss: 0.0053565180860459805\n",
      "Epoch: 327, Train Loss: 7.240791092044674e-06\n",
      "Epoch: 327, Validation Loss: 0.005356413312256336\n",
      "Epoch: 328, Train Loss: 7.24065057511325e-06\n",
      "Epoch: 328, Validation Loss: 0.005356309469789267\n",
      "Epoch: 329, Train Loss: 7.240510512929177e-06\n",
      "Epoch: 329, Validation Loss: 0.005356204230338335\n",
      "Epoch: 330, Train Loss: 7.240370905492455e-06\n",
      "Epoch: 330, Validation Loss: 0.005356100853532553\n",
      "Epoch: 331, Train Loss: 7.240232207550434e-06\n",
      "Epoch: 331, Validation Loss: 0.005355998873710632\n",
      "Epoch: 332, Train Loss: 7.240093964355765e-06\n",
      "Epoch: 332, Validation Loss: 0.0053558978252112865\n",
      "Epoch: 333, Train Loss: 7.239956175908446e-06\n",
      "Epoch: 333, Validation Loss: 0.005355797708034515\n",
      "Epoch: 334, Train Loss: 7.239818842208479e-06\n",
      "Epoch: 334, Validation Loss: 0.005355695728212595\n",
      "Epoch: 335, Train Loss: 7.239681508508511e-06\n",
      "Epoch: 335, Validation Loss: 0.0053555965423583984\n",
      "Epoch: 336, Train Loss: 7.239545993797947e-06\n",
      "Epoch: 336, Validation Loss: 0.005355496425181627\n",
      "Epoch: 337, Train Loss: 7.239410024340032e-06\n",
      "Epoch: 337, Validation Loss: 0.005355394445359707\n",
      "Epoch: 338, Train Loss: 7.239274964376818e-06\n",
      "Epoch: 338, Validation Loss: 0.005355294793844223\n",
      "Epoch: 339, Train Loss: 7.239141268655658e-06\n",
      "Epoch: 339, Validation Loss: 0.005355194676667452\n",
      "Epoch: 340, Train Loss: 7.239007572934497e-06\n",
      "Epoch: 340, Validation Loss: 0.0053550987504422665\n",
      "Epoch: 341, Train Loss: 7.2388738772133365e-06\n",
      "Epoch: 341, Validation Loss: 0.005355000030249357\n",
      "Epoch: 342, Train Loss: 7.2387415457342286e-06\n",
      "Epoch: 342, Validation Loss: 0.005354900378733873\n",
      "Epoch: 343, Train Loss: 7.2386096690024715e-06\n",
      "Epoch: 343, Validation Loss: 0.005354803521186113\n",
      "Epoch: 344, Train Loss: 7.238478247018065e-06\n",
      "Epoch: 344, Validation Loss: 0.005354705732315779\n",
      "Epoch: 345, Train Loss: 7.23834727978101e-06\n",
      "Epoch: 345, Validation Loss: 0.005354611668735743\n",
      "Epoch: 346, Train Loss: 7.238216312543955e-06\n",
      "Epoch: 346, Validation Loss: 0.005354513879865408\n",
      "Epoch: 347, Train Loss: 7.238087164296303e-06\n",
      "Epoch: 347, Validation Loss: 0.0053544193506240845\n",
      "Epoch: 348, Train Loss: 7.2379575613013e-06\n",
      "Epoch: 348, Validation Loss: 0.0053543271496891975\n",
      "Epoch: 349, Train Loss: 7.2378284130536485e-06\n",
      "Epoch: 349, Validation Loss: 0.0053542302921414375\n",
      "Epoch: 350, Train Loss: 7.2377001743006986e-06\n",
      "Epoch: 350, Validation Loss: 0.005354135762900114\n",
      "Epoch: 351, Train Loss: 7.2375723902950995e-06\n",
      "Epoch: 351, Validation Loss: 0.00535404309630394\n",
      "Epoch: 352, Train Loss: 7.237445061036851e-06\n",
      "Epoch: 352, Validation Loss: 0.005353947170078754\n",
      "Epoch: 353, Train Loss: 7.237318641273305e-06\n",
      "Epoch: 353, Validation Loss: 0.005353854037821293\n",
      "Epoch: 354, Train Loss: 7.237192676257109e-06\n",
      "Epoch: 354, Validation Loss: 0.0053537627682089806\n",
      "Epoch: 355, Train Loss: 7.237066711240914e-06\n",
      "Epoch: 355, Validation Loss: 0.005353664979338646\n",
      "Epoch: 356, Train Loss: 7.23694165571942e-06\n",
      "Epoch: 356, Validation Loss: 0.005353574175387621\n",
      "Epoch: 357, Train Loss: 7.236817509692628e-06\n",
      "Epoch: 357, Validation Loss: 0.005353482905775309\n",
      "Epoch: 358, Train Loss: 7.236693818413187e-06\n",
      "Epoch: 358, Validation Loss: 0.0053533934988081455\n",
      "Epoch: 359, Train Loss: 7.236569672386395e-06\n",
      "Epoch: 359, Validation Loss: 0.005353300366550684\n",
      "Epoch: 360, Train Loss: 7.2364468906016555e-06\n",
      "Epoch: 360, Validation Loss: 0.005353213753551245\n",
      "Epoch: 361, Train Loss: 7.236324108816916e-06\n",
      "Epoch: 361, Validation Loss: 0.005353120155632496\n",
      "Epoch: 362, Train Loss: 7.2362026912742294e-06\n",
      "Epoch: 362, Validation Loss: 0.0053530302830040455\n",
      "Epoch: 363, Train Loss: 7.236081273731543e-06\n",
      "Epoch: 363, Validation Loss: 0.0053529394790530205\n",
      "Epoch: 364, Train Loss: 7.235960310936207e-06\n",
      "Epoch: 364, Validation Loss: 0.005352855194360018\n",
      "Epoch: 365, Train Loss: 7.235840257635573e-06\n",
      "Epoch: 365, Validation Loss: 0.005352765321731567\n",
      "Epoch: 366, Train Loss: 7.23572065908229e-06\n",
      "Epoch: 366, Validation Loss: 0.005352676380425692\n",
      "Epoch: 367, Train Loss: 7.2356015152763575e-06\n",
      "Epoch: 367, Validation Loss: 0.005352588836103678\n",
      "Epoch: 368, Train Loss: 7.235482826217776e-06\n",
      "Epoch: 368, Validation Loss: 0.005352501291781664\n",
      "Epoch: 369, Train Loss: 7.235363682411844e-06\n",
      "Epoch: 369, Validation Loss: 0.005352412350475788\n",
      "Epoch: 370, Train Loss: 7.235246357595315e-06\n",
      "Epoch: 370, Validation Loss: 0.00535232899710536\n",
      "Epoch: 371, Train Loss: 7.235129487526137e-06\n",
      "Epoch: 371, Validation Loss: 0.005352240055799484\n",
      "Epoch: 372, Train Loss: 7.2350126174569596e-06\n",
      "Epoch: 372, Validation Loss: 0.005352153442800045\n",
      "Epoch: 373, Train Loss: 7.234896202135133e-06\n",
      "Epoch: 373, Validation Loss: 0.005352066829800606\n",
      "Epoch: 374, Train Loss: 7.234780241560657e-06\n",
      "Epoch: 374, Validation Loss: 0.00535198487341404\n",
      "Epoch: 375, Train Loss: 7.234665190480882e-06\n",
      "Epoch: 375, Validation Loss: 0.0053518982604146\n",
      "Epoch: 376, Train Loss: 7.234550594148459e-06\n",
      "Epoch: 376, Validation Loss: 0.00535181351006031\n",
      "Epoch: 377, Train Loss: 7.2344359978160355e-06\n",
      "Epoch: 377, Validation Loss: 0.00535173062235117\n",
      "Epoch: 378, Train Loss: 7.234321856230963e-06\n",
      "Epoch: 378, Validation Loss: 0.005351645406335592\n",
      "Epoch: 379, Train Loss: 7.234208169393241e-06\n",
      "Epoch: 379, Validation Loss: 0.005351564381271601\n",
      "Epoch: 380, Train Loss: 7.234094937302871e-06\n",
      "Epoch: 380, Validation Loss: 0.005351478699594736\n",
      "Epoch: 381, Train Loss: 7.2339835242019035e-06\n",
      "Epoch: 381, Validation Loss: 0.005351395811885595\n",
      "Epoch: 382, Train Loss: 7.2338712016062345e-06\n",
      "Epoch: 382, Validation Loss: 0.005351313855499029\n",
      "Epoch: 383, Train Loss: 7.233759788505267e-06\n",
      "Epoch: 383, Validation Loss: 0.00535123236477375\n",
      "Epoch: 384, Train Loss: 7.233648830151651e-06\n",
      "Epoch: 384, Validation Loss: 0.005351149011403322\n",
      "Epoch: 385, Train Loss: 7.233537871798035e-06\n",
      "Epoch: 385, Validation Loss: 0.00535106984898448\n",
      "Epoch: 386, Train Loss: 7.233428277686471e-06\n",
      "Epoch: 386, Validation Loss: 0.0053509874269366264\n",
      "Epoch: 387, Train Loss: 7.233318228827557e-06\n",
      "Epoch: 387, Validation Loss: 0.005350906867533922\n",
      "Epoch: 388, Train Loss: 7.233209089463344e-06\n",
      "Epoch: 388, Validation Loss: 0.005350826773792505\n",
      "Epoch: 389, Train Loss: 7.233100859593833e-06\n",
      "Epoch: 389, Validation Loss: 0.005350745283067226\n",
      "Epoch: 390, Train Loss: 7.232992174976971e-06\n",
      "Epoch: 390, Validation Loss: 0.005350664723664522\n",
      "Epoch: 391, Train Loss: 7.232884399854811e-06\n",
      "Epoch: 391, Validation Loss: 0.00535058556124568\n",
      "Epoch: 392, Train Loss: 7.232777079480002e-06\n",
      "Epoch: 392, Validation Loss: 0.005350508261471987\n",
      "Epoch: 393, Train Loss: 7.232670668599894e-06\n",
      "Epoch: 393, Validation Loss: 0.005350429564714432\n",
      "Epoch: 394, Train Loss: 7.232564257719787e-06\n",
      "Epoch: 394, Validation Loss: 0.005350350867956877\n",
      "Epoch: 395, Train Loss: 7.23245830158703e-06\n",
      "Epoch: 395, Validation Loss: 0.005350271239876747\n",
      "Epoch: 396, Train Loss: 7.232352800201625e-06\n",
      "Epoch: 396, Validation Loss: 0.005350194405764341\n",
      "Epoch: 397, Train Loss: 7.232247298816219e-06\n",
      "Epoch: 397, Validation Loss: 0.005350115709006786\n",
      "Epoch: 398, Train Loss: 7.232142706925515e-06\n",
      "Epoch: 398, Validation Loss: 0.005350039340555668\n",
      "Epoch: 399, Train Loss: 7.232038569782162e-06\n",
      "Epoch: 399, Validation Loss: 0.005349965300410986\n",
      "Epoch: 400, Train Loss: 7.23193488738616e-06\n",
      "Epoch: 400, Validation Loss: 0.005349888000637293\n",
      "Epoch: 401, Train Loss: 7.231831659737509e-06\n",
      "Epoch: 401, Validation Loss: 0.005349811632186174\n",
      "Epoch: 402, Train Loss: 7.2317284320888575e-06\n",
      "Epoch: 402, Validation Loss: 0.005349735729396343\n",
      "Epoch: 403, Train Loss: 7.231626113934908e-06\n",
      "Epoch: 403, Validation Loss: 0.005349660292267799\n",
      "Epoch: 404, Train Loss: 7.2315242505283095e-06\n",
      "Epoch: 404, Validation Loss: 0.0053495848551392555\n",
      "Epoch: 405, Train Loss: 7.231422841869062e-06\n",
      "Epoch: 405, Validation Loss: 0.005349509418010712\n",
      "Epoch: 406, Train Loss: 7.231321433209814e-06\n",
      "Epoch: 406, Validation Loss: 0.005349437240511179\n",
      "Epoch: 407, Train Loss: 7.231220934045268e-06\n",
      "Epoch: 407, Validation Loss: 0.005349359940737486\n",
      "Epoch: 408, Train Loss: 7.231120434880722e-06\n",
      "Epoch: 408, Validation Loss: 0.005349286366254091\n",
      "Epoch: 409, Train Loss: 7.231020845210878e-06\n",
      "Epoch: 409, Validation Loss: 0.005349212791770697\n",
      "Epoch: 410, Train Loss: 7.230920800793683e-06\n",
      "Epoch: 410, Validation Loss: 0.005349141079932451\n",
      "Epoch: 411, Train Loss: 7.230822575365892e-06\n",
      "Epoch: 411, Validation Loss: 0.0053490702994167805\n",
      "Epoch: 412, Train Loss: 7.230723895190749e-06\n",
      "Epoch: 412, Validation Loss: 0.005348993930965662\n",
      "Epoch: 413, Train Loss: 7.230625215015607e-06\n",
      "Epoch: 413, Validation Loss: 0.005348924547433853\n",
      "Epoch: 414, Train Loss: 7.230527444335166e-06\n",
      "Epoch: 414, Validation Loss: 0.0053488509729504585\n",
      "Epoch: 415, Train Loss: 7.230430128402077e-06\n",
      "Epoch: 415, Validation Loss: 0.005348781123757362\n",
      "Epoch: 416, Train Loss: 7.230332812468987e-06\n",
      "Epoch: 416, Validation Loss: 0.0053487070836126804\n",
      "Epoch: 417, Train Loss: 7.23023686077795e-06\n",
      "Epoch: 417, Validation Loss: 0.005348635371774435\n",
      "Epoch: 418, Train Loss: 7.230140909086913e-06\n",
      "Epoch: 418, Validation Loss: 0.005348564591258764\n",
      "Epoch: 419, Train Loss: 7.230045412143227e-06\n",
      "Epoch: 419, Validation Loss: 0.005348494742065668\n",
      "Epoch: 420, Train Loss: 7.2299499151995406e-06\n",
      "Epoch: 420, Validation Loss: 0.005348423961549997\n",
      "Epoch: 421, Train Loss: 7.229854873003205e-06\n",
      "Epoch: 421, Validation Loss: 0.0053483545780181885\n",
      "Epoch: 422, Train Loss: 7.229760285554221e-06\n",
      "Epoch: 422, Validation Loss: 0.005348284263163805\n",
      "Epoch: 423, Train Loss: 7.229666607599938e-06\n",
      "Epoch: 423, Validation Loss: 0.005348216742277145\n",
      "Epoch: 424, Train Loss: 7.229572474898305e-06\n",
      "Epoch: 424, Validation Loss: 0.0053481473587453365\n",
      "Epoch: 425, Train Loss: 7.229479251691373e-06\n",
      "Epoch: 425, Validation Loss: 0.005348079837858677\n",
      "Epoch: 426, Train Loss: 7.229386483231792e-06\n",
      "Epoch: 426, Validation Loss: 0.005348011385649443\n",
      "Epoch: 427, Train Loss: 7.229294169519562e-06\n",
      "Epoch: 427, Validation Loss: 0.0053479415364563465\n",
      "Epoch: 428, Train Loss: 7.229201855807332e-06\n",
      "Epoch: 428, Validation Loss: 0.0053478735499084\n",
      "Epoch: 429, Train Loss: 7.229109996842453e-06\n",
      "Epoch: 429, Validation Loss: 0.005347805563360453\n",
      "Epoch: 430, Train Loss: 7.229018137877574e-06\n",
      "Epoch: 430, Validation Loss: 0.005347738973796368\n",
      "Epoch: 431, Train Loss: 7.228927643154748e-06\n",
      "Epoch: 431, Validation Loss: 0.005347671918570995\n",
      "Epoch: 432, Train Loss: 7.228837148431921e-06\n",
      "Epoch: 432, Validation Loss: 0.005347603466361761\n",
      "Epoch: 433, Train Loss: 7.228746653709095e-06\n",
      "Epoch: 433, Validation Loss: 0.005347537808120251\n",
      "Epoch: 434, Train Loss: 7.22865706848097e-06\n",
      "Epoch: 434, Validation Loss: 0.00534747214987874\n",
      "Epoch: 435, Train Loss: 7.228567483252846e-06\n",
      "Epoch: 435, Validation Loss: 0.0053474074229598045\n",
      "Epoch: 436, Train Loss: 7.228478352772072e-06\n",
      "Epoch: 436, Validation Loss: 0.005347343161702156\n",
      "Epoch: 437, Train Loss: 7.228389677038649e-06\n",
      "Epoch: 437, Validation Loss: 0.005347275175154209\n",
      "Epoch: 438, Train Loss: 7.228301910799928e-06\n",
      "Epoch: 438, Validation Loss: 0.005347210448235273\n",
      "Epoch: 439, Train Loss: 7.228213689813856e-06\n",
      "Epoch: 439, Validation Loss: 0.00534714525565505\n",
      "Epoch: 440, Train Loss: 7.228125923575135e-06\n",
      "Epoch: 440, Validation Loss: 0.005347083322703838\n",
      "Epoch: 441, Train Loss: 7.228039066831116e-06\n",
      "Epoch: 441, Validation Loss: 0.005347017198801041\n",
      "Epoch: 442, Train Loss: 7.227952210087096e-06\n",
      "Epoch: 442, Validation Loss: 0.005346952471882105\n",
      "Epoch: 443, Train Loss: 7.227865353343077e-06\n",
      "Epoch: 443, Validation Loss: 0.005346887744963169\n",
      "Epoch: 444, Train Loss: 7.2277794060937595e-06\n",
      "Epoch: 444, Validation Loss: 0.005346827208995819\n",
      "Epoch: 445, Train Loss: 7.227693913591793e-06\n",
      "Epoch: 445, Validation Loss: 0.005346763413399458\n",
      "Epoch: 446, Train Loss: 7.227608421089826e-06\n",
      "Epoch: 446, Validation Loss: 0.005346701946109533\n",
      "Epoch: 447, Train Loss: 7.22752338333521e-06\n",
      "Epoch: 447, Validation Loss: 0.0053466372191905975\n",
      "Epoch: 448, Train Loss: 7.227439255075296e-06\n",
      "Epoch: 448, Validation Loss: 0.0053465766832232475\n",
      "Epoch: 449, Train Loss: 7.2273546720680315e-06\n",
      "Epoch: 449, Validation Loss: 0.005346512887626886\n",
      "Epoch: 450, Train Loss: 7.2272705438081175e-06\n",
      "Epoch: 450, Validation Loss: 0.0053464509546756744\n",
      "Epoch: 451, Train Loss: 7.227187325042905e-06\n",
      "Epoch: 451, Validation Loss: 0.005346390418708324\n",
      "Epoch: 452, Train Loss: 7.227103651530342e-06\n",
      "Epoch: 452, Validation Loss: 0.0053463289514184\n",
      "Epoch: 453, Train Loss: 7.227021342259832e-06\n",
      "Epoch: 453, Validation Loss: 0.00534626841545105\n",
      "Epoch: 454, Train Loss: 7.226939032989321e-06\n",
      "Epoch: 454, Validation Loss: 0.005346205085515976\n",
      "Epoch: 455, Train Loss: 7.22685626897146e-06\n",
      "Epoch: 455, Validation Loss: 0.0053461454808712006\n",
      "Epoch: 456, Train Loss: 7.226774869195651e-06\n",
      "Epoch: 456, Validation Loss: 0.005346085876226425\n",
      "Epoch: 457, Train Loss: 7.226693469419843e-06\n",
      "Epoch: 457, Validation Loss: 0.00534602627158165\n",
      "Epoch: 458, Train Loss: 7.226612069644034e-06\n",
      "Epoch: 458, Validation Loss: 0.0053459657356143\n",
      "Epoch: 459, Train Loss: 7.226531124615576e-06\n",
      "Epoch: 459, Validation Loss: 0.0053459047339856625\n",
      "Epoch: 460, Train Loss: 7.22645108908182e-06\n",
      "Epoch: 460, Validation Loss: 0.005345846991986036\n",
      "Epoch: 461, Train Loss: 7.226370598800713e-06\n",
      "Epoch: 461, Validation Loss: 0.005345787853002548\n",
      "Epoch: 462, Train Loss: 7.226290563266957e-06\n",
      "Epoch: 462, Validation Loss: 0.005345729645341635\n",
      "Epoch: 463, Train Loss: 7.226211437227903e-06\n",
      "Epoch: 463, Validation Loss: 0.005345672369003296\n",
      "Epoch: 464, Train Loss: 7.2261327659361996e-06\n",
      "Epoch: 464, Validation Loss: 0.005345613695681095\n",
      "Epoch: 465, Train Loss: 7.226053639897145e-06\n",
      "Epoch: 465, Validation Loss: 0.005345553625375032\n",
      "Epoch: 466, Train Loss: 7.225974968605442e-06\n",
      "Epoch: 466, Validation Loss: 0.005345496814697981\n",
      "Epoch: 467, Train Loss: 7.22589720680844e-06\n",
      "Epoch: 467, Validation Loss: 0.005345439538359642\n",
      "Epoch: 468, Train Loss: 7.225819445011439e-06\n",
      "Epoch: 468, Validation Loss: 0.005345383193343878\n",
      "Epoch: 469, Train Loss: 7.225742137961788e-06\n",
      "Epoch: 469, Validation Loss: 0.005345324985682964\n",
      "Epoch: 470, Train Loss: 7.225665285659488e-06\n",
      "Epoch: 470, Validation Loss: 0.005345269571989775\n",
      "Epoch: 471, Train Loss: 7.2255884333571885e-06\n",
      "Epoch: 471, Validation Loss: 0.005345210433006287\n",
      "Epoch: 472, Train Loss: 7.225511581054889e-06\n",
      "Epoch: 472, Validation Loss: 0.005345155019313097\n",
      "Epoch: 473, Train Loss: 7.22543518349994e-06\n",
      "Epoch: 473, Validation Loss: 0.005345097742974758\n",
      "Epoch: 474, Train Loss: 7.225359695439693e-06\n",
      "Epoch: 474, Validation Loss: 0.0053450423292815685\n",
      "Epoch: 475, Train Loss: 7.225284662126796e-06\n",
      "Epoch: 475, Validation Loss: 0.005344988312572241\n",
      "Epoch: 476, Train Loss: 7.225209174066549e-06\n",
      "Epoch: 476, Validation Loss: 0.0053449333645403385\n",
      "Epoch: 477, Train Loss: 7.225134140753653e-06\n",
      "Epoch: 477, Validation Loss: 0.005344878416508436\n",
      "Epoch: 478, Train Loss: 7.2250595621881075e-06\n",
      "Epoch: 478, Validation Loss: 0.005344823002815247\n",
      "Epoch: 479, Train Loss: 7.224985893117264e-06\n",
      "Epoch: 479, Validation Loss: 0.005344768054783344\n",
      "Epoch: 480, Train Loss: 7.224911769299069e-06\n",
      "Epoch: 480, Validation Loss: 0.005344712641090155\n",
      "Epoch: 481, Train Loss: 7.224837645480875e-06\n",
      "Epoch: 481, Validation Loss: 0.0053446595557034016\n",
      "Epoch: 482, Train Loss: 7.224764885904733e-06\n",
      "Epoch: 482, Validation Loss: 0.0053446064703166485\n",
      "Epoch: 483, Train Loss: 7.224692126328591e-06\n",
      "Epoch: 483, Validation Loss: 0.005344552453607321\n",
      "Epoch: 484, Train Loss: 7.2246189120050985e-06\n",
      "Epoch: 484, Validation Loss: 0.00534449890255928\n",
      "Epoch: 485, Train Loss: 7.2245470619236585e-06\n",
      "Epoch: 485, Validation Loss: 0.005344443954527378\n",
      "Epoch: 486, Train Loss: 7.2244747570948675e-06\n",
      "Epoch: 486, Validation Loss: 0.005344392731785774\n",
      "Epoch: 487, Train Loss: 7.224403361760778e-06\n",
      "Epoch: 487, Validation Loss: 0.005344339646399021\n",
      "Epoch: 488, Train Loss: 7.224331511679338e-06\n",
      "Epoch: 488, Validation Loss: 0.005344287026673555\n",
      "Epoch: 489, Train Loss: 7.224261025839951e-06\n",
      "Epoch: 489, Validation Loss: 0.005344233941286802\n",
      "Epoch: 490, Train Loss: 7.224189630505862e-06\n",
      "Epoch: 490, Validation Loss: 0.0053441813215613365\n",
      "Epoch: 491, Train Loss: 7.2241186899191234e-06\n",
      "Epoch: 491, Validation Loss: 0.005344129167497158\n",
      "Epoch: 492, Train Loss: 7.224048658827087e-06\n",
      "Epoch: 492, Validation Loss: 0.005344077944755554\n",
      "Epoch: 493, Train Loss: 7.22397862773505e-06\n",
      "Epoch: 493, Validation Loss: 0.005344027187675238\n",
      "Epoch: 494, Train Loss: 7.223909506137716e-06\n",
      "Epoch: 494, Validation Loss: 0.005343974102288485\n",
      "Epoch: 495, Train Loss: 7.22383992979303e-06\n",
      "Epoch: 495, Validation Loss: 0.005343925207853317\n",
      "Epoch: 496, Train Loss: 7.223771262943046e-06\n",
      "Epoch: 496, Validation Loss: 0.005343873053789139\n",
      "Epoch: 497, Train Loss: 7.223702596093062e-06\n",
      "Epoch: 497, Validation Loss: 0.0053438227623701096\n",
      "Epoch: 498, Train Loss: 7.2236334744957276e-06\n",
      "Epoch: 498, Validation Loss: 0.005343771539628506\n",
      "Epoch: 499, Train Loss: 7.223565262393095e-06\n",
      "Epoch: 499, Validation Loss: 0.0053437212482094765\n",
      "Epoch: 500, Train Loss: 7.223497959785163e-06\n",
      "Epoch: 500, Validation Loss: 0.005343671888113022\n",
      "Epoch: 501, Train Loss: 7.2234297476825304e-06\n",
      "Epoch: 501, Validation Loss: 0.00534362206235528\n",
      "Epoch: 502, Train Loss: 7.22336289982195e-06\n",
      "Epoch: 502, Validation Loss: 0.0053435736335814\n",
      "Epoch: 503, Train Loss: 7.223295597214019e-06\n",
      "Epoch: 503, Validation Loss: 0.005343522410839796\n",
      "Epoch: 504, Train Loss: 7.223228749353439e-06\n",
      "Epoch: 504, Validation Loss: 0.005343474913388491\n",
      "Epoch: 505, Train Loss: 7.223162356240209e-06\n",
      "Epoch: 505, Validation Loss: 0.005343423690646887\n",
      "Epoch: 506, Train Loss: 7.223096417874331e-06\n",
      "Epoch: 506, Validation Loss: 0.005343377590179443\n",
      "Epoch: 507, Train Loss: 7.223030479508452e-06\n",
      "Epoch: 507, Validation Loss: 0.005343327298760414\n",
      "Epoch: 508, Train Loss: 7.2229649958899245e-06\n",
      "Epoch: 508, Validation Loss: 0.005343279801309109\n",
      "Epoch: 509, Train Loss: 7.222899057524046e-06\n",
      "Epoch: 509, Validation Loss: 0.005343231372535229\n",
      "Epoch: 510, Train Loss: 7.222834028652869e-06\n",
      "Epoch: 510, Validation Loss: 0.005343182943761349\n",
      "Epoch: 511, Train Loss: 7.222769454529043e-06\n",
      "Epoch: 511, Validation Loss: 0.005343136377632618\n",
      "Epoch: 512, Train Loss: 7.222705335152568e-06\n",
      "Epoch: 512, Validation Loss: 0.005343087948858738\n",
      "Epoch: 513, Train Loss: 7.222640761028742e-06\n",
      "Epoch: 513, Validation Loss: 0.0053430404514074326\n",
      "Epoch: 514, Train Loss: 7.222577096399618e-06\n",
      "Epoch: 514, Validation Loss: 0.005342993885278702\n",
      "Epoch: 515, Train Loss: 7.222512977023143e-06\n",
      "Epoch: 515, Validation Loss: 0.005342945922166109\n",
      "Epoch: 516, Train Loss: 7.222449312394019e-06\n",
      "Epoch: 516, Validation Loss: 0.005342898890376091\n",
      "Epoch: 517, Train Loss: 7.222386557259597e-06\n",
      "Epoch: 517, Validation Loss: 0.005342853721231222\n",
      "Epoch: 518, Train Loss: 7.222323347377824e-06\n",
      "Epoch: 518, Validation Loss: 0.005342806223779917\n",
      "Epoch: 519, Train Loss: 7.222260592243401e-06\n",
      "Epoch: 519, Validation Loss: 0.005342759657651186\n",
      "Epoch: 520, Train Loss: 7.22219829185633e-06\n",
      "Epoch: 520, Validation Loss: 0.0053427135571837425\n",
      "Epoch: 521, Train Loss: 7.2221359914692584e-06\n",
      "Epoch: 521, Validation Loss: 0.005342668853700161\n",
      "Epoch: 522, Train Loss: 7.222074145829538e-06\n",
      "Epoch: 522, Validation Loss: 0.0053426241502165794\n",
      "Epoch: 523, Train Loss: 7.222012754937168e-06\n",
      "Epoch: 523, Validation Loss: 0.005342578049749136\n",
      "Epoch: 524, Train Loss: 7.2219513640447985e-06\n",
      "Epoch: 524, Validation Loss: 0.00534253241494298\n",
      "Epoch: 525, Train Loss: 7.221889973152429e-06\n",
      "Epoch: 525, Validation Loss: 0.005342487711459398\n",
      "Epoch: 526, Train Loss: 7.221829491754761e-06\n",
      "Epoch: 526, Validation Loss: 0.005342443939298391\n",
      "Epoch: 527, Train Loss: 7.221769465104444e-06\n",
      "Epoch: 527, Validation Loss: 0.005342398304492235\n",
      "Epoch: 528, Train Loss: 7.221708074212074e-06\n",
      "Epoch: 528, Validation Loss: 0.005342352669686079\n",
      "Epoch: 529, Train Loss: 7.221647592814406e-06\n",
      "Epoch: 529, Validation Loss: 0.005342309828847647\n",
      "Epoch: 530, Train Loss: 7.221588475658791e-06\n",
      "Epoch: 530, Validation Loss: 0.005342266522347927\n",
      "Epoch: 531, Train Loss: 7.221528903755825e-06\n",
      "Epoch: 531, Validation Loss: 0.0053422218188643456\n",
      "Epoch: 532, Train Loss: 7.221469331852859e-06\n",
      "Epoch: 532, Validation Loss: 0.005342177581042051\n",
      "Epoch: 533, Train Loss: 7.221410214697244e-06\n",
      "Epoch: 533, Validation Loss: 0.005342134274542332\n",
      "Epoch: 534, Train Loss: 7.22135200703633e-06\n",
      "Epoch: 534, Validation Loss: 0.005342091433703899\n",
      "Epoch: 535, Train Loss: 7.221293799375417e-06\n",
      "Epoch: 535, Validation Loss: 0.005342049524188042\n",
      "Epoch: 536, Train Loss: 7.221235591714503e-06\n",
      "Epoch: 536, Validation Loss: 0.00534200482070446\n",
      "Epoch: 537, Train Loss: 7.22117738405359e-06\n",
      "Epoch: 537, Validation Loss: 0.005341962445527315\n",
      "Epoch: 538, Train Loss: 7.221119631140027e-06\n",
      "Epoch: 538, Validation Loss: 0.005341919604688883\n",
      "Epoch: 539, Train Loss: 7.2210623329738155e-06\n",
      "Epoch: 539, Validation Loss: 0.005341876298189163\n",
      "Epoch: 540, Train Loss: 7.221004580060253e-06\n",
      "Epoch: 540, Validation Loss: 0.00534183531999588\n",
      "Epoch: 541, Train Loss: 7.220947281894041e-06\n",
      "Epoch: 541, Validation Loss: 0.00534179387614131\n",
      "Epoch: 542, Train Loss: 7.220890893222531e-06\n",
      "Epoch: 542, Validation Loss: 0.005341752432286739\n",
      "Epoch: 543, Train Loss: 7.2208340498036705e-06\n",
      "Epoch: 543, Validation Loss: 0.005341709591448307\n",
      "Epoch: 544, Train Loss: 7.2207781158795115e-06\n",
      "Epoch: 544, Validation Loss: 0.005341668147593737\n",
      "Epoch: 545, Train Loss: 7.2207217272080015e-06\n",
      "Epoch: 545, Validation Loss: 0.005341627635061741\n",
      "Epoch: 546, Train Loss: 7.220666248031193e-06\n",
      "Epoch: 546, Validation Loss: 0.005341585725545883\n",
      "Epoch: 547, Train Loss: 7.220610314107034e-06\n",
      "Epoch: 547, Validation Loss: 0.005341545213013887\n",
      "Epoch: 548, Train Loss: 7.220554380182875e-06\n",
      "Epoch: 548, Validation Loss: 0.005341504234820604\n",
      "Epoch: 549, Train Loss: 7.220499810500769e-06\n",
      "Epoch: 549, Validation Loss: 0.005341463256627321\n",
      "Epoch: 550, Train Loss: 7.220444786071312e-06\n",
      "Epoch: 550, Validation Loss: 0.005341423209756613\n",
      "Epoch: 551, Train Loss: 7.220390216389205e-06\n",
      "Epoch: 551, Validation Loss: 0.005341382697224617\n",
      "Epoch: 552, Train Loss: 7.220335646707099e-06\n",
      "Epoch: 552, Validation Loss: 0.0053413426503539085\n",
      "Epoch: 553, Train Loss: 7.2202810770249926e-06\n",
      "Epoch: 553, Validation Loss: 0.005341302137821913\n",
      "Epoch: 554, Train Loss: 7.220227416837588e-06\n",
      "Epoch: 554, Validation Loss: 0.005341263022273779\n",
      "Epoch: 555, Train Loss: 7.2201733019028325e-06\n",
      "Epoch: 555, Validation Loss: 0.0053412229754030704\n",
      "Epoch: 556, Train Loss: 7.220120096462779e-06\n",
      "Epoch: 556, Validation Loss: 0.005341183859854937\n",
      "Epoch: 557, Train Loss: 7.220066891022725e-06\n",
      "Epoch: 557, Validation Loss: 0.005341145675629377\n",
      "Epoch: 558, Train Loss: 7.2200132308353204e-06\n",
      "Epoch: 558, Validation Loss: 0.005341105163097382\n",
      "Epoch: 559, Train Loss: 7.2199609348899685e-06\n",
      "Epoch: 559, Validation Loss: 0.005341066513210535\n",
      "Epoch: 560, Train Loss: 7.219908184197266e-06\n",
      "Epoch: 560, Validation Loss: 0.005341028328984976\n",
      "Epoch: 561, Train Loss: 7.219855888251914e-06\n",
      "Epoch: 561, Validation Loss: 0.005340988282114267\n",
      "Epoch: 562, Train Loss: 7.219803137559211e-06\n",
      "Epoch: 562, Validation Loss: 0.005340950563549995\n",
      "Epoch: 563, Train Loss: 7.21975129636121e-06\n",
      "Epoch: 563, Validation Loss: 0.0053409128449857235\n",
      "Epoch: 564, Train Loss: 7.21969990991056e-06\n",
      "Epoch: 564, Validation Loss: 0.005340875126421452\n",
      "Epoch: 565, Train Loss: 7.2196485234599095e-06\n",
      "Epoch: 565, Validation Loss: 0.005340836476534605\n",
      "Epoch: 566, Train Loss: 7.219597137009259e-06\n",
      "Epoch: 566, Validation Loss: 0.005340798757970333\n",
      "Epoch: 567, Train Loss: 7.21954620530596e-06\n",
      "Epoch: 567, Validation Loss: 0.005340761039406061\n",
      "Epoch: 568, Train Loss: 7.21949481885531e-06\n",
      "Epoch: 568, Validation Loss: 0.005340723320841789\n",
      "Epoch: 569, Train Loss: 7.2194443418993615e-06\n",
      "Epoch: 569, Validation Loss: 0.005340685602277517\n",
      "Epoch: 570, Train Loss: 7.219393864943413e-06\n",
      "Epoch: 570, Validation Loss: 0.005340649280697107\n",
      "Epoch: 571, Train Loss: 7.219343387987465e-06\n",
      "Epoch: 571, Validation Loss: 0.005340612027794123\n",
      "Epoch: 572, Train Loss: 7.219293365778867e-06\n",
      "Epoch: 572, Validation Loss: 0.005340574774891138\n",
      "Epoch: 573, Train Loss: 7.2192437983176205e-06\n",
      "Epoch: 573, Validation Loss: 0.0053405375219881535\n",
      "Epoch: 574, Train Loss: 7.219194230856374e-06\n",
      "Epoch: 574, Validation Loss: 0.005340500734746456\n",
      "Epoch: 575, Train Loss: 7.219144663395127e-06\n",
      "Epoch: 575, Validation Loss: 0.005340466741472483\n",
      "Epoch: 576, Train Loss: 7.2190955506812315e-06\n",
      "Epoch: 576, Validation Loss: 0.005340429022908211\n",
      "Epoch: 577, Train Loss: 7.219046437967336e-06\n",
      "Epoch: 577, Validation Loss: 0.005340392701327801\n",
      "Epoch: 578, Train Loss: 7.218997780000791e-06\n",
      "Epoch: 578, Validation Loss: 0.005340357776731253\n",
      "Epoch: 579, Train Loss: 7.218949122034246e-06\n",
      "Epoch: 579, Validation Loss: 0.005340320523828268\n",
      "Epoch: 580, Train Loss: 7.218900918815052e-06\n",
      "Epoch: 580, Validation Loss: 0.00534028559923172\n",
      "Epoch: 581, Train Loss: 7.218852715595858e-06\n",
      "Epoch: 581, Validation Loss: 0.005340249743312597\n",
      "Epoch: 582, Train Loss: 7.218804967124015e-06\n",
      "Epoch: 582, Validation Loss: 0.005340215750038624\n",
      "Epoch: 583, Train Loss: 7.218757218652172e-06\n",
      "Epoch: 583, Validation Loss: 0.005340179894119501\n",
      "Epoch: 584, Train Loss: 7.218709470180329e-06\n",
      "Epoch: 584, Validation Loss: 0.005340144503861666\n",
      "Epoch: 585, Train Loss: 7.218661721708486e-06\n",
      "Epoch: 585, Validation Loss: 0.005340110044926405\n",
      "Epoch: 586, Train Loss: 7.218614427983994e-06\n",
      "Epoch: 586, Validation Loss: 0.005340075120329857\n",
      "Epoch: 587, Train Loss: 7.218568043754203e-06\n",
      "Epoch: 587, Validation Loss: 0.005340040661394596\n",
      "Epoch: 588, Train Loss: 7.218521204777062e-06\n",
      "Epoch: 588, Validation Loss: 0.005340006668120623\n",
      "Epoch: 589, Train Loss: 7.218474365799921e-06\n",
      "Epoch: 589, Validation Loss: 0.0053399708122015\n",
      "Epoch: 590, Train Loss: 7.21842798157013e-06\n",
      "Epoch: 590, Validation Loss: 0.005339937750250101\n",
      "Epoch: 591, Train Loss: 7.218382052087691e-06\n",
      "Epoch: 591, Validation Loss: 0.005339904222637415\n",
      "Epoch: 592, Train Loss: 7.2183356678579e-06\n",
      "Epoch: 592, Validation Loss: 0.005339871160686016\n",
      "Epoch: 593, Train Loss: 7.2182906478701625e-06\n",
      "Epoch: 593, Validation Loss: 0.005339836236089468\n",
      "Epoch: 594, Train Loss: 7.218244718387723e-06\n",
      "Epoch: 594, Validation Loss: 0.0053398022428154945\n",
      "Epoch: 595, Train Loss: 7.218199243652634e-06\n",
      "Epoch: 595, Validation Loss: 0.005339769180864096\n",
      "Epoch: 596, Train Loss: 7.2181542236648966e-06\n",
      "Epoch: 596, Validation Loss: 0.005339734256267548\n",
      "Epoch: 597, Train Loss: 7.218108748929808e-06\n",
      "Epoch: 597, Validation Loss: 0.005339701659977436\n",
      "Epoch: 598, Train Loss: 7.218064183689421e-06\n",
      "Epoch: 598, Validation Loss: 0.005339668598026037\n",
      "Epoch: 599, Train Loss: 7.218019618449034e-06\n",
      "Epoch: 599, Validation Loss: 0.005339636467397213\n",
      "Epoch: 600, Train Loss: 7.217975507955998e-06\n",
      "Epoch: 600, Validation Loss: 0.005339603405445814\n",
      "Epoch: 601, Train Loss: 7.217930942715611e-06\n",
      "Epoch: 601, Validation Loss: 0.0053395722061395645\n",
      "Epoch: 602, Train Loss: 7.217886832222575e-06\n",
      "Epoch: 602, Validation Loss: 0.005339539609849453\n",
      "Epoch: 603, Train Loss: 7.21784317647689e-06\n",
      "Epoch: 603, Validation Loss: 0.005339506547898054\n",
      "Epoch: 604, Train Loss: 7.217799520731205e-06\n",
      "Epoch: 604, Validation Loss: 0.00533947441726923\n",
      "Epoch: 605, Train Loss: 7.217756319732871e-06\n",
      "Epoch: 605, Validation Loss: 0.005339442286640406\n",
      "Epoch: 606, Train Loss: 7.217712663987186e-06\n",
      "Epoch: 606, Validation Loss: 0.005339411087334156\n",
      "Epoch: 607, Train Loss: 7.217669462988852e-06\n",
      "Epoch: 607, Validation Loss: 0.0053393784910440445\n",
      "Epoch: 608, Train Loss: 7.217626716737868e-06\n",
      "Epoch: 608, Validation Loss: 0.005339347291737795\n",
      "Epoch: 609, Train Loss: 7.217583970486885e-06\n",
      "Epoch: 609, Validation Loss: 0.005339315626770258\n",
      "Epoch: 610, Train Loss: 7.217541224235902e-06\n",
      "Epoch: 610, Validation Loss: 0.005339283961802721\n",
      "Epoch: 611, Train Loss: 7.217498932732269e-06\n",
      "Epoch: 611, Validation Loss: 0.005339252296835184\n",
      "Epoch: 612, Train Loss: 7.217456186481286e-06\n",
      "Epoch: 612, Validation Loss: 0.005339220631867647\n",
      "Epoch: 613, Train Loss: 7.217414804472355e-06\n",
      "Epoch: 613, Validation Loss: 0.005339189898222685\n",
      "Epoch: 614, Train Loss: 7.217372512968723e-06\n",
      "Epoch: 614, Validation Loss: 0.00533915963023901\n",
      "Epoch: 615, Train Loss: 7.217331130959792e-06\n",
      "Epoch: 615, Validation Loss: 0.0053391302935779095\n",
      "Epoch: 616, Train Loss: 7.2172892942035105e-06\n",
      "Epoch: 616, Validation Loss: 0.005339098162949085\n",
      "Epoch: 617, Train Loss: 7.21724791219458e-06\n",
      "Epoch: 617, Validation Loss: 0.0053390683606266975\n",
      "Epoch: 618, Train Loss: 7.217206984933e-06\n",
      "Epoch: 618, Validation Loss: 0.005339037626981735\n",
      "Epoch: 619, Train Loss: 7.21716605767142e-06\n",
      "Epoch: 619, Validation Loss: 0.0053390078246593475\n",
      "Epoch: 620, Train Loss: 7.2171251304098405e-06\n",
      "Epoch: 620, Validation Loss: 0.005338976625353098\n",
      "Epoch: 621, Train Loss: 7.217084203148261e-06\n",
      "Epoch: 621, Validation Loss: 0.0053389472886919975\n",
      "Epoch: 622, Train Loss: 7.217044185381383e-06\n",
      "Epoch: 622, Validation Loss: 0.005338917952030897\n",
      "Epoch: 623, Train Loss: 7.217003712867154e-06\n",
      "Epoch: 623, Validation Loss: 0.005338889081031084\n",
      "Epoch: 624, Train Loss: 7.216963695100276e-06\n",
      "Epoch: 624, Validation Loss: 0.005338858813047409\n",
      "Epoch: 625, Train Loss: 7.216923677333398e-06\n",
      "Epoch: 625, Validation Loss: 0.005338829476386309\n",
      "Epoch: 626, Train Loss: 7.21688365956652e-06\n",
      "Epoch: 626, Validation Loss: 0.005338799674063921\n",
      "Epoch: 627, Train Loss: 7.216843641799642e-06\n",
      "Epoch: 627, Validation Loss: 0.005338771268725395\n",
      "Epoch: 628, Train Loss: 7.2168049882748164e-06\n",
      "Epoch: 628, Validation Loss: 0.005338742397725582\n",
      "Epoch: 629, Train Loss: 7.216764970507938e-06\n",
      "Epoch: 629, Validation Loss: 0.005338713526725769\n",
      "Epoch: 630, Train Loss: 7.216726316983113e-06\n",
      "Epoch: 630, Validation Loss: 0.005338682793080807\n",
      "Epoch: 631, Train Loss: 7.216687208710937e-06\n",
      "Epoch: 631, Validation Loss: 0.005338655784726143\n",
      "Epoch: 632, Train Loss: 7.2166485551861115e-06\n",
      "Epoch: 632, Validation Loss: 0.005338625516742468\n",
      "Epoch: 633, Train Loss: 7.216610356408637e-06\n",
      "Epoch: 633, Validation Loss: 0.005338598042726517\n",
      "Epoch: 634, Train Loss: 7.216571248136461e-06\n",
      "Epoch: 634, Validation Loss: 0.005338570103049278\n",
      "Epoch: 635, Train Loss: 7.216533504106337e-06\n",
      "Epoch: 635, Validation Loss: 0.005338540766388178\n",
      "Epoch: 636, Train Loss: 7.216494850581512e-06\n",
      "Epoch: 636, Validation Loss: 0.005338514223694801\n",
      "Epoch: 637, Train Loss: 7.216457106551388e-06\n",
      "Epoch: 637, Validation Loss: 0.0053384858183562756\n",
      "Epoch: 638, Train Loss: 7.216419362521265e-06\n",
      "Epoch: 638, Validation Loss: 0.005338457878679037\n",
      "Epoch: 639, Train Loss: 7.216381618491141e-06\n",
      "Epoch: 639, Validation Loss: 0.005338429473340511\n",
      "Epoch: 640, Train Loss: 7.2163443292083684e-06\n",
      "Epoch: 640, Validation Loss: 0.00533840199932456\n",
      "Epoch: 641, Train Loss: 7.216306585178245e-06\n",
      "Epoch: 641, Validation Loss: 0.005338375456631184\n",
      "Epoch: 642, Train Loss: 7.216269295895472e-06\n",
      "Epoch: 642, Validation Loss: 0.00533834844827652\n",
      "Epoch: 643, Train Loss: 7.216232916107401e-06\n",
      "Epoch: 643, Validation Loss: 0.005338320508599281\n",
      "Epoch: 644, Train Loss: 7.2161956268246286e-06\n",
      "Epoch: 644, Validation Loss: 0.00533829303458333\n",
      "Epoch: 645, Train Loss: 7.216158792289207e-06\n",
      "Epoch: 645, Validation Loss: 0.005338265560567379\n",
      "Epoch: 646, Train Loss: 7.216122412501136e-06\n",
      "Epoch: 646, Validation Loss: 0.0053382390178740025\n",
      "Epoch: 647, Train Loss: 7.216086032713065e-06\n",
      "Epoch: 647, Validation Loss: 0.005338212009519339\n",
      "Epoch: 648, Train Loss: 7.216050107672345e-06\n",
      "Epoch: 648, Validation Loss: 0.005338185932487249\n",
      "Epoch: 649, Train Loss: 7.216013273136923e-06\n",
      "Epoch: 649, Validation Loss: 0.005338157992810011\n",
      "Epoch: 650, Train Loss: 7.215977348096203e-06\n",
      "Epoch: 650, Validation Loss: 0.005338132381439209\n",
      "Epoch: 651, Train Loss: 7.215941877802834e-06\n",
      "Epoch: 651, Validation Loss: 0.0053381058387458324\n",
      "Epoch: 652, Train Loss: 7.215905952762114e-06\n",
      "Epoch: 652, Validation Loss: 0.005338079296052456\n",
      "Epoch: 653, Train Loss: 7.215870937216096e-06\n",
      "Epoch: 653, Validation Loss: 0.005338053684681654\n",
      "Epoch: 654, Train Loss: 7.2158354669227265e-06\n",
      "Epoch: 654, Validation Loss: 0.0053380271419882774\n",
      "Epoch: 655, Train Loss: 7.215799996629357e-06\n",
      "Epoch: 655, Validation Loss: 0.005338001996278763\n",
      "Epoch: 656, Train Loss: 7.21576543583069e-06\n",
      "Epoch: 656, Validation Loss: 0.005337975919246674\n",
      "Epoch: 657, Train Loss: 7.215729965537321e-06\n",
      "Epoch: 657, Validation Loss: 0.00533794891089201\n",
      "Epoch: 658, Train Loss: 7.2156954047386535e-06\n",
      "Epoch: 658, Validation Loss: 0.00533792469650507\n",
      "Epoch: 659, Train Loss: 7.215660843939986e-06\n",
      "Epoch: 659, Validation Loss: 0.005337899085134268\n",
      "Epoch: 660, Train Loss: 7.21562673788867e-06\n",
      "Epoch: 660, Validation Loss: 0.005337873939424753\n",
      "Epoch: 661, Train Loss: 7.215591722342651e-06\n",
      "Epoch: 661, Validation Loss: 0.005337848328053951\n",
      "Epoch: 662, Train Loss: 7.215557616291335e-06\n",
      "Epoch: 662, Validation Loss: 0.005337823648005724\n",
      "Epoch: 663, Train Loss: 7.215523510240018e-06\n",
      "Epoch: 663, Validation Loss: 0.005337798502296209\n",
      "Epoch: 664, Train Loss: 7.215490313683404e-06\n",
      "Epoch: 664, Validation Loss: 0.00533777242526412\n",
      "Epoch: 665, Train Loss: 7.215456207632087e-06\n",
      "Epoch: 665, Validation Loss: 0.005337749142199755\n",
      "Epoch: 666, Train Loss: 7.2154225563281216e-06\n",
      "Epoch: 666, Validation Loss: 0.0053377230651676655\n",
      "Epoch: 667, Train Loss: 7.215389359771507e-06\n",
      "Epoch: 667, Validation Loss: 0.005337699316442013\n",
      "Epoch: 668, Train Loss: 7.215355708467541e-06\n",
      "Epoch: 668, Validation Loss: 0.0053376746363937855\n",
      "Epoch: 669, Train Loss: 7.2153229666582774e-06\n",
      "Epoch: 669, Validation Loss: 0.0053376504220068455\n",
      "Epoch: 670, Train Loss: 7.215290224849014e-06\n",
      "Epoch: 670, Validation Loss: 0.005337625276297331\n",
      "Epoch: 671, Train Loss: 7.215257028292399e-06\n",
      "Epoch: 671, Validation Loss: 0.005337601527571678\n",
      "Epoch: 672, Train Loss: 7.215224286483135e-06\n",
      "Epoch: 672, Validation Loss: 0.005337578244507313\n",
      "Epoch: 673, Train Loss: 7.215191544673871e-06\n",
      "Epoch: 673, Validation Loss: 0.005337553098797798\n",
      "Epoch: 674, Train Loss: 7.215159257611958e-06\n",
      "Epoch: 674, Validation Loss: 0.00533753028139472\n",
      "Epoch: 675, Train Loss: 7.215126970550045e-06\n",
      "Epoch: 675, Validation Loss: 0.005337505601346493\n",
      "Epoch: 676, Train Loss: 7.2150946834881324e-06\n",
      "Epoch: 676, Validation Loss: 0.00533748185262084\n",
      "Epoch: 677, Train Loss: 7.2150623964262195e-06\n",
      "Epoch: 677, Validation Loss: 0.005337458569556475\n",
      "Epoch: 678, Train Loss: 7.2150305641116574e-06\n",
      "Epoch: 678, Validation Loss: 0.005337435286492109\n",
      "Epoch: 679, Train Loss: 7.214998731797095e-06\n",
      "Epoch: 679, Validation Loss: 0.005337411072105169\n",
      "Epoch: 680, Train Loss: 7.214967354229884e-06\n",
      "Epoch: 680, Validation Loss: 0.0053373887203633785\n",
      "Epoch: 681, Train Loss: 7.214935521915322e-06\n",
      "Epoch: 681, Validation Loss: 0.005337364971637726\n",
      "Epoch: 682, Train Loss: 7.214904599095462e-06\n",
      "Epoch: 682, Validation Loss: 0.0053373416885733604\n",
      "Epoch: 683, Train Loss: 7.2148727667809e-06\n",
      "Epoch: 683, Validation Loss: 0.005337318871170282\n",
      "Epoch: 684, Train Loss: 7.2148418439610396e-06\n",
      "Epoch: 684, Validation Loss: 0.005337296519428492\n",
      "Epoch: 685, Train Loss: 7.214810921141179e-06\n",
      "Epoch: 685, Validation Loss: 0.005337273236364126\n",
      "Epoch: 686, Train Loss: 7.214779998321319e-06\n",
      "Epoch: 686, Validation Loss: 0.005337249953299761\n",
      "Epoch: 687, Train Loss: 7.21474953024881e-06\n",
      "Epoch: 687, Validation Loss: 0.005337227135896683\n",
      "Epoch: 688, Train Loss: 7.214718607428949e-06\n",
      "Epoch: 688, Validation Loss: 0.005337204784154892\n",
      "Epoch: 689, Train Loss: 7.21468813935644e-06\n",
      "Epoch: 689, Validation Loss: 0.005337181966751814\n",
      "Epoch: 690, Train Loss: 7.2146576712839305e-06\n",
      "Epoch: 690, Validation Loss: 0.005337160546332598\n",
      "Epoch: 691, Train Loss: 7.214627657958772e-06\n",
      "Epoch: 691, Validation Loss: 0.00533713772892952\n",
      "Epoch: 692, Train Loss: 7.2145976446336135e-06\n",
      "Epoch: 692, Validation Loss: 0.005337115377187729\n",
      "Epoch: 693, Train Loss: 7.214567176561104e-06\n",
      "Epoch: 693, Validation Loss: 0.005337093491107225\n",
      "Epoch: 694, Train Loss: 7.2145376179832965e-06\n",
      "Epoch: 694, Validation Loss: 0.005337072536349297\n",
      "Epoch: 695, Train Loss: 7.214508059405489e-06\n",
      "Epoch: 695, Validation Loss: 0.005337050650268793\n",
      "Epoch: 696, Train Loss: 7.214478955575032e-06\n",
      "Epoch: 696, Validation Loss: 0.005337027832865715\n",
      "Epoch: 697, Train Loss: 7.214448942249874e-06\n",
      "Epoch: 697, Validation Loss: 0.005337006878107786\n",
      "Epoch: 698, Train Loss: 7.214419838419417e-06\n",
      "Epoch: 698, Validation Loss: 0.005336984992027283\n",
      "Epoch: 699, Train Loss: 7.21439073458896e-06\n",
      "Epoch: 699, Validation Loss: 0.005336963105946779\n",
      "Epoch: 700, Train Loss: 7.2143616307585035e-06\n",
      "Epoch: 700, Validation Loss: 0.005336941219866276\n",
      "Epoch: 701, Train Loss: 7.214332526928047e-06\n",
      "Epoch: 701, Validation Loss: 0.005336920730769634\n",
      "Epoch: 702, Train Loss: 7.214303877844941e-06\n",
      "Epoch: 702, Validation Loss: 0.005336898844689131\n",
      "Epoch: 703, Train Loss: 7.214274774014484e-06\n",
      "Epoch: 703, Validation Loss: 0.005336877424269915\n",
      "Epoch: 704, Train Loss: 7.214246124931378e-06\n",
      "Epoch: 704, Validation Loss: 0.005336856469511986\n",
      "Epoch: 705, Train Loss: 7.214217930595623e-06\n",
      "Epoch: 705, Validation Loss: 0.005336835980415344\n",
      "Epoch: 706, Train Loss: 7.2141892815125175e-06\n",
      "Epoch: 706, Validation Loss: 0.005336815025657415\n",
      "Epoch: 707, Train Loss: 7.214161541924113e-06\n",
      "Epoch: 707, Validation Loss: 0.005336794536560774\n",
      "Epoch: 708, Train Loss: 7.2141328928410076e-06\n",
      "Epoch: 708, Validation Loss: 0.005336773581802845\n",
      "Epoch: 709, Train Loss: 7.2141051532526035e-06\n",
      "Epoch: 709, Validation Loss: 0.005336752627044916\n",
      "Epoch: 710, Train Loss: 7.2140769589168485e-06\n",
      "Epoch: 710, Validation Loss: 0.005336732137948275\n",
      "Epoch: 711, Train Loss: 7.2140492193284445e-06\n",
      "Epoch: 711, Validation Loss: 0.005336712580174208\n",
      "Epoch: 712, Train Loss: 7.214021934487391e-06\n",
      "Epoch: 712, Validation Loss: 0.005336692091077566\n",
      "Epoch: 713, Train Loss: 7.213994649646338e-06\n",
      "Epoch: 713, Validation Loss: 0.005336671601980925\n",
      "Epoch: 714, Train Loss: 7.213966910057934e-06\n",
      "Epoch: 714, Validation Loss: 0.005336651112884283\n",
      "Epoch: 715, Train Loss: 7.213939625216881e-06\n",
      "Epoch: 715, Validation Loss: 0.005336631089448929\n",
      "Epoch: 716, Train Loss: 7.2139127951231785e-06\n",
      "Epoch: 716, Validation Loss: 0.005336611066013575\n",
      "Epoch: 717, Train Loss: 7.213885510282125e-06\n",
      "Epoch: 717, Validation Loss: 0.005336590576916933\n",
      "Epoch: 718, Train Loss: 7.213858225441072e-06\n",
      "Epoch: 718, Validation Loss: 0.005336571019142866\n",
      "Epoch: 719, Train Loss: 7.213831850094721e-06\n",
      "Epoch: 719, Validation Loss: 0.0053365519270300865\n",
      "Epoch: 720, Train Loss: 7.213805020001018e-06\n",
      "Epoch: 720, Validation Loss: 0.005336532834917307\n",
      "Epoch: 721, Train Loss: 7.213778189907316e-06\n",
      "Epoch: 721, Validation Loss: 0.005336511880159378\n",
      "Epoch: 722, Train Loss: 7.213751814560965e-06\n",
      "Epoch: 722, Validation Loss: 0.0053364927880465984\n",
      "Epoch: 723, Train Loss: 7.213725893961964e-06\n",
      "Epoch: 723, Validation Loss: 0.005336473695933819\n",
      "Epoch: 724, Train Loss: 7.213699063868262e-06\n",
      "Epoch: 724, Validation Loss: 0.005336453672498465\n",
      "Epoch: 725, Train Loss: 7.213673143269261e-06\n",
      "Epoch: 725, Validation Loss: 0.00533643551170826\n",
      "Epoch: 726, Train Loss: 7.213647222670261e-06\n",
      "Epoch: 726, Validation Loss: 0.005336415953934193\n",
      "Epoch: 727, Train Loss: 7.213620847323909e-06\n",
      "Epoch: 727, Validation Loss: 0.005336396861821413\n",
      "Epoch: 728, Train Loss: 7.21359538147226e-06\n",
      "Epoch: 728, Validation Loss: 0.005336377769708633\n",
      "Epoch: 729, Train Loss: 7.213569460873259e-06\n",
      "Epoch: 729, Validation Loss: 0.005336358677595854\n",
      "Epoch: 730, Train Loss: 7.21354399502161e-06\n",
      "Epoch: 730, Validation Loss: 0.0053363400511443615\n",
      "Epoch: 731, Train Loss: 7.21351852916996e-06\n",
      "Epoch: 731, Validation Loss: 0.005336320959031582\n",
      "Epoch: 732, Train Loss: 7.213493518065661e-06\n",
      "Epoch: 732, Validation Loss: 0.005336302798241377\n",
      "Epoch: 733, Train Loss: 7.2134680522140115e-06\n",
      "Epoch: 733, Validation Loss: 0.00533628324046731\n",
      "Epoch: 734, Train Loss: 7.213442586362362e-06\n",
      "Epoch: 734, Validation Loss: 0.005336265079677105\n",
      "Epoch: 735, Train Loss: 7.213417575258063e-06\n",
      "Epoch: 735, Validation Loss: 0.005336247384548187\n",
      "Epoch: 736, Train Loss: 7.213393018901115e-06\n",
      "Epoch: 736, Validation Loss: 0.005336228758096695\n",
      "Epoch: 737, Train Loss: 7.2133680077968165e-06\n",
      "Epoch: 737, Validation Loss: 0.005336210131645203\n",
      "Epoch: 738, Train Loss: 7.213343451439869e-06\n",
      "Epoch: 738, Validation Loss: 0.005336191970854998\n",
      "Epoch: 739, Train Loss: 7.21331844033557e-06\n",
      "Epoch: 739, Validation Loss: 0.005336173344403505\n",
      "Epoch: 740, Train Loss: 7.213293883978622e-06\n",
      "Epoch: 740, Validation Loss: 0.005336155649274588\n",
      "Epoch: 741, Train Loss: 7.213269782369025e-06\n",
      "Epoch: 741, Validation Loss: 0.005336137022823095\n",
      "Epoch: 742, Train Loss: 7.213244771264726e-06\n",
      "Epoch: 742, Validation Loss: 0.005336119793355465\n",
      "Epoch: 743, Train Loss: 7.213220669655129e-06\n",
      "Epoch: 743, Validation Loss: 0.005336101166903973\n",
      "Epoch: 744, Train Loss: 7.213196568045532e-06\n",
      "Epoch: 744, Validation Loss: 0.0053360844030976295\n",
      "Epoch: 745, Train Loss: 7.213172921183286e-06\n",
      "Epoch: 745, Validation Loss: 0.005336065776646137\n",
      "Epoch: 746, Train Loss: 7.213148819573689e-06\n",
      "Epoch: 746, Validation Loss: 0.005336049012839794\n",
      "Epoch: 747, Train Loss: 7.213125172711443e-06\n",
      "Epoch: 747, Validation Loss: 0.0053360313177108765\n",
      "Epoch: 748, Train Loss: 7.213101071101846e-06\n",
      "Epoch: 748, Validation Loss: 0.0053360131569206715\n",
      "Epoch: 749, Train Loss: 7.2130774242396e-06\n",
      "Epoch: 749, Validation Loss: 0.005335995927453041\n",
      "Epoch: 750, Train Loss: 7.213053777377354e-06\n",
      "Epoch: 750, Validation Loss: 0.005335979163646698\n",
      "Epoch: 751, Train Loss: 7.21303104000981e-06\n",
      "Epoch: 751, Validation Loss: 0.00533596146851778\n",
      "Epoch: 752, Train Loss: 7.2130073931475636e-06\n",
      "Epoch: 752, Validation Loss: 0.00533594423905015\n",
      "Epoch: 753, Train Loss: 7.2129837462853175e-06\n",
      "Epoch: 753, Validation Loss: 0.005335926543921232\n",
      "Epoch: 754, Train Loss: 7.212961008917773e-06\n",
      "Epoch: 754, Validation Loss: 0.005335910711437464\n",
      "Epoch: 755, Train Loss: 7.212937816802878e-06\n",
      "Epoch: 755, Validation Loss: 0.005335893016308546\n",
      "Epoch: 756, Train Loss: 7.212915079435334e-06\n",
      "Epoch: 756, Validation Loss: 0.005335876252502203\n",
      "Epoch: 757, Train Loss: 7.212892342067789e-06\n",
      "Epoch: 757, Validation Loss: 0.00533585948869586\n",
      "Epoch: 758, Train Loss: 7.212869149952894e-06\n",
      "Epoch: 758, Validation Loss: 0.005335843190550804\n",
      "Epoch: 759, Train Loss: 7.212846867332701e-06\n",
      "Epoch: 759, Validation Loss: 0.0053358254954218864\n",
      "Epoch: 760, Train Loss: 7.212824129965156e-06\n",
      "Epoch: 760, Validation Loss: 0.005335808731615543\n",
      "Epoch: 761, Train Loss: 7.212801847344963e-06\n",
      "Epoch: 761, Validation Loss: 0.005335793364793062\n",
      "Epoch: 762, Train Loss: 7.2127791099774186e-06\n",
      "Epoch: 762, Validation Loss: 0.005335776135325432\n",
      "Epoch: 763, Train Loss: 7.212756827357225e-06\n",
      "Epoch: 763, Validation Loss: 0.005335760302841663\n",
      "Epoch: 764, Train Loss: 7.2127354542317335e-06\n",
      "Epoch: 764, Validation Loss: 0.005335743073374033\n",
      "Epoch: 765, Train Loss: 7.21271317161154e-06\n",
      "Epoch: 765, Validation Loss: 0.005335726775228977\n",
      "Epoch: 766, Train Loss: 7.212690888991347e-06\n",
      "Epoch: 766, Validation Loss: 0.005335710942745209\n",
      "Epoch: 767, Train Loss: 7.212669061118504e-06\n",
      "Epoch: 767, Validation Loss: 0.005335695575922728\n",
      "Epoch: 768, Train Loss: 7.2126472332456615e-06\n",
      "Epoch: 768, Validation Loss: 0.005335679277777672\n",
      "Epoch: 769, Train Loss: 7.212625405372819e-06\n",
      "Epoch: 769, Validation Loss: 0.005335662979632616\n",
      "Epoch: 770, Train Loss: 7.212604032247327e-06\n",
      "Epoch: 770, Validation Loss: 0.00533564668148756\n",
      "Epoch: 771, Train Loss: 7.212582204374485e-06\n",
      "Epoch: 771, Validation Loss: 0.005335631780326366\n",
      "Epoch: 772, Train Loss: 7.212561285996344e-06\n",
      "Epoch: 772, Validation Loss: 0.005335615482181311\n",
      "Epoch: 773, Train Loss: 7.212539458123501e-06\n",
      "Epoch: 773, Validation Loss: 0.005335599649697542\n",
      "Epoch: 774, Train Loss: 7.212518539745361e-06\n",
      "Epoch: 774, Validation Loss: 0.005335584282875061\n",
      "Epoch: 775, Train Loss: 7.212497166619869e-06\n",
      "Epoch: 775, Validation Loss: 0.00533556891605258\n",
      "Epoch: 776, Train Loss: 7.212476248241728e-06\n",
      "Epoch: 776, Validation Loss: 0.005335553083568811\n",
      "Epoch: 777, Train Loss: 7.212455329863587e-06\n",
      "Epoch: 777, Validation Loss: 0.00533553771674633\n",
      "Epoch: 778, Train Loss: 7.212433956738096e-06\n",
      "Epoch: 778, Validation Loss: 0.005335522349923849\n",
      "Epoch: 779, Train Loss: 7.212413493107306e-06\n",
      "Epoch: 779, Validation Loss: 0.005335506517440081\n",
      "Epoch: 780, Train Loss: 7.212392574729165e-06\n",
      "Epoch: 780, Validation Loss: 0.005335491616278887\n",
      "Epoch: 781, Train Loss: 7.212372111098375e-06\n",
      "Epoch: 781, Validation Loss: 0.005335476715117693\n",
      "Epoch: 782, Train Loss: 7.212351647467585e-06\n",
      "Epoch: 782, Validation Loss: 0.005335461348295212\n",
      "Epoch: 783, Train Loss: 7.212331183836795e-06\n",
      "Epoch: 783, Validation Loss: 0.005335445515811443\n",
      "Epoch: 784, Train Loss: 7.2123107202060055e-06\n",
      "Epoch: 784, Validation Loss: 0.005335432011634111\n",
      "Epoch: 785, Train Loss: 7.2122907113225665e-06\n",
      "Epoch: 785, Validation Loss: 0.00533541664481163\n",
      "Epoch: 786, Train Loss: 7.2122707024391275e-06\n",
      "Epoch: 786, Validation Loss: 0.005335401277989149\n",
      "Epoch: 787, Train Loss: 7.212250238808338e-06\n",
      "Epoch: 787, Validation Loss: 0.005335386376827955\n",
      "Epoch: 788, Train Loss: 7.212230229924899e-06\n",
      "Epoch: 788, Validation Loss: 0.005335372406989336\n",
      "Epoch: 789, Train Loss: 7.21221022104146e-06\n",
      "Epoch: 789, Validation Loss: 0.0053353579714894295\n",
      "Epoch: 790, Train Loss: 7.2121906669053715e-06\n",
      "Epoch: 790, Validation Loss: 0.005335342604666948\n",
      "Epoch: 791, Train Loss: 7.2121706580219325e-06\n",
      "Epoch: 791, Validation Loss: 0.005335328169167042\n",
      "Epoch: 792, Train Loss: 7.2121506491384935e-06\n",
      "Epoch: 792, Validation Loss: 0.0053353141993284225\n",
      "Epoch: 793, Train Loss: 7.212131549749756e-06\n",
      "Epoch: 793, Validation Loss: 0.005335299298167229\n",
      "Epoch: 794, Train Loss: 7.212111995613668e-06\n",
      "Epoch: 794, Validation Loss: 0.005335284862667322\n",
      "Epoch: 795, Train Loss: 7.21209244147758e-06\n",
      "Epoch: 795, Validation Loss: 0.005335270892828703\n",
      "Epoch: 796, Train Loss: 7.212073342088843e-06\n",
      "Epoch: 796, Validation Loss: 0.005335255991667509\n",
      "Epoch: 797, Train Loss: 7.2120542427001055e-06\n",
      "Epoch: 797, Validation Loss: 0.00533524202182889\n",
      "Epoch: 798, Train Loss: 7.212035143311368e-06\n",
      "Epoch: 798, Validation Loss: 0.005335228051990271\n",
      "Epoch: 799, Train Loss: 7.21201558917528e-06\n",
      "Epoch: 799, Validation Loss: 0.005335213616490364\n",
      "Epoch: 800, Train Loss: 7.211996489786543e-06\n",
      "Epoch: 800, Validation Loss: 0.005335199646651745\n",
      "Epoch: 801, Train Loss: 7.211977390397806e-06\n",
      "Epoch: 801, Validation Loss: 0.005335185676813126\n",
      "Epoch: 802, Train Loss: 7.211958745756419e-06\n",
      "Epoch: 802, Validation Loss: 0.005335172172635794\n",
      "Epoch: 803, Train Loss: 7.211940101115033e-06\n",
      "Epoch: 803, Validation Loss: 0.005335158668458462\n",
      "Epoch: 804, Train Loss: 7.211921456473647e-06\n",
      "Epoch: 804, Validation Loss: 0.005335144232958555\n",
      "Epoch: 805, Train Loss: 7.2119023570849095e-06\n",
      "Epoch: 805, Validation Loss: 0.005335130728781223\n",
      "Epoch: 806, Train Loss: 7.211884167190874e-06\n",
      "Epoch: 806, Validation Loss: 0.005335116758942604\n",
      "Epoch: 807, Train Loss: 7.2118659772968385e-06\n",
      "Epoch: 807, Validation Loss: 0.005335103254765272\n",
      "Epoch: 808, Train Loss: 7.211847332655452e-06\n",
      "Epoch: 808, Validation Loss: 0.00533508975058794\n",
      "Epoch: 809, Train Loss: 7.211829142761417e-06\n",
      "Epoch: 809, Validation Loss: 0.005335076712071896\n",
      "Epoch: 810, Train Loss: 7.21181049812003e-06\n",
      "Epoch: 810, Validation Loss: 0.005335062742233276\n",
      "Epoch: 811, Train Loss: 7.211792762973346e-06\n",
      "Epoch: 811, Validation Loss: 0.0053350492380559444\n",
      "Epoch: 812, Train Loss: 7.21177457307931e-06\n",
      "Epoch: 812, Validation Loss: 0.0053350361995399\n",
      "Epoch: 813, Train Loss: 7.211756837932626e-06\n",
      "Epoch: 813, Validation Loss: 0.005335022229701281\n",
      "Epoch: 814, Train Loss: 7.21173864803859e-06\n",
      "Epoch: 814, Validation Loss: 0.005335009191185236\n",
      "Epoch: 815, Train Loss: 7.211720912891906e-06\n",
      "Epoch: 815, Validation Loss: 0.005334996152669191\n",
      "Epoch: 816, Train Loss: 7.21170272299787e-06\n",
      "Epoch: 816, Validation Loss: 0.005334983579814434\n",
      "Epoch: 817, Train Loss: 7.211685442598537e-06\n",
      "Epoch: 817, Validation Loss: 0.005334970075637102\n",
      "Epoch: 818, Train Loss: 7.211667252704501e-06\n",
      "Epoch: 818, Validation Loss: 0.005334957502782345\n",
      "Epoch: 819, Train Loss: 7.2116499723051675e-06\n",
      "Epoch: 819, Validation Loss: 0.0053349449299275875\n",
      "Epoch: 820, Train Loss: 7.211632691905834e-06\n",
      "Epoch: 820, Validation Loss: 0.005334931425750256\n",
      "Epoch: 821, Train Loss: 7.2116154115065e-06\n",
      "Epoch: 821, Validation Loss: 0.005334919318556786\n",
      "Epoch: 822, Train Loss: 7.211598131107166e-06\n",
      "Epoch: 822, Validation Loss: 0.005334905814379454\n",
      "Epoch: 823, Train Loss: 7.211580850707833e-06\n",
      "Epoch: 823, Validation Loss: 0.005334893241524696\n",
      "Epoch: 824, Train Loss: 7.211563115561148e-06\n",
      "Epoch: 824, Validation Loss: 0.005334880203008652\n",
      "Epoch: 825, Train Loss: 7.2115458351618145e-06\n",
      "Epoch: 825, Validation Loss: 0.005334868561476469\n",
      "Epoch: 826, Train Loss: 7.211529009509832e-06\n",
      "Epoch: 826, Validation Loss: 0.005334855988621712\n",
      "Epoch: 827, Train Loss: 7.211511729110498e-06\n",
      "Epoch: 827, Validation Loss: 0.005334842950105667\n",
      "Epoch: 828, Train Loss: 7.211494903458515e-06\n",
      "Epoch: 828, Validation Loss: 0.005334830842912197\n",
      "Epoch: 829, Train Loss: 7.211478532553883e-06\n",
      "Epoch: 829, Validation Loss: 0.0053348178043961525\n",
      "Epoch: 830, Train Loss: 7.2114617069019005e-06\n",
      "Epoch: 830, Validation Loss: 0.0053348056972026825\n",
      "Epoch: 831, Train Loss: 7.211444881249918e-06\n",
      "Epoch: 831, Validation Loss: 0.005334793124347925\n",
      "Epoch: 832, Train Loss: 7.211428055597935e-06\n",
      "Epoch: 832, Validation Loss: 0.005334781017154455\n",
      "Epoch: 833, Train Loss: 7.211412139440654e-06\n",
      "Epoch: 833, Validation Loss: 0.005334768909960985\n",
      "Epoch: 834, Train Loss: 7.211395313788671e-06\n",
      "Epoch: 834, Validation Loss: 0.005334756802767515\n",
      "Epoch: 835, Train Loss: 7.21137939763139e-06\n",
      "Epoch: 835, Validation Loss: 0.0053347451612353325\n",
      "Epoch: 836, Train Loss: 7.211362571979407e-06\n",
      "Epoch: 836, Validation Loss: 0.0053347330540418625\n",
      "Epoch: 837, Train Loss: 7.211346201074775e-06\n",
      "Epoch: 837, Validation Loss: 0.0053347209468483925\n",
      "Epoch: 838, Train Loss: 7.211330284917494e-06\n",
      "Epoch: 838, Validation Loss: 0.00533470930531621\n",
      "Epoch: 839, Train Loss: 7.211314368760213e-06\n",
      "Epoch: 839, Validation Loss: 0.00533469719812274\n",
      "Epoch: 840, Train Loss: 7.211297997855581e-06\n",
      "Epoch: 840, Validation Loss: 0.00533468509092927\n",
      "Epoch: 841, Train Loss: 7.2112820816983e-06\n",
      "Epoch: 841, Validation Loss: 0.0053346729837358\n",
      "Epoch: 842, Train Loss: 7.211265710793668e-06\n",
      "Epoch: 842, Validation Loss: 0.005334661342203617\n",
      "Epoch: 843, Train Loss: 7.211250249383738e-06\n",
      "Epoch: 843, Validation Loss: 0.005334650166332722\n",
      "Epoch: 844, Train Loss: 7.211234333226457e-06\n",
      "Epoch: 844, Validation Loss: 0.005334638524800539\n",
      "Epoch: 845, Train Loss: 7.211218871816527e-06\n",
      "Epoch: 845, Validation Loss: 0.005334626883268356\n",
      "Epoch: 846, Train Loss: 7.211203410406597e-06\n",
      "Epoch: 846, Validation Loss: 0.005334614776074886\n",
      "Epoch: 847, Train Loss: 7.211187039501965e-06\n",
      "Epoch: 847, Validation Loss: 0.005334603600203991\n",
      "Epoch: 848, Train Loss: 7.211171578092035e-06\n",
      "Epoch: 848, Validation Loss: 0.005334591958671808\n",
      "Epoch: 849, Train Loss: 7.211156116682105e-06\n",
      "Epoch: 849, Validation Loss: 0.005334580782800913\n",
      "Epoch: 850, Train Loss: 7.2111411100195255e-06\n",
      "Epoch: 850, Validation Loss: 0.0053345696069300175\n",
      "Epoch: 851, Train Loss: 7.211125648609595e-06\n",
      "Epoch: 851, Validation Loss: 0.005334557965397835\n",
      "Epoch: 852, Train Loss: 7.211110187199665e-06\n",
      "Epoch: 852, Validation Loss: 0.005334546789526939\n",
      "Epoch: 853, Train Loss: 7.211095180537086e-06\n",
      "Epoch: 853, Validation Loss: 0.005334535613656044\n",
      "Epoch: 854, Train Loss: 7.211079719127156e-06\n",
      "Epoch: 854, Validation Loss: 0.005334524437785149\n",
      "Epoch: 855, Train Loss: 7.2110651672119275e-06\n",
      "Epoch: 855, Validation Loss: 0.0053345137275755405\n",
      "Epoch: 856, Train Loss: 7.211050160549348e-06\n",
      "Epoch: 856, Validation Loss: 0.005334502551704645\n",
      "Epoch: 857, Train Loss: 7.211035153886769e-06\n",
      "Epoch: 857, Validation Loss: 0.00533449137583375\n",
      "Epoch: 858, Train Loss: 7.211020601971541e-06\n",
      "Epoch: 858, Validation Loss: 0.005334480665624142\n",
      "Epoch: 859, Train Loss: 7.211005595308961e-06\n",
      "Epoch: 859, Validation Loss: 0.005334469955414534\n",
      "Epoch: 860, Train Loss: 7.210990588646382e-06\n",
      "Epoch: 860, Validation Loss: 0.005334458313882351\n",
      "Epoch: 861, Train Loss: 7.210975581983803e-06\n",
      "Epoch: 861, Validation Loss: 0.00533444806933403\n",
      "Epoch: 862, Train Loss: 7.210961484815925e-06\n",
      "Epoch: 862, Validation Loss: 0.005334436893463135\n",
      "Epoch: 863, Train Loss: 7.210946478153346e-06\n",
      "Epoch: 863, Validation Loss: 0.005334426183253527\n",
      "Epoch: 864, Train Loss: 7.210932380985469e-06\n",
      "Epoch: 864, Validation Loss: 0.005334415938705206\n",
      "Epoch: 865, Train Loss: 7.210917374322889e-06\n",
      "Epoch: 865, Validation Loss: 0.0053344047628343105\n",
      "Epoch: 866, Train Loss: 7.210903277155012e-06\n",
      "Epoch: 866, Validation Loss: 0.0053343940526247025\n",
      "Epoch: 867, Train Loss: 7.210889179987134e-06\n",
      "Epoch: 867, Validation Loss: 0.005334384273737669\n",
      "Epoch: 868, Train Loss: 7.210874628071906e-06\n",
      "Epoch: 868, Validation Loss: 0.005334373563528061\n",
      "Epoch: 869, Train Loss: 7.2108605309040286e-06\n",
      "Epoch: 869, Validation Loss: 0.005334362853318453\n",
      "Epoch: 870, Train Loss: 7.210846433736151e-06\n",
      "Epoch: 870, Validation Loss: 0.005334352608770132\n",
      "Epoch: 871, Train Loss: 7.2108327913156245e-06\n",
      "Epoch: 871, Validation Loss: 0.005334341898560524\n",
      "Epoch: 872, Train Loss: 7.210818239400396e-06\n",
      "Epoch: 872, Validation Loss: 0.0053343321196734905\n",
      "Epoch: 873, Train Loss: 7.210804142232519e-06\n",
      "Epoch: 873, Validation Loss: 0.00533432187512517\n",
      "Epoch: 874, Train Loss: 7.210790499811992e-06\n",
      "Epoch: 874, Validation Loss: 0.005334311164915562\n",
      "Epoch: 875, Train Loss: 7.2107768573914655e-06\n",
      "Epoch: 875, Validation Loss: 0.005334301386028528\n",
      "Epoch: 876, Train Loss: 7.210762760223588e-06\n",
      "Epoch: 876, Validation Loss: 0.00533429067581892\n",
      "Epoch: 877, Train Loss: 7.210749117803061e-06\n",
      "Epoch: 877, Validation Loss: 0.005334280896931887\n",
      "Epoch: 878, Train Loss: 7.210735475382535e-06\n",
      "Epoch: 878, Validation Loss: 0.005334270652383566\n",
      "Epoch: 879, Train Loss: 7.210721832962008e-06\n",
      "Epoch: 879, Validation Loss: 0.005334260407835245\n",
      "Epoch: 880, Train Loss: 7.210708190541482e-06\n",
      "Epoch: 880, Validation Loss: 0.005334251094609499\n",
      "Epoch: 881, Train Loss: 7.210695002868306e-06\n",
      "Epoch: 881, Validation Loss: 0.005334240850061178\n",
      "Epoch: 882, Train Loss: 7.210681360447779e-06\n",
      "Epoch: 882, Validation Loss: 0.005334231071174145\n",
      "Epoch: 883, Train Loss: 7.210667718027253e-06\n",
      "Epoch: 883, Validation Loss: 0.005334221292287111\n",
      "Epoch: 884, Train Loss: 7.210654530354077e-06\n",
      "Epoch: 884, Validation Loss: 0.0053342110477387905\n",
      "Epoch: 885, Train Loss: 7.21064088793355e-06\n",
      "Epoch: 885, Validation Loss: 0.005334201268851757\n",
      "Epoch: 886, Train Loss: 7.210628155007726e-06\n",
      "Epoch: 886, Validation Loss: 0.005334191489964724\n",
      "Epoch: 887, Train Loss: 7.21061496733455e-06\n",
      "Epoch: 887, Validation Loss: 0.005334182642400265\n",
      "Epoch: 888, Train Loss: 7.210602234408725e-06\n",
      "Epoch: 888, Validation Loss: 0.005334172397851944\n",
      "Epoch: 889, Train Loss: 7.210589046735549e-06\n",
      "Epoch: 889, Validation Loss: 0.0053341626189649105\n",
      "Epoch: 890, Train Loss: 7.210575859062374e-06\n",
      "Epoch: 890, Validation Loss: 0.005334153771400452\n",
      "Epoch: 891, Train Loss: 7.210563126136549e-06\n",
      "Epoch: 891, Validation Loss: 0.005334143526852131\n",
      "Epoch: 892, Train Loss: 7.210549938463373e-06\n",
      "Epoch: 892, Validation Loss: 0.005334133747965097\n",
      "Epoch: 893, Train Loss: 7.210536750790197e-06\n",
      "Epoch: 893, Validation Loss: 0.005334124900400639\n",
      "Epoch: 894, Train Loss: 7.2105244726117235e-06\n",
      "Epoch: 894, Validation Loss: 0.005334115121513605\n",
      "Epoch: 895, Train Loss: 7.210511739685899e-06\n",
      "Epoch: 895, Validation Loss: 0.005334105808287859\n",
      "Epoch: 896, Train Loss: 7.210499006760074e-06\n",
      "Epoch: 896, Validation Loss: 0.0053340960294008255\n",
      "Epoch: 897, Train Loss: 7.210486273834249e-06\n",
      "Epoch: 897, Validation Loss: 0.005334087647497654\n",
      "Epoch: 898, Train Loss: 7.210473540908424e-06\n",
      "Epoch: 898, Validation Loss: 0.0053340778686106205\n",
      "Epoch: 899, Train Loss: 7.21046126272995e-06\n",
      "Epoch: 899, Validation Loss: 0.005334068555384874\n",
      "Epoch: 900, Train Loss: 7.210448984551476e-06\n",
      "Epoch: 900, Validation Loss: 0.0053340597078204155\n",
      "Epoch: 901, Train Loss: 7.210436706373002e-06\n",
      "Epoch: 901, Validation Loss: 0.005334050860255957\n",
      "Epoch: 902, Train Loss: 7.2104239734471776e-06\n",
      "Epoch: 902, Validation Loss: 0.005334041081368923\n",
      "Epoch: 903, Train Loss: 7.210411695268704e-06\n",
      "Epoch: 903, Validation Loss: 0.005334032233804464\n",
      "Epoch: 904, Train Loss: 7.21039941709023e-06\n",
      "Epoch: 904, Validation Loss: 0.0053340233862400055\n",
      "Epoch: 905, Train Loss: 7.210387593659107e-06\n",
      "Epoch: 905, Validation Loss: 0.005334014073014259\n",
      "Epoch: 906, Train Loss: 7.210375315480633e-06\n",
      "Epoch: 906, Validation Loss: 0.0053340052254498005\n",
      "Epoch: 907, Train Loss: 7.21036349204951e-06\n",
      "Epoch: 907, Validation Loss: 0.005333996843546629\n",
      "Epoch: 908, Train Loss: 7.210351213871036e-06\n",
      "Epoch: 908, Validation Loss: 0.005333987530320883\n",
      "Epoch: 909, Train Loss: 7.210339390439913e-06\n",
      "Epoch: 909, Validation Loss: 0.005333979148417711\n",
      "Epoch: 910, Train Loss: 7.21032756700879e-06\n",
      "Epoch: 910, Validation Loss: 0.005333970300853252\n",
      "Epoch: 911, Train Loss: 7.210314834082965e-06\n",
      "Epoch: 911, Validation Loss: 0.005333960987627506\n",
      "Epoch: 912, Train Loss: 7.210303465399193e-06\n",
      "Epoch: 912, Validation Loss: 0.005333952605724335\n",
      "Epoch: 913, Train Loss: 7.21029164196807e-06\n",
      "Epoch: 913, Validation Loss: 0.005333943758159876\n",
      "Epoch: 914, Train Loss: 7.2102798185369466e-06\n",
      "Epoch: 914, Validation Loss: 0.005333935376256704\n",
      "Epoch: 915, Train Loss: 7.210268449853174e-06\n",
      "Epoch: 915, Validation Loss: 0.005333926994353533\n",
      "Epoch: 916, Train Loss: 7.210256626422051e-06\n",
      "Epoch: 916, Validation Loss: 0.005333917681127787\n",
      "Epoch: 917, Train Loss: 7.210244802990928e-06\n",
      "Epoch: 917, Validation Loss: 0.005333909299224615\n",
      "Epoch: 918, Train Loss: 7.210233434307156e-06\n",
      "Epoch: 918, Validation Loss: 0.005333901382982731\n",
      "Epoch: 919, Train Loss: 7.210222065623384e-06\n",
      "Epoch: 919, Validation Loss: 0.005333892069756985\n",
      "Epoch: 920, Train Loss: 7.210210242192261e-06\n",
      "Epoch: 920, Validation Loss: 0.005333884619176388\n",
      "Epoch: 921, Train Loss: 7.21019932825584e-06\n",
      "Epoch: 921, Validation Loss: 0.005333875771611929\n",
      "Epoch: 922, Train Loss: 7.2101879595720675e-06\n",
      "Epoch: 922, Validation Loss: 0.005333867389708757\n",
      "Epoch: 923, Train Loss: 7.210177045635646e-06\n",
      "Epoch: 923, Validation Loss: 0.005333859007805586\n",
      "Epoch: 924, Train Loss: 7.210165222204523e-06\n",
      "Epoch: 924, Validation Loss: 0.005333850625902414\n",
      "Epoch: 925, Train Loss: 7.210153853520751e-06\n",
      "Epoch: 925, Validation Loss: 0.00533384270966053\n",
      "Epoch: 926, Train Loss: 7.21014293958433e-06\n",
      "Epoch: 926, Validation Loss: 0.005333833862096071\n",
      "Epoch: 927, Train Loss: 7.2101315709005576e-06\n",
      "Epoch: 927, Validation Loss: 0.005333825945854187\n",
      "Epoch: 928, Train Loss: 7.210121111711487e-06\n",
      "Epoch: 928, Validation Loss: 0.00533381849527359\n",
      "Epoch: 929, Train Loss: 7.210109743027715e-06\n",
      "Epoch: 929, Validation Loss: 0.005333809647709131\n",
      "Epoch: 930, Train Loss: 7.210098829091294e-06\n",
      "Epoch: 930, Validation Loss: 0.005333802197128534\n",
      "Epoch: 931, Train Loss: 7.210088369902223e-06\n",
      "Epoch: 931, Validation Loss: 0.005333793815225363\n",
      "Epoch: 932, Train Loss: 7.210077001218451e-06\n",
      "Epoch: 932, Validation Loss: 0.0053337858989834785\n",
      "Epoch: 933, Train Loss: 7.210066542029381e-06\n",
      "Epoch: 933, Validation Loss: 0.005333777982741594\n",
      "Epoch: 934, Train Loss: 7.210055173345609e-06\n",
      "Epoch: 934, Validation Loss: 0.005333769600838423\n",
      "Epoch: 935, Train Loss: 7.210044714156538e-06\n",
      "Epoch: 935, Validation Loss: 0.005333762615919113\n",
      "Epoch: 936, Train Loss: 7.210034254967468e-06\n",
      "Epoch: 936, Validation Loss: 0.005333753768354654\n",
      "Epoch: 937, Train Loss: 7.210023341031047e-06\n",
      "Epoch: 937, Validation Loss: 0.00533374585211277\n",
      "Epoch: 938, Train Loss: 7.210012427094625e-06\n",
      "Epoch: 938, Validation Loss: 0.005333737935870886\n",
      "Epoch: 939, Train Loss: 7.210001967905555e-06\n",
      "Epoch: 939, Validation Loss: 0.005333730950951576\n",
      "Epoch: 940, Train Loss: 7.2099915087164845e-06\n",
      "Epoch: 940, Validation Loss: 0.005333723034709692\n",
      "Epoch: 941, Train Loss: 7.209981504274765e-06\n",
      "Epoch: 941, Validation Loss: 0.005333715118467808\n",
      "Epoch: 942, Train Loss: 7.209971045085695e-06\n",
      "Epoch: 942, Validation Loss: 0.0053337072022259235\n",
      "Epoch: 943, Train Loss: 7.209960585896624e-06\n",
      "Epoch: 943, Validation Loss: 0.005333699751645327\n",
      "Epoch: 944, Train Loss: 7.209950126707554e-06\n",
      "Epoch: 944, Validation Loss: 0.00533369230106473\n",
      "Epoch: 945, Train Loss: 7.209940122265834e-06\n",
      "Epoch: 945, Validation Loss: 0.005333684850484133\n",
      "Epoch: 946, Train Loss: 7.209929663076764e-06\n",
      "Epoch: 946, Validation Loss: 0.0053336769342422485\n",
      "Epoch: 947, Train Loss: 7.2099196586350445e-06\n",
      "Epoch: 947, Validation Loss: 0.005333669483661652\n",
      "Epoch: 948, Train Loss: 7.209909199445974e-06\n",
      "Epoch: 948, Validation Loss: 0.005333662498742342\n",
      "Epoch: 949, Train Loss: 7.209899195004255e-06\n",
      "Epoch: 949, Validation Loss: 0.005333654582500458\n",
      "Epoch: 950, Train Loss: 7.209889190562535e-06\n",
      "Epoch: 950, Validation Loss: 0.005333647597581148\n",
      "Epoch: 951, Train Loss: 7.2098791861208156e-06\n",
      "Epoch: 951, Validation Loss: 0.005333640147000551\n",
      "Epoch: 952, Train Loss: 7.209868726931745e-06\n",
      "Epoch: 952, Validation Loss: 0.005333632230758667\n",
      "Epoch: 953, Train Loss: 7.209858722490026e-06\n",
      "Epoch: 953, Validation Loss: 0.00533362478017807\n",
      "Epoch: 954, Train Loss: 7.209849172795657e-06\n",
      "Epoch: 954, Validation Loss: 0.0053336177952587605\n",
      "Epoch: 955, Train Loss: 7.2098391683539376e-06\n",
      "Epoch: 955, Validation Loss: 0.005333610810339451\n",
      "Epoch: 956, Train Loss: 7.209829618659569e-06\n",
      "Epoch: 956, Validation Loss: 0.005333603359758854\n",
      "Epoch: 957, Train Loss: 7.2098200689652e-06\n",
      "Epoch: 957, Validation Loss: 0.005333595909178257\n",
      "Epoch: 958, Train Loss: 7.209810064523481e-06\n",
      "Epoch: 958, Validation Loss: 0.005333588924258947\n",
      "Epoch: 959, Train Loss: 7.209800514829112e-06\n",
      "Epoch: 959, Validation Loss: 0.005333581939339638\n",
      "Epoch: 960, Train Loss: 7.209790510387393e-06\n",
      "Epoch: 960, Validation Loss: 0.005333574488759041\n",
      "Epoch: 961, Train Loss: 7.209780960693024e-06\n",
      "Epoch: 961, Validation Loss: 0.005333567503839731\n",
      "Epoch: 962, Train Loss: 7.2097714109986555e-06\n",
      "Epoch: 962, Validation Loss: 0.005333560518920422\n",
      "Epoch: 963, Train Loss: 7.209762316051638e-06\n",
      "Epoch: 963, Validation Loss: 0.005333553534001112\n",
      "Epoch: 964, Train Loss: 7.209752311609918e-06\n",
      "Epoch: 964, Validation Loss: 0.005333546549081802\n",
      "Epoch: 965, Train Loss: 7.2097432166629005e-06\n",
      "Epoch: 965, Validation Loss: 0.00533354002982378\n",
      "Epoch: 966, Train Loss: 7.209733666968532e-06\n",
      "Epoch: 966, Validation Loss: 0.005333532579243183\n",
      "Epoch: 967, Train Loss: 7.209724572021514e-06\n",
      "Epoch: 967, Validation Loss: 0.005333526059985161\n",
      "Epoch: 968, Train Loss: 7.2097150223271456e-06\n",
      "Epoch: 968, Validation Loss: 0.005333519075065851\n",
      "Epoch: 969, Train Loss: 7.209705927380128e-06\n",
      "Epoch: 969, Validation Loss: 0.005333512090146542\n",
      "Epoch: 970, Train Loss: 7.209696377685759e-06\n",
      "Epoch: 970, Validation Loss: 0.005333505105227232\n",
      "Epoch: 971, Train Loss: 7.2096872827387415e-06\n",
      "Epoch: 971, Validation Loss: 0.005333498120307922\n",
      "Epoch: 972, Train Loss: 7.209677733044373e-06\n",
      "Epoch: 972, Validation Loss: 0.005333492066711187\n",
      "Epoch: 973, Train Loss: 7.209668638097355e-06\n",
      "Epoch: 973, Validation Loss: 0.005333485081791878\n",
      "Epoch: 974, Train Loss: 7.209659997897688e-06\n",
      "Epoch: 974, Validation Loss: 0.0053334785625338554\n",
      "Epoch: 975, Train Loss: 7.20965044820332e-06\n",
      "Epoch: 975, Validation Loss: 0.005333471577614546\n",
      "Epoch: 976, Train Loss: 7.209641808003653e-06\n",
      "Epoch: 976, Validation Loss: 0.005333465524017811\n",
      "Epoch: 977, Train Loss: 7.209632713056635e-06\n",
      "Epoch: 977, Validation Loss: 0.005333458539098501\n",
      "Epoch: 978, Train Loss: 7.209624072856968e-06\n",
      "Epoch: 978, Validation Loss: 0.005333451554179192\n",
      "Epoch: 979, Train Loss: 7.2096145231626e-06\n",
      "Epoch: 979, Validation Loss: 0.005333445034921169\n",
      "Epoch: 980, Train Loss: 7.209605882962933e-06\n",
      "Epoch: 980, Validation Loss: 0.005333438515663147\n",
      "Epoch: 981, Train Loss: 7.209597242763266e-06\n",
      "Epoch: 981, Validation Loss: 0.005333431996405125\n",
      "Epoch: 982, Train Loss: 7.209588147816248e-06\n",
      "Epoch: 982, Validation Loss: 0.00533342594280839\n",
      "Epoch: 983, Train Loss: 7.209579507616581e-06\n",
      "Epoch: 983, Validation Loss: 0.00533341895788908\n",
      "Epoch: 984, Train Loss: 7.2095708674169146e-06\n",
      "Epoch: 984, Validation Loss: 0.005333412438631058\n",
      "Epoch: 985, Train Loss: 7.209562227217248e-06\n",
      "Epoch: 985, Validation Loss: 0.00533340685069561\n",
      "Epoch: 986, Train Loss: 7.20955313227023e-06\n",
      "Epoch: 986, Validation Loss: 0.0053333998657763\n",
      "Epoch: 987, Train Loss: 7.209544492070563e-06\n",
      "Epoch: 987, Validation Loss: 0.005333393812179565\n",
      "Epoch: 988, Train Loss: 7.209536761365598e-06\n",
      "Epoch: 988, Validation Loss: 0.005333387292921543\n",
      "Epoch: 989, Train Loss: 7.209528121165931e-06\n",
      "Epoch: 989, Validation Loss: 0.005333380773663521\n",
      "Epoch: 990, Train Loss: 7.209519480966264e-06\n",
      "Epoch: 990, Validation Loss: 0.005333374720066786\n",
      "Epoch: 991, Train Loss: 7.209510386019247e-06\n",
      "Epoch: 991, Validation Loss: 0.005333369132131338\n",
      "Epoch: 992, Train Loss: 7.209502655314282e-06\n",
      "Epoch: 992, Validation Loss: 0.0053333621472120285\n",
      "Epoch: 993, Train Loss: 7.209494015114615e-06\n",
      "Epoch: 993, Validation Loss: 0.0053333560936152935\n",
      "Epoch: 994, Train Loss: 7.209485829662299e-06\n",
      "Epoch: 994, Validation Loss: 0.005333349574357271\n",
      "Epoch: 995, Train Loss: 7.209477189462632e-06\n",
      "Epoch: 995, Validation Loss: 0.005333344452083111\n",
      "Epoch: 996, Train Loss: 7.209469004010316e-06\n",
      "Epoch: 996, Validation Loss: 0.0053333379328250885\n",
      "Epoch: 997, Train Loss: 7.209460818558e-06\n",
      "Epoch: 997, Validation Loss: 0.005333331413567066\n",
      "Epoch: 998, Train Loss: 7.209452178358333e-06\n",
      "Epoch: 998, Validation Loss: 0.005333325359970331\n",
      "Epoch: 999, Train Loss: 7.209444447653368e-06\n",
      "Epoch: 999, Validation Loss: 0.005333319306373596\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(num_nodes=num_nodes, embedding_dim=64, num_layers=3)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train_positive_edges = train_graph.edge_index[:, train_graph.edge_attr >= 3.5]\n",
    "train_negative_edges = train_graph.edge_index[:, train_graph.edge_attr <= 2.5]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    train_positive_edges, train_negative_edges = resample_edges(train_positive_edges, train_negative_edges)\n",
    "    train_positive_ranks = model(train_positive_edges)\n",
    "    train_negative_ranks = model(train_negative_edges)\n",
    "    train_loss = model.recommendation_loss(train_positive_ranks, train_negative_ranks)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss}')\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    optim.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    validation_positive_edges = validation_graph.edge_index[:, validation_graph.edge_attr >= 3.5]\n",
    "    validation_negative_edges = validation_graph.edge_index[:, validation_graph.edge_attr <= 2.5]\n",
    "    validation_positive_edges, validation_negative_edges = resample_edges(validation_positive_edges, validation_negative_edges)\n",
    "    validation_positive_ranks = model(validation_positive_edges)\n",
    "    validation_negative_ranks = model(validation_negative_edges)\n",
    "    validation_loss = model.recommendation_loss(validation_positive_ranks, validation_negative_ranks)\n",
    "    print(f'Epoch: {epoch}, Validation Loss: {validation_loss}')\n",
    "    writer.add_scalar('Loss/validation', validation_loss, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
