{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Varun\\anaconda3\\envs\\p38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.models.lightgcn import LightGCN\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use the first 10k rows of the data, set to None to use all data\n",
    "AMOUNT_TO_LOAD = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 441/63111 [00:00<01:37, 642.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: 468889434_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2690/63111 [00:04<01:33, 644.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: alinetta_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 5477/63111 [00:12<05:24, 177.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: austinsiemens_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11133/63111 [00:44<04:34, 189.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: chisvy_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12808/63111 [00:54<04:55, 170.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: critics_said_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 14456/63111 [01:03<04:34, 177.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: demeguajara_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 15675/63111 [01:10<04:26, 178.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: dragospal_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15881/63111 [01:11<04:36, 170.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: ds612_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 16953/63111 [01:17<03:56, 195.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: elinesophie_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 20009/63111 [01:34<03:24, 211.14it/s]\n"
     ]
    }
   ],
   "source": [
    "user_reviews_dir = 'user_reviews'\n",
    "user_review_data = dict()\n",
    "\n",
    "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
    "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
    "        break\n",
    "    try:\n",
    "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Empty file: {filename}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
    "\n",
    "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001kidd_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(list(user_review_data.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:15<00:00, 1255.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove all values with nan in the review column\n",
    "for key in tqdm(user_review_data.keys()):\n",
    "    user_review_data[key] = user_review_data[key].dropna(subset=['movie_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:51<00:00, 388.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 8633417\n",
      "Validation reviews: 92835\n",
      "Test reviews: 92835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = []\n",
    "validation_reviews = []\n",
    "test_reviews = []\n",
    "for user_id, reviews in tqdm(user_review_data.items()):\n",
    "    if len(reviews) > 15:\n",
    "        validation_review_data = reviews.sample(5, replace=False).to_dict('records')\n",
    "        for review in validation_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        validation_reviews.extend(validation_review_data)\n",
    "        test_review_data = reviews.sample(5, replace=False).to_dict('records')\n",
    "        for review in test_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        test_reviews.extend(test_review_data)\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "    else:\n",
    "        # if the user has less than 5 reviews, we will use all of them for training\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "\n",
    "print(f'Train reviews: {len(train_reviews)}')\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Now that we have the training data, let's construct the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train users: 20000\n",
      "Number of train items: 195378\n",
      "Number of nodes: 215378\n"
     ]
    }
   ],
   "source": [
    "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
    "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
    "num_nodes = num_train_users + num_train_items\n",
    "print(f'Number of train users: {num_train_users}')\n",
    "print(f'Number of train items: {num_train_items}')\n",
    "print(f'Number of nodes: {num_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map users to ids\n",
    "movie_id_to_movie_name = dict()\n",
    "for review in train_reviews:\n",
    "    movie_id_to_movie_name[review['movie_id']] = review['movie_title']\n",
    "\n",
    "user_to_id = dict()\n",
    "for i, user_id in enumerate(set([review['user_id'] for review in train_reviews])):\n",
    "    user_to_id[user_id] = i\n",
    "\n",
    "# Let's map movies to ids\n",
    "movie_to_id = dict()\n",
    "for i, movie_id in enumerate(set([review['movie_id'] for review in train_reviews])):\n",
    "    movie_to_id[movie_id] = i + num_train_users\n",
    "\n",
    "# Let's map ids to users\n",
    "id_to_user = dict()\n",
    "for user_id, index in user_to_id.items():\n",
    "    id_to_user[index] = user_id\n",
    "\n",
    "# Let's map ids to movies\n",
    "id_to_movie = dict()\n",
    "for movie_id, index in movie_to_id.items():\n",
    "    id_to_movie[index] = movie_id\n",
    "\n",
    "# Let's map movie names to movie ids\n",
    "movie_name_to_movie_id = dict()\n",
    "for movie_id, movie_name in movie_id_to_movie_name.items():\n",
    "    movie_name_to_movie_id[movie_name] = movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add nodes that are in our validation and test sets but not in our training set\n",
    "for review in validation_reviews:\n",
    "    if review['user_id'] not in user_to_id:\n",
    "        user_to_id[review['user_id']] = len(user_to_id)\n",
    "    if review['movie_id'] not in movie_to_id:\n",
    "        movie_to_id[review['movie_id']] = len(movie_to_id) + num_train_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert_review_to_edge(review):\n",
    "    user_id = user_to_id[review['user_id']]\n",
    "    movie_id = movie_to_id[review['movie_id']]\n",
    "    edge_weight = review['movie_rating']\n",
    "    if (edge_weight < 3.5 and edge_weight > 2.5):\n",
    "        return None, None\n",
    "    edge = (user_id, movie_id)\n",
    "    edge_weight = review['movie_rating']\n",
    "    return edge, edge_weight\n",
    "\n",
    "def shuffle_edges_and_edge_weights(edges, edge_weights):\n",
    "    c = list(zip(edges, edge_weights))\n",
    "    random.shuffle(c)\n",
    "    return zip(*c)\n",
    "\n",
    "def convert_reviews_to_edges(reviews):\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    for review in tqdm(reviews):\n",
    "        edge, edge_weight = convert_review_to_edge(review)\n",
    "        if edge is not None:\n",
    "            edges.append(edge)\n",
    "            edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Reformat the edges to be a tensor\n",
    "    edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edges, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8633417/8633417 [00:07<00:00, 1131476.82it/s]\n",
      "100%|██████████| 92835/92835 [00:00<00:00, 936883.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 6933969\n",
      "Validation edges: 77432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the edges between users and movies.\n",
    "# The id of the user will be the index of the user in the user_to_id dict\n",
    "# The id of the movie will be the index of the movie in the movie_to_id dict + the number of users\n",
    "\n",
    "train_edges, train_edge_weights = convert_reviews_to_edges(train_reviews)\n",
    "validation_edges, validation_edge_weights = convert_reviews_to_edges(validation_reviews)\n",
    "\n",
    "print(f'Train edges: {train_edges.shape[1]}')\n",
    "print(f'Validation edges: {validation_edges.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as data\n",
    "\n",
    "# create the graph\n",
    "train_graph = data.Data(\n",
    "    edge_index=train_edges,\n",
    "    edge_attr=torch.tensor(train_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "validation_graph = data.Data(\n",
    "    edge_index=validation_edges,\n",
    "    edge_attr=torch.tensor(validation_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.validate(raise_on_error=True)\n",
    "validation_graph.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some negative edges\n",
    "def resample_edges_for_user(user_positive_edges, user_negative_edges):\n",
    "    num_negative_edges_to_add = user_positive_edges.shape[1] * 3 - user_negative_edges.shape[1]\n",
    "    if (num_negative_edges_to_add <= 0):\n",
    "        num_negative_edges_to_remove = -num_negative_edges_to_add\n",
    "        # choose the negative edges to keep\n",
    "        negative_edges_to_keep = torch.randint(user_negative_edges.shape[1], (user_negative_edges.shape[1] - num_negative_edges_to_remove,))\n",
    "        # remove all the negative edges for this user\n",
    "        user_negative_edges = user_negative_edges[:, negative_edges_to_keep]\n",
    "    else:\n",
    "        # Create new negative edges\n",
    "        negative_edges_to_add = torch.tensor([[user_id] * num_negative_edges_to_add, torch.randint(num_train_users, num_train_items, (num_negative_edges_to_add,))], dtype=torch.long)\n",
    "        # Add the negative edges to the negative edges for this user\n",
    "        user_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
    "    return user_positive_edges, user_negative_edges\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute ndcg\n",
    "def compute_ndcg_at_k(relevances, k=5):\n",
    "    relevances = relevances[:k]\n",
    "    dcg = 0\n",
    "    for i, relevance in enumerate(relevances):\n",
    "        dcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
    "    idcg = 0\n",
    "    for i, relevance in enumerate(sorted(relevances, reverse=True)):\n",
    "        idcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
    "    return dcg / idcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 99.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.09114858678381386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/79 [00:03<04:40,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/79 [00:06<04:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/79 [00:07<04:33,  3.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m users_in_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandperm(num_train_users)[start_idx:start_idx \u001b[39m+\u001b[39m BATCH_SIZE]\n\u001b[0;32m     85\u001b[0m \u001b[39mfor\u001b[39;00m user_id \u001b[39min\u001b[39;00m users_in_batch:\n\u001b[0;32m     86\u001b[0m     \u001b[39m# get all the edges specific to this user\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m     user_positive_edges \u001b[39m=\u001b[39m train_positive_edges[:, train_positive_edges[\u001b[39m0\u001b[39;49m] \u001b[39m==\u001b[39;49m user_id]\n\u001b[0;32m     88\u001b[0m     user_negative_edges \u001b[39m=\u001b[39m train_negative_edges[:, train_negative_edges[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m user_id]\n\u001b[0;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m (user_positive_edges\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m user_negative_edges\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "LR = 5e-05\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_DIM = 64\n",
    "model = LightGCN(num_nodes=num_nodes, embedding_dim=EMBEDDING_DIM, num_layers=NUM_LAYERS)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Running on device: {}\".format(device))\n",
    "print(EMBEDDING_DIM)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[100, 200, 300, 400], gamma=0.5)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=100)\n",
    "\n",
    "train_positive_edges = train_graph.edge_index[:, train_graph.edge_attr >= 3.5]\n",
    "train_negative_edges = train_graph.edge_index[:, train_graph.edge_attr <= 2.5]\n",
    "\n",
    "validation_df = pd.DataFrame.from_dict(validation_reviews)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment=f'LightGCN_{EMBEDDING_DIM}_layers_{NUM_LAYERS}_batch_size_{BATCH_SIZE}_lr_{LR}_num_train_users_{num_train_users}_num_train_items_{num_train_items}')\n",
    "\n",
    "for epoch in range(10001):\n",
    "    # we are using BPR so we go by user\n",
    "    average_loss = 0\n",
    "    # We'll proceed in batches of users\n",
    "    for start_idx in tqdm(range(0, num_train_users, BATCH_SIZE)):\n",
    "        model.train()\n",
    "        loss = torch.tensor(0.0, requires_grad=True)\n",
    "        # randomly select a batch of users\n",
    "        users_in_batch = torch.randperm(num_train_users)[start_idx:start_idx + BATCH_SIZE]\n",
    "        for user_id in users_in_batch:\n",
    "            # get all the edges specific to this user\n",
    "            user_positive_edges = train_positive_edges[:, train_positive_edges[0] == user_id]\n",
    "            user_negative_edges = train_negative_edges[:, train_negative_edges[0] == user_id]\n",
    "            if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
    "                continue\n",
    "            # limit the number of positive edges to 5000\n",
    "            if (user_positive_edges.shape[1] > 5000):\n",
    "                user_positive_edges = user_positive_edges[:, :5000]\n",
    "            # Get at most 15000 negative edges\n",
    "            if (user_negative_edges.shape[1] > 15000):\n",
    "                user_negative_edges = user_negative_edges[:, :15000]\n",
    "            # resample the negative edges if we don't have enough\n",
    "            user_positive_edges, user_negative_edges = resample_edges_for_user(user_positive_edges, user_negative_edges)\n",
    "            # concatenate the positive and negative edges\n",
    "            user_edges = torch.cat([user_positive_edges, user_negative_edges], dim=1)\n",
    "            # get the rankings for this user\n",
    "            user_edges = user_edges.to(device)\n",
    "            user_rankings = model(user_edges)\n",
    "            # divide the rankings into positive and negative rankings\n",
    "            user_positive_rankings = user_rankings[:user_positive_edges.shape[1]]\n",
    "            user_negative_rankings = user_rankings[user_positive_edges.shape[1]:]\n",
    "            # create all pairs of positive and negative rankings\n",
    "            user_positive_rankings = user_positive_rankings.unsqueeze(1).repeat(1, user_negative_rankings.shape[0])\n",
    "            user_negative_rankings = user_negative_rankings.unsqueeze(0).repeat(user_positive_rankings.shape[0], 1)\n",
    "            # get the user loss\n",
    "            user_loss = model.recommendation_loss(user_positive_rankings, user_negative_rankings, 1e-4)\n",
    "            # add the user loss to the total loss\n",
    "            loss = loss + user_loss\n",
    "        # divide the loss by the number of users\n",
    "        loss = loss / BATCH_SIZE\n",
    "        # log the loss\n",
    "        # backprop\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "        print(epoch * BATCH_SIZE + start_idx // BATCH_SIZE)\n",
    "        average_loss = 0\n",
    "    if (epoch % 1 == 0):\n",
    "        # evaluate the model\n",
    "        model.eval()\n",
    "        # iterate over all users in the validation set\n",
    "        validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "        # randomly select 1000 of the users\n",
    "        validation_users = random.sample(validation_users, 1000)\n",
    "        mean_ndcg = 0\n",
    "        ndcg_scores = []\n",
    "        for user in tqdm(validation_users):\n",
    "            user_id = id_to_user[user]\n",
    "            relevant_reviews = validation_df[validation_df['user_id'] == user_id]\n",
    "            user_validation_edges = validation_edges[:, validation_edges[0] == user]\n",
    "            user_validation_edges = user_validation_edges.to(device)\n",
    "            user_rankings = model(user_validation_edges)\n",
    "            edges_sorted = list(user_validation_edges[1, user_rankings.argsort(descending=True)])\n",
    "            # use validation_df to get the relevances via the movie_id column and the movie_rating column\n",
    "            relevances = []\n",
    "            for edge in edges_sorted:\n",
    "                movie_id = id_to_movie[int(edge)]\n",
    "                if (movie_id in relevant_reviews['movie_id'].values):\n",
    "                    relevances.append(relevant_reviews[relevant_reviews['movie_id'] == movie_id]['movie_rating'].values[0])\n",
    "                else:\n",
    "                    relevances.append(0)\n",
    "            # calculate the ndcg\n",
    "            ndcg = compute_ndcg_at_k(relevances)\n",
    "            if (math.isnan(ndcg)):\n",
    "                print(relevant_reviews)\n",
    "                input()\n",
    "            mean_ndcg += ndcg\n",
    "            ndcg_scores.append(ndcg)\n",
    "        mean_ndcg = mean_ndcg / len(validation_users)\n",
    "        print(\"Standard Deviation: {}\".format(np.std(ndcg_scores)))\n",
    "        # create a histogram of the ndcg scores, make bins for each 0.1\n",
    "        ndcg_scores = np.array(ndcg_scores).squeeze()\n",
    "        writer.add_histogram(\"hist_NDCG/val\", ndcg_scores, epoch)\n",
    "        # also make a histogram in matplotlib and save as png\n",
    "        plt.hist(ndcg_scores, bins=np.arange(0, 1.1, 0.1))\n",
    "        plt.suptitle(\"Validation NDCG Histogram\")\n",
    "        # write information about the model to the histogram\n",
    "        plt.title(f\"Model: LightGCN, Embedding Dim: {EMBEDDING_DIM}, Num Layers: {NUM_LAYERS}, Batch Size: {BATCH_SIZE}, LR: {LR}, Num Train Users: {num_train_users}, Num Train Items: {num_train_items}\", fontsize=8, wrap=True)\n",
    "        plt.xlabel(\"NDCG\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        # save the figure in the hist_NDCG folder, with the title having the model information and the epoch number\n",
    "        plt.savefig(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.png\")\n",
    "        plt.close()\n",
    "        # Also save the raw NDCG scores to a csv file, with the model information in the title, and the epoch number\n",
    "        np.savetxt(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.csv\", ndcg_scores, delimiter=\",\")\n",
    "        writer.add_scalar(\"NDCG/val\", mean_ndcg, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "validation_df[validation_df.user_id == id_to_user[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_edges[:, validation_edges[0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c42c7180e3fe8f66898b46e6715a19bce1ca98993688e7fbe101188e1cfb25f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
