{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.0.0%2Bcu117.html\n",
      "Requirement already satisfied: torch-sparse in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (0.6.17+pt20cu117)\n",
      "Collecting torch-scatter\n",
      "  Using cached https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_scatter-2.1.1%2Bpt20cu117-cp38-cp38-win_amd64.whl (3.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from torch-sparse) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from scipy->torch-sparse) (1.23.5)\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.1+pt20cu117\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-sparse torch-scatter -f https://data.pyg.org/whl/torch-2.0.0%2Bcu117.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (3.7.1)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (4.39.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from tensorboard) (0.38.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from tensorboard) (65.6.3)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.51.3-cp38-cp38-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 12.0 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "     ---------------------------------------- 177.2/177.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from tensorboard) (2.28.1)\n",
      "Collecting protobuf>=3.19.6\n",
      "  Downloading protobuf-4.22.1-cp38-cp38-win_amd64.whl (420 kB)\n",
      "     ------------------------------------- 420.6/420.6 kB 13.2 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from markdown>=2.6.8->tensorboard) (6.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, werkzeug, tensorboard-data-server, rsa, pyasn1-modules, protobuf, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard\n",
      "Successfully installed absl-py-1.4.0 cachetools-5.3.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 grpcio-1.51.3 markdown-3.4.1 oauthlib-3.2.2 protobuf-4.22.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 werkzeug-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models.lightgcn import LightGCN\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use the first 10k rows of the data, set to None to use all data\n",
    "AMOUNT_TO_LOAD = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 335/63111 [00:01<06:10, 169.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: 468889434_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 501/63111 [00:02<06:04, 171.70it/s]\n"
     ]
    }
   ],
   "source": [
    "user_reviews_dir = 'user_reviews'\n",
    "user_review_data = dict()\n",
    "\n",
    "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
    "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
    "        break\n",
    "    try:\n",
    "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Empty file: {filename}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
    "\n",
    "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001kidd_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(list(user_review_data.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1140.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove all values with nan in the review column\n",
    "for key in tqdm(user_review_data.keys()):\n",
    "    user_review_data[key] = user_review_data[key].dropna(subset=['movie_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 369.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 160650\n",
      "Validation reviews: 14000\n",
      "Test reviews: 7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = []\n",
    "validation_reviews = []\n",
    "test_reviews = []\n",
    "for user_id, reviews in tqdm(user_review_data.items()):\n",
    "    if len(reviews) > 80:\n",
    "        validation_review_data_df = reviews.sample(40, replace=False)\n",
    "        validation_review_data = validation_review_data_df.to_dict('records')\n",
    "        for review in validation_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        validation_reviews.extend(validation_review_data)\n",
    "        # remove the validation reviews from the training data\n",
    "        reviews = reviews.drop(validation_review_data_df.index)\n",
    "        test_review_data_df = reviews.sample(20, replace=False)\n",
    "        test_review_data = test_review_data_df.to_dict('records')\n",
    "        for review in test_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        test_reviews.extend(test_review_data)\n",
    "        # remove the test reviews from the training data\n",
    "        reviews = reviews.drop(test_review_data_df.index)\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "    else:\n",
    "        # if the user has less than 5 reviews, we will use all of them for training\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "\n",
    "print(f'Train reviews: {len(train_reviews)}')\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Now that we have the training data, let's construct the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train users: 500\n",
      "Number of train items: 28009\n",
      "Number of nodes: 29592\n"
     ]
    }
   ],
   "source": [
    "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
    "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
    "num_total_items = len(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews]))\n",
    "num_nodes = num_train_users + num_total_items\n",
    "print(f'Number of train users: {num_train_users}')\n",
    "print(f'Number of train items: {num_train_items}')\n",
    "print(f'Number of nodes: {num_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val_users = len(set([review['user_id'] for review in validation_reviews]))\n",
    "num_val_items = len(set([review['movie_id'] for review in validation_reviews]))\n",
    "num_val_nodes = num_val_users + num_val_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map users to ids\n",
    "movie_id_to_movie_name = dict()\n",
    "for review in train_reviews + validation_reviews + test_reviews:\n",
    "    movie_id_to_movie_name[review['movie_id']] = review['movie_title']\n",
    "\n",
    "user_to_id = dict()\n",
    "for i, user_id in enumerate(set([review['user_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
    "    user_to_id[user_id] = i\n",
    "\n",
    "# Let's map movies to ids\n",
    "movie_to_id = dict()\n",
    "for i, movie_id in enumerate(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
    "    movie_to_id[movie_id] = i + num_train_users\n",
    "\n",
    "# Let's map ids to users\n",
    "id_to_user = dict()\n",
    "for user_id, index in user_to_id.items():\n",
    "    id_to_user[index] = user_id\n",
    "\n",
    "# Let's map ids to movies\n",
    "id_to_movie = dict()\n",
    "for movie_id, index in movie_to_id.items():\n",
    "    id_to_movie[index] = movie_id\n",
    "\n",
    "# Let's map movie names to movie ids\n",
    "movie_name_to_movie_id = dict()\n",
    "for movie_id, movie_name in movie_id_to_movie_name.items():\n",
    "    movie_name_to_movie_id[movie_name] = movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert_review_to_edge(review):\n",
    "    user_id = user_to_id[review['user_id']]\n",
    "    movie_id = movie_to_id[review['movie_id']]\n",
    "    edge_weight = review['movie_rating']\n",
    "    if (edge_weight < 3.5 and edge_weight > 2.5):\n",
    "        return None, None\n",
    "    edge = (user_id, movie_id)\n",
    "    edge_weight = review['movie_rating']\n",
    "    return edge, edge_weight\n",
    "\n",
    "def shuffle_edges_and_edge_weights(edges, edge_weights):\n",
    "    c = list(zip(edges, edge_weights))\n",
    "    random.shuffle(c)\n",
    "    return zip(*c)\n",
    "\n",
    "def convert_reviews_to_edges(reviews):\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    for review in tqdm(reviews):\n",
    "        edge, edge_weight = convert_review_to_edge(review)\n",
    "        if edge is not None:\n",
    "            edges.append(edge)\n",
    "            edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Reformat the edges to be a tensor\n",
    "    edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edges, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160650/160650 [00:00<00:00, 927686.57it/s]\n",
      "100%|██████████| 14000/14000 [00:00<00:00, 932467.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 130791\n",
      "Validation edges: 11591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the edges between users and movies.\n",
    "# The id of the user will be the index of the user in the user_to_id dict\n",
    "# The id of the movie will be the index of the movie in the movie_to_id dict + the number of users\n",
    "\n",
    "train_edges, train_edge_weights = convert_reviews_to_edges(train_reviews)\n",
    "validation_edges, validation_edge_weights = convert_reviews_to_edges(validation_reviews)\n",
    "\n",
    "print(f'Train edges: {train_edges.shape[1]}')\n",
    "print(f'Validation edges: {validation_edges.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as data\n",
    "\n",
    "# create the graph\n",
    "train_graph = data.Data(\n",
    "    edge_index=train_edges,\n",
    "    edge_attr=torch.tensor(train_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "validation_graph = data.Data(\n",
    "    edge_index=validation_edges,\n",
    "    edge_attr=torch.tensor(validation_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.validate(raise_on_error=True)\n",
    "validation_graph.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some negative edges\n",
    "def resample_edges_for_user(user_positive_edges, user_negative_edges):\n",
    "    num_negative_edges_to_add = user_positive_edges.shape[1] * 3 - user_negative_edges.shape[1]\n",
    "    if (num_negative_edges_to_add <= 0):\n",
    "        num_negative_edges_to_remove = -num_negative_edges_to_add\n",
    "        # choose the negative edges to keep\n",
    "        negative_edges_to_keep = torch.randint(user_negative_edges.shape[1], (user_negative_edges.shape[1] - num_negative_edges_to_remove,))\n",
    "        # remove all the negative edges for this user\n",
    "        user_negative_edges = user_negative_edges[:, negative_edges_to_keep]\n",
    "    else:\n",
    "        # Create new negative edges\n",
    "        negative_edges_to_add = torch.tensor([[user_id] * num_negative_edges_to_add, torch.randint(num_train_users, num_train_items, (num_negative_edges_to_add,))], dtype=torch.long)\n",
    "        # Add the negative edges to the negative edges for this user\n",
    "        user_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
    "    return user_positive_edges, user_negative_edges\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute ndcg\n",
    "def compute_ndcg_at_k(relevances, k=5):\n",
    "    dcg = 0\n",
    "    for i, relevance in enumerate(relevances):\n",
    "        if i == k:\n",
    "            break\n",
    "        dcg += (relevance) / np.log2(i + 2)\n",
    "    idcg = 0\n",
    "    for i, relevance in enumerate(sorted(relevances, reverse=True)):\n",
    "        if i == k:\n",
    "            break\n",
    "        idcg += (relevance) / np.log2(i + 2)\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def compute_recall_at_k(validation_graph, model, K):\n",
    "    # get positive edges in validation set\n",
    "    positive_edges = validation_graph.edge_index[:, validation_graph.edge_attr > 3.5]\n",
    "\n",
    "    # map users to positive edges\n",
    "    user_pos_items = get_user_positive_items(positive_edges)\n",
    "\n",
    "    # get users\n",
    "    users = positive_edges[0].unique()\n",
    "\n",
    "    users = users[torch.randint(users.shape[0], (min(200, len(users)),))]\n",
    "    # filter the validation edges to only the users we want to evaluate\n",
    "    user_validation_edges = []\n",
    "    for user in users:\n",
    "        user_validation_edges.append(validation_graph.edge_index[:, validation_graph.edge_index[0] == user])\n",
    "    user_validation_edges = torch.cat(user_validation_edges, dim=1)\n",
    "    print(user_validation_edges.shape)\n",
    "\n",
    "    first_user_id = users[0].item()\n",
    "    user_name = id_to_user[first_user_id]\n",
    "    print(f'User: {user_name}')\n",
    "\n",
    "    # get movies\n",
    "    movie_indices = torch.LongTensor([_ for _ in range(len(users) + 1, validation_graph.num_nodes)]).to(device)\n",
    "\n",
    "    # Get positive items for each user in validation set\n",
    "    truth_items = [set(user_pos_items[user.item()]) for user in users]\n",
    "\n",
    "    first_user_truth_items = truth_items[0]\n",
    "    first_user_truth_items = [id_to_movie[item] for item in first_user_truth_items]\n",
    "    first_user_truth_items = [movie_id_to_movie_name[item] for item in first_user_truth_items]\n",
    "    print(first_user_truth_items)\n",
    "\n",
    "    training_edges = train_graph.edge_index\n",
    "\n",
    "    # Get top-K recommended items for each user in validation set\n",
    "    total_recall = 0\n",
    "    print(\"Computing recommendations for {} users\".format(len(users)))\n",
    "    for user_index, user_id in tqdm(enumerate(users), total=len(users)):\n",
    "        tick = time.time()\n",
    "        all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "        recommendations = model.recommend(all_edges.to(device), src_index=torch.tensor([user_id]).to(device), dst_index=torch.tensor([x for x in range(num_train_users + 1, num_train_items)]).to(device), k=10 * K)[0]\n",
    "        tock = time.time()\n",
    "        train_edges_for_user = training_edges[:, training_edges[0] == user_id].to(device)\n",
    "        # remove all the recommendations that are in the training set\n",
    "        recommendations = recommendations[~torch.isin(recommendations, train_edges_for_user[1])][:K]\n",
    "        if (len(recommendations) < K):\n",
    "            print(\"Not enough recommendations for user {}\".format(user_id))\n",
    "            continue\n",
    "        if (user_id == first_user_id):\n",
    "            first_user_recommended_items = recommendations\n",
    "            first_user_recommended_items = [id_to_movie[item.item()] for item in first_user_recommended_items if item.item() > num_train_users]\n",
    "            first_user_recommended_items = [movie_id_to_movie_name[item] for item in first_user_recommended_items if item in movie_id_to_movie_name]\n",
    "            print(first_user_recommended_items)\n",
    "        # num_intersect = 0\n",
    "        truth_items_for_user = truth_items[user_index]\n",
    "        # for item in recommendations:\n",
    "        #     item = item.item()\n",
    "        #     if item in truth_items_for_user:\n",
    "        #         num_intersect += 1\n",
    "        # print(num_intersect)\n",
    "        num_intersect = len(set([item.item() for item in recommendations]).intersection(truth_items[user_index]))\n",
    "        recall = num_intersect / len(truth_items_for_user)\n",
    "        total_recall += recall\n",
    "    return total_recall / len(users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adapted from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/lightgcn.html\"\"\"\n",
    "class CustomLightGCN(torch.nn.Module):\n",
    "    \"\"\"From the <https://arxiv.org/abs/2002.02126>` paper.\n",
    "\n",
    "    Args:\n",
    "        num_nodes (int): The number of nodes in the graph.\n",
    "        embedding_dim (int): The dimensionality of node embeddings.\n",
    "        num_layers (int): The number of layers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        embedding_dim: int,\n",
    "        num_layers: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "        self.alpha = torch.tensor([1. / (num_layers + 1)] * (num_layers + 1))\n",
    "        self.convs = ModuleList([GATConv(embedding_dim, embedding_dim, heads=8, dropout=0.6) for _ in range(num_layers)])\n",
    "        self.linears = ModuleList([Linear(embedding_dim * 8, embedding_dim) for _ in range(num_layers)])\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def get_embedding(self, edge_index):\n",
    "        x = self.embedding.weight\n",
    "        out = x * self.alpha[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.linears[i](x.view(-1, self.embedding_dim * 8))\n",
    "            out = out + x * self.alpha[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        edge_label_index = edge_index\n",
    "        out = self.get_embedding(edge_index)\n",
    "        user = out[edge_label_index[0]]\n",
    "        movie = out[edge_label_index[1]]\n",
    "        return (user * movie).sum(dim=-1)\n",
    "\n",
    "\n",
    "    def predict_link(self, edge_index, edge_label_index):\n",
    "        \"Predict links between nodes specified in edge_label_index.\"\"\"\n",
    "        pred = self(edge_index, edge_label_index).sigmoid()\n",
    "        return pred.round()\n",
    "\n",
    "\n",
    "    def recommend(self, edge_index, k):\n",
    "        \"\"\"Get top-k recommendations for nodes in src_index.\"\"\"\n",
    "        out_user = self.get_embedding(edge_index)\n",
    "        out_movie = self.get_embedding(edge_index)\n",
    "        pred = out_user @ out_movie.t()\n",
    "        top_index = pred.topk(k, dim=-1).indices\n",
    "        return top_index\n",
    "\n",
    "\n",
    "    def link_pred_loss(self, pred, edge_label):\n",
    "        \"\"\"Computes the model loss for a link prediction using torch.nn.BCEWithLogitsLoss.\n",
    "        \n",
    "        Args:\n",
    "            pred (torch.Tensor): The predictions.\n",
    "            edge_label (torch.Tensor): The ground-truth edge labels.\n",
    "        \"\"\"\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "\n",
    "    def recommendation_loss(self, pos_edge_rank, neg_edge_rank,\n",
    "                            lambda_reg: float = 1e-4):\n",
    "        \"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "        Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "            pos_edge_rank (torch.Tensor): Positive edge rankings.\n",
    "            neg_edge_rank (torch.Tensor): Negative edge rankings.\n",
    "            lambda_reg (int, optional): The L2 regularization strength\n",
    "                of the Bayesian Personalized Ranking (BPR) loss.\n",
    "        \"\"\"\n",
    "        loss_fn = BPRLoss(lambda_reg)\n",
    "        return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is verbatim from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/lightgcn.html. \"\"\"\n",
    "class BPRLoss(_Loss):\n",
    "    \"\"\"The Bayesian Personalized Ranking (BPR) loss.\"\"\"\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = 0\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        \"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).mean()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 81.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.06404778441018344\n",
      "0.8803228327080053\n",
      "torch.Size([2, 6685])\n",
      "User: 223_reviews.csv\n",
      "['Kung Fu Panda 3', 'I, Daniel Blake', 'Midnight in Paris', 'Trolls Holiday', 'Thor: Ragnarok', 'Farewell My Concubine', 'In the Mood for Love', 'Hello Ghost', 'Everything Everywhere All at Once', 'Terrorizers', 'Portrait of a Lady on Fire', 'The Lobster', \"Long Day's Journey Into Night\", 'Titanic', 'Punch-Drunk Love', 'Nope', 'Fourth Place']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Turning Red', 'Glass Onion: A Knives Out Mystery', 'Avatar: The Way of Water', 'The Batman', 'Everything Everywhere All at Once', 'Mean Girls', 'The Truman Show', 'Scott Pilgrim vs. the World', 'The Perks of Being a Wallflower', 'The Edge of Seventeen', 'Shutter Island', \"Howl's Moving Castle\", 'All Quiet on the Western Front', 'American Psycho', 'Soul', 'Jurassic Park', 'Forrest Gump', 'tick, tick...BOOM!', 'Girl, Interrupted', 'The Shining']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 99/200 [00:07<00:07, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Turning Red', 'Glass Onion: A Knives Out Mystery', 'Avatar: The Way of Water', 'The Batman', 'Everything Everywhere All at Once', 'Mean Girls', 'The Truman Show', 'Scott Pilgrim vs. the World', 'The Perks of Being a Wallflower', 'The Edge of Seventeen', 'Shutter Island', \"Howl's Moving Castle\", 'All Quiet on the Western Front', 'American Psycho', 'Soul', 'Jurassic Park', 'Forrest Gump', 'tick, tick...BOOM!', 'Girl, Interrupted', 'The Shining']\n",
      "['Turning Red', 'Glass Onion: A Knives Out Mystery', 'Avatar: The Way of Water', 'The Batman', 'Everything Everywhere All at Once', 'Mean Girls', 'The Truman Show', 'Scott Pilgrim vs. the World', 'The Perks of Being a Wallflower', 'The Edge of Seventeen', 'Shutter Island', \"Howl's Moving Castle\", 'All Quiet on the Western Front', 'American Psycho', 'Soul', 'Jurassic Park', 'Forrest Gump', 'tick, tick...BOOM!', 'Girl, Interrupted', 'The Shining']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06598101962081238\n",
      "Epoch: 50, NDCG: 0.8803228327080053, Recall@20: 0.06598101962081238\n",
      "Average number of matches: 5.082857142857143\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 79.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.062495219977439204\n",
      "0.8869869430771371\n",
      "torch.Size([2, 6594])\n",
      "User: 6789andre_reviews.csv\n",
      "['Back to the Future Part III', 'Ratatouille', 'Toy Story 3', 'Come and See', 'Coraline', 'Inglourious Basterds', 'Kung Fu Panda', 'Paths of Glory', 'John Wick: Chapter 3 - Parabellum', 'Pay It Forward', 'Klaus', 'Reservoir Dogs', 'The Guardians of the Galaxy Holiday Special', 'Shrek', 'For a Few Dollars More', 'The Thin Red Line', 'Catch Me If You Can', 'Groundhog Day', 'The Silence of the Lambs', 'Platoon', 'Cars', 'Leaving Las Vegas', 'The Social Network', 'The Good, the Bad and the Ugly', 'The Matrix', 'Forrest Gump', 'Raiders of the Lost Ark', 'John Wick', 'Toy Story 2', 'Captain America: The Winter Soldier', 'Saving Private Ryan']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glass Onion: A Knives Out Mystery', 'La La Land', 'Knives Out', 'The Dark Knight', 'Get Out', 'Parasite', 'Turning Red', 'The Grand Budapest Hotel', 'Black Adam', 'Lady Bird', 'The Truman Show', 'Mad Max: Fury Road', 'Inglourious Basterds', 'Spirited Away', \"Howl's Moving Castle\", 'Dune', \"Guillermo del Toro's Pinocchio\", 'Eternal Sunshine of the Spotless Mind', 'Dead Poets Society', 'Inception']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 104/200 [00:07<00:05, 17.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glass Onion: A Knives Out Mystery', 'La La Land', 'Knives Out', 'The Dark Knight', 'Get Out', 'Parasite', 'Turning Red', 'The Grand Budapest Hotel', 'Black Adam', 'Lady Bird', 'The Truman Show', 'Mad Max: Fury Road', 'Inglourious Basterds', 'Spirited Away', \"Howl's Moving Castle\", 'Dune', \"Guillermo del Toro's Pinocchio\", 'Eternal Sunshine of the Spotless Mind', 'Dead Poets Society', 'Inception']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08771430569549138\n",
      "Epoch: 100, NDCG: 0.8869869430771371, Recall@20: 0.08771430569549138\n",
      "Average number of matches: 5.365714285714286\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 78.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.062219206055680334\n",
      "0.8884017964798591\n",
      "torch.Size([2, 6553])\n",
      "User: 2mad2chill_reviews.csv\n",
      "['Joker', 'The Platform', 'Fight Club', 'Warrior', 'Blade Runner 2049', 'Blade Runner', 'The Grand Budapest Hotel', 'Memories of Murder', 'Good Will Hunting', 'Mustang', 'Lady Bird']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glass Onion: A Knives Out Mystery', 'Knives Out', 'Everything Everywhere All at Once', 'Fight Club', 'The Batman', 'The Dark Knight', 'Lady Bird', 'Hwarang: The Poet Warrior Youth', 'The Grand Budapest Hotel', 'Spirited Away', 'La La Land', 'Pulp Fiction', 'Get Out', 'The Truman Show', 'Ratatouille', 'Spree', 'Scott Pilgrim vs. the World', 'Dune', 'Taxi Driver', 'Avengers: Endgame']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07117751775146658\n",
      "Epoch: 150, NDCG: 0.8884017964798591, Recall@20: 0.07117751775146658\n",
      "Average number of matches: 5.357142857142857\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 82.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.06209804140015314\n",
      "0.8884278255573627\n",
      "torch.Size([2, 6611])\n",
      "User: 23minutes_reviews.csv\n",
      "[\"Ferris Bueller's Day Off\", 'El Mariachi', 'Psycho', 'Dirty Harry', 'They Live', 'The Muppet Movie', 'Scrooged', 'Night of the Living Dead', 'Howards End', 'From Beyond', 'Boyz n the Hood', 'Akira', 'Phenomena', 'Two Way Stretch', 'E.T. the Extra-Terrestrial', 'Star Wars', 'The Fly', 'Friday the 13th: The Final Chapter', 'The Godfather: Part II', 'The Brood', 'The Hills Have Eyes', 'The Meaning of Life', 'The Terminator', 'The Third Man', 'The Hitcher']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glass Onion: A Knives Out Mystery', 'Boyhood', 'Lady Bird', 'Get Out', 'La La Land', \"The King's Man\", 'Fight Club', 'The Batman', 'The Grand Budapest Hotel', 'Knock at the Cabin', \"To All the Boys I've Loved Before\", 'The Dark Knight', 'Spree', 'Dune', 'Annihilation', 'Spirited Away', 'Black Adam', 'Anna', 'Enola Holmes', 'Taxi Driver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 130/200 [00:09<00:05, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glass Onion: A Knives Out Mystery', 'Boyhood', 'Lady Bird', 'Get Out', 'La La Land', \"The King's Man\", 'Fight Club', 'The Batman', 'The Grand Budapest Hotel', 'Knock at the Cabin', \"To All the Boys I've Loved Before\", 'The Dark Knight', 'Spree', 'Dune', 'Annihilation', 'Spirited Away', 'Black Adam', 'Anna', 'Enola Holmes', 'Taxi Driver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07371586492180504\n",
      "Epoch: 200, NDCG: 0.8884278255573627, Recall@20: 0.07371586492180504\n",
      "Average number of matches: 5.36\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 78.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.06205472151279737\n",
      "0.88848333551898\n",
      "torch.Size([2, 6707])\n",
      "User: 1morena3_reviews.csv\n",
      "['Mulholland Drive', 'Thor: Ragnarok', \"There's a Man in the Woods\", 'Knives Out', 'El Angel', 'WandaVision', 'Moulin Rouge!', 'Perfect Blue', 'The Exterminating Angel', 'The Texas Chain Saw Massacre', 'Another Round', 'Rojo', 'Akira', 'Requiem for a Dream', 'Bad Education', 'Shutter Island', 'The French Dispatch', 'Barbarian', 'Suspiria', 'Stand by Me', 'Coffee and Cigarettes', 'The Secret in Their Eyes', 'Memento', 'Man Facing Southeast', 'Adaptation.']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knives Out', 'Whiplash', 'The Batman', 'The Dark Knight', 'Fight Club', 'The Grand Budapest Hotel', 'Turning Red', 'Spirited Away', 'Avatar: The Way of Water', 'Mad Max: Fury Road', 'Pulp Fiction', 'Ratatouille', 'Shutter Island', 'Scott Pilgrim vs. the World', 'Top Gun: Maverick', 'Eternal Sunshine of the Spotless Mind', 'LÃ©on: The Professional', 'Spree', 'Forrest Gump', 'Memento']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07571118714473979\n",
      "Epoch: 250, NDCG: 0.88848333551898, Recall@20: 0.07571118714473979\n",
      "Average number of matches: 5.36\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 80.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.0620727721422019\n",
      "0.8884503437277618\n",
      "torch.Size([2, 6658])\n",
      "User: 6renda_reviews.csv\n",
      "['Ratatouille', 'Maze Runner: The Scorch Trials', 'The Twilight Saga: New Moon', 'Fear Street: 1666', 'The Boy in the Striped Pyjamas', 'Harry Potter and the Order of the Phoenix', 'Harry Potter and the Goblet of Fire', 'What Happened to Monday', 'Legally Blonde', 'The Black Phone', 'The Conjuring', 'White Chicks', 'Maze Runner: The Death Cure', 'Maleficent', 'The Twilight Saga: Breaking Dawn - Part 2', 'The Chronicles of Narnia: The Voyage of the Dawn Treader', 'Harry Potter and the Deathly Hallows: Part 1', 'The Twilight Saga: Eclipse', 'Ponyo', 'Monster House', 'Now You See Me 2', 'Project X', 'Alice in Wonderland', 'Avengers: Endgame', '1917', '13 Going on 30', 'Ready or Not', 'X', 'Corpse Bride', 'Midsommar']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:00<00:11, 16.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avatar: The Way of Water', 'Turning Red', 'The Virgin Suicides', 'Irreversible', 'Knives Out', 'Thor: Love and Thunder', 'Whiplash', 'Glass Onion: A Knives Out Mystery', 'Easter Sunday', 'The Killing of a Sacred Deer', 'The Batman', 'Parasite', 'Everything Everywhere All at Once', 'Avengers: Age of Ultron', 'Home Alone', 'The Dark Knight', 'The Perks of Being a Wallflower', 'Sorry to Bother You', 'Spider-Man: No Way Home', 'Midsommar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 67/200 [00:05<00:09, 13.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avatar: The Way of Water', 'Turning Red', 'The Virgin Suicides', 'Irreversible', 'Knives Out', 'Thor: Love and Thunder', 'Whiplash', 'Glass Onion: A Knives Out Mystery', 'Easter Sunday', 'The Killing of a Sacred Deer', 'The Batman', 'Parasite', 'Everything Everywhere All at Once', 'Avengers: Age of Ultron', 'Home Alone', 'The Dark Knight', 'The Perks of Being a Wallflower', 'Sorry to Bother You', 'Spider-Man: No Way Home', 'Midsommar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06737109872768\n",
      "Epoch: 300, NDCG: 0.8884503437277618, Recall@20: 0.06737109872768\n",
      "Average number of matches: 5.36\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 80.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.062054529955538736\n",
      "0.8886445148971284\n",
      "torch.Size([2, 6654])\n",
      "User: 09wynna_reviews.csv\n",
      "['The Shining', 'The Raid', 'Return of the Jedi', 'The X Files', 'Inside Out', 'Pulp Fiction', 'Lilo & Stitch', '28 Days Later', 'Glass Onion: A Knives Out Mystery', 'ERASED', 'The Silence of the Lambs', 'The Lord of the Rings: The Return of the King', 'The Last Samurai', 'The Muppet Christmas Carol', 'Little Miss Sunshine', 'Parasite', 'Back to the Future', 'Jojo Rabbit', 'The Lighthouse', 'X', 'No Time to Die', 'Shang-Chi and the Legend of the Ten Rings', 'Togo', 'Skyfall', 'Sound of Metal', 'Midsommar', 'The Truman Show', 'The Thing']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:00<00:11, 16.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glass Onion: A Knives Out Mystery', 'Parasite', 'Avatar: The Way of Water', 'Turning Red', 'Avengers: Endgame', 'Get Out', 'Lady Bird', 'The Truman Show', 'Shutter Island', 'Mad Max: Fury Road', 'Pulp Fiction', 'Top Gun: Maverick', 'Spree', 'Eternal Sunshine of the Spotless Mind', 'Django Unchained', 'Coco', 'Gone Girl', \"Guillermo del Toro's Pinocchio\", 'Scott Pilgrim vs. the World', 'Memento']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [00:00<00:11, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glass Onion: A Knives Out Mystery', 'Parasite', 'Avatar: The Way of Water', 'Turning Red', 'Avengers: Endgame', 'Get Out', 'Lady Bird', 'The Truman Show', 'Shutter Island', 'Mad Max: Fury Road', 'Pulp Fiction', 'Top Gun: Maverick', 'Spree', 'Eternal Sunshine of the Spotless Mind', 'Django Unchained', 'Coco', 'Gone Girl', \"Guillermo del Toro's Pinocchio\", 'Scott Pilgrim vs. the World', 'Memento']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 83/200 [00:06<00:10, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glass Onion: A Knives Out Mystery', 'Parasite', 'Avatar: The Way of Water', 'Turning Red', 'Avengers: Endgame', 'Get Out', 'Lady Bird', 'The Truman Show', 'Shutter Island', 'Mad Max: Fury Road', 'Pulp Fiction', 'Top Gun: Maverick', 'Spree', 'Eternal Sunshine of the Spotless Mind', 'Django Unchained', 'Coco', 'Gone Girl', \"Guillermo del Toro's Pinocchio\", 'Scott Pilgrim vs. the World', 'Memento']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07000605063787811\n",
      "Epoch: 350, NDCG: 0.8886445148971284, Recall@20: 0.07000605063787811\n",
      "Average number of matches: 5.36\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 79.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.062132017945506955\n",
      "0.8887486186419744\n",
      "torch.Size([2, 6627])\n",
      "User: 58hutchi_reviews.csv\n",
      "['Seven Samurai', 'Hannah and Her Sisters', 'The Conversation', 'Michael Clayton', 'A Gray State', 'The Cheat', 'A Foreign Affair', 'Apocalypse Now', 'The Rules of the Game', \"Singin' in the Rain\", 'The Aviator', 'Groundhog Day', 'Shame', 'I Am Cuba', '2001: A Space Odyssey']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:00<00:13, 14.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Black Adam', 'Avatar: The Way of Water', \"The King's Man\", 'Sixteen Candles', 'Thor: Love and Thunder', 'Employee of the Month', 'Captain Marvel', 'Enola Holmes', 'The Power of the Dog', \"To All the Boys I've Loved Before\", 'Knock at the Cabin', 'Hereditary', 'The Mist', 'Do Revenge', 'Avengers: Age of Ultron', 'Before I Fall', 'La La Land', 'Anna', 'Iron Man 3', 'Triangle of Sadness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07700816048160122\n",
      "Epoch: 400, NDCG: 0.8887486186419744, Recall@20: 0.07700816048160122\n",
      "Average number of matches: 5.36\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 75.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.0621588645927732\n",
      "0.8887729298783651\n",
      "torch.Size([2, 6610])\n",
      "User: 2goofs_reviews.csv\n",
      "['Living in Oblivion', 'The Killing of a Chinese Bookie', 'After Hours', 'The Green Knight', 'Good Time', 'The Family Stone', 'Perfect Blue', 'Daisies', 'Monster', 'Chernobyl', 'Under the Silver Lake', 'Taxi Driver', 'TÃ\\x81R', 'Honey Boy', 'Vagabond', \"Who's Afraid of Virginia Woolf?\", 'Sound of Metal', 'Licorice Pizza', 'The Mummy']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:00<00:13, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knives Out', 'Whiplash', 'Glass Onion: A Knives Out Mystery', 'The Batman', 'The Dark Knight', 'Fight Club', 'Turning Red', 'The Grand Budapest Hotel', 'Lady Bird', 'La La Land', 'Get Out', 'Spirited Away', 'Avatar: The Way of Water', 'Dune', 'The Truman Show', 'Shutter Island', 'Taxi Driver', 'Mad Max: Fury Road', 'Inglourious Basterds', 'Pulp Fiction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 106/200 [00:07<00:06, 14.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knives Out', 'Whiplash', 'Glass Onion: A Knives Out Mystery', 'The Batman', 'The Dark Knight', 'Fight Club', 'Turning Red', 'The Grand Budapest Hotel', 'Lady Bird', 'La La Land', 'Get Out', 'Spirited Away', 'Avatar: The Way of Water', 'Dune', 'The Truman Show', 'Shutter Island', 'Taxi Driver', 'Mad Max: Fury Road', 'Inglourious Basterds', 'Pulp Fiction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 110/200 [00:08<00:06, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knives Out', 'Whiplash', 'Glass Onion: A Knives Out Mystery', 'The Batman', 'The Dark Knight', 'Fight Club', 'Turning Red', 'The Grand Budapest Hotel', 'Lady Bird', 'La La Land', 'Get Out', 'Spirited Away', 'Avatar: The Way of Water', 'Dune', 'The Truman Show', 'Shutter Island', 'Taxi Driver', 'Mad Max: Fury Road', 'Inglourious Basterds', 'Pulp Fiction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07103304286504923\n",
      "Epoch: 450, NDCG: 0.8887729298783651, Recall@20: 0.07103304286504923\n",
      "Average number of matches: 5.36\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 79.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.0621570982039993\n",
      "0.8887714176524009\n",
      "torch.Size([2, 6562])\n",
      "User: 6kamikaze9_reviews.csv\n",
      "['Puparia', 'A Monster Calls', 'Dark Shadows', 'Saw', 'Edward Scissorhands', \"Miss Peregrine's Home for Peculiar Children\", 'Champions', 'Dahmer â\\x80\\x93 Monster: The Jeffrey Dahmer Story', 'Puss in Boots', 'Moon Knight', 'Death Proof', 'Kimi no Iro', 'The Stranger by the Shore', 'Colorful', 'Jumanji: The Next Level', 'Alvin and the Chipmunks: The Road Chip', 'From Dusk Till Dawn', 'Joker', \"Zip & Zap and the Captain's Island\", 'Top Gun']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whiplash', 'Glass Onion: A Knives Out Mystery', 'The French Dispatch', 'Everything Everywhere All at Once', 'The Grand Budapest Hotel', 'Fight Club', 'Lady Bird', 'The Piano Teacher', 'The Dark Knight', 'Parasite', 'The Batman', 'La La Land', 'Spirited Away', 'Get Out', 'Spree', 'Scott Pilgrim vs. the World', 'Inglourious Basterds', 'Dune', 'Mad Max: Fury Road', 'Top Gun: Maverick']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:14<00:00, 13.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07565412724795317\n",
      "Epoch: 500, NDCG: 0.8887714176524009, Recall@20: 0.07565412724795317\n",
      "Average number of matches: 5.36\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 350/350 [00:04<00:00, 77.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.06206550076630591\n",
      "0.8886659323718712\n",
      "torch.Size([2, 6718])\n",
      "User: 0becalp_reviews.csv\n",
      "['The Devil Wears Prada', 'My Love From Another Star', 'The Perks of Being a Wallflower', 'Doctor Strange in the Multiverse of Madness', \"Howl's Moving Castle\", 'Venom', 'Labyrinth', 'Emergency Couple', 'Spirited Away', 'Fantastic Beasts: The Crimes of Grindelwald', 'Search: WWW', 'Sense and Sensibility', 'Hercules', 'Shang-Chi and the Legend of the Ten Rings', \"Master's Sun\", 'The Day Naruto Became Hokage', 'Spellbound', \"But I'm a Cheerleader\", 'Along with the Gods: The Two Worlds']\n",
      "Computing recommendations for 200 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knives Out', 'Whiplash', 'Glass Onion: A Knives Out Mystery', 'Everything Everywhere All at Once', 'The Batman', 'Turning Red', 'The Dark Knight', 'Fight Club', 'Get Out', 'The Grand Budapest Hotel', 'Lady Bird', 'Spirited Away', 'La La Land', 'Avatar: The Way of Water', 'Dune', 'Taxi Driver', 'The Truman Show', 'Mad Max: Fury Road', 'Shutter Island', 'Scott Pilgrim vs. the World']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 64/200 [00:04<00:08, 16.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knives Out', 'Whiplash', 'Glass Onion: A Knives Out Mystery', 'Everything Everywhere All at Once', 'The Batman', 'Turning Red', 'The Dark Knight', 'Fight Club', 'Get Out', 'The Grand Budapest Hotel', 'Lady Bird', 'Spirited Away', 'La La Land', 'Avatar: The Way of Water', 'Dune', 'Taxi Driver', 'The Truman Show', 'Mad Max: Fury Road', 'Shutter Island', 'Scott Pilgrim vs. the World']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 73/200 [00:05<00:09, 13.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 152\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39mprint\u001b[39m(mean_ndcg)\n\u001b[0;32m    151\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mNDCG/val\u001b[39m\u001b[39m\"\u001b[39m, mean_ndcg\u001b[39m.\u001b[39mitem(), epoch \u001b[39m*\u001b[39m (num_train_users \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m BATCH_SIZE) \u001b[39m+\u001b[39m start_idx \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m BATCH_SIZE)\n\u001b[1;32m--> 152\u001b[0m recall_at_k \u001b[39m=\u001b[39m compute_recall_at_k(validation_graph, model, K)\n\u001b[0;32m    153\u001b[0m \u001b[39mprint\u001b[39m(recall_at_k)\n\u001b[0;32m    154\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mRecall@K/val\u001b[39m\u001b[39m\"\u001b[39m, recall_at_k, epoch \u001b[39m*\u001b[39m (num_train_users \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m BATCH_SIZE) \u001b[39m+\u001b[39m start_idx \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m BATCH_SIZE)\n",
      "Cell \u001b[1;32mIn[100], line 43\u001b[0m, in \u001b[0;36mcompute_recall_at_k\u001b[1;34m(validation_graph, model, K)\u001b[0m\n\u001b[0;32m     41\u001b[0m tick \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m all_edges \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([(user_id, item_id) \u001b[39mfor\u001b[39;00m item_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_train_users, num_train_items)], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39mt()\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m---> 43\u001b[0m recommendations \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mrecommend(all_edges\u001b[39m.\u001b[39mto(device), src_index\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor([user_id])\u001b[39m.\u001b[39mto(device), dst_index\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mtensor([x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(num_train_users \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, num_train_items)])\u001b[39m.\u001b[39;49mto(device), k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m K)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     44\u001b[0m tock \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     45\u001b[0m train_edges_for_user \u001b[39m=\u001b[39m training_edges[:, training_edges[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m user_id]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "LR = 1e-1\n",
    "BATCH_SIZE = min(4096, len(user_review_data))\n",
    "EMBEDDING_DIM = 512\n",
    "LOAD_CHECKPOINT = False\n",
    "K = 20\n",
    "model = LightGCN(num_nodes=num_nodes, embedding_dim=EMBEDDING_DIM, num_layers=NUM_LAYERS)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    model.load_state_dict(torch.load(f'models/{EMBEDDING_DIM}_{NUM_LAYERS}_{1024}_{1e-3}_{num_train_users}_{143295}.pt', map_location=device))\n",
    "\n",
    "print(\"Running on device: {}\".format(device))\n",
    "print(EMBEDDING_DIM)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[100, 200, 300, 400], gamma=0.5)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=100)\n",
    "\n",
    "train_positive_edges = train_graph.edge_index[:, train_graph.edge_attr >= 3.5].cuda()\n",
    "train_negative_edges = train_graph.edge_index[:, train_graph.edge_attr <= 2.5].cuda()\n",
    "\n",
    "validation_df = pd.DataFrame.from_dict(validation_reviews)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment=f'LightGCN_{EMBEDDING_DIM}_layers_{NUM_LAYERS}_batch_size_{BATCH_SIZE}_lr_{LR}_num_train_users_{num_train_users}_num_train_items_{num_train_items}_recall_{K}')\n",
    "\n",
    "for epoch in range(10001):\n",
    "    # we are using BPR so we go by user\n",
    "    # We'll proceed in batches of users\n",
    "    for start_idx in tqdm(range(0, num_train_users, BATCH_SIZE)):\n",
    "        model.train()\n",
    "        all_positive_rankings = torch.tensor([]).cuda()\n",
    "        all_negative_rankings = torch.tensor([]).cuda()\n",
    "        # randomly select a batch of users\n",
    "        users_in_batch = torch.randperm(num_train_users)[start_idx:start_idx + BATCH_SIZE]\n",
    "        # for each user randomly select a positive edge and 5 negative edges\n",
    "        # use torch to do this efficiently\n",
    "        for user_id in users_in_batch:\n",
    "            # get one random positive edge\n",
    "            user_positive_edges = train_positive_edges[:, train_positive_edges[0] == user_id]\n",
    "            user_negative_edges = train_negative_edges[:, train_negative_edges[0] == user_id]\n",
    "            if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
    "                continue\n",
    "            # randomly select a positive edge\n",
    "            positive_edge = user_positive_edges[:, torch.randint(0, user_positive_edges.shape[1], (1,))]\n",
    "            # randomly select 5 negative edges\n",
    "            negative_edges = user_negative_edges[:, torch.randint(0, user_negative_edges.shape[1], (5,))]\n",
    "            user_edges = torch.cat((positive_edge, negative_edges), dim=1)\n",
    "            # get the rankings of the positive and negative edges\n",
    "            user_rankings = model(user_edges)\n",
    "            # compute the loss\n",
    "            positive_rankings = user_rankings[0].unsqueeze(0).repeat(5)\n",
    "            negative_rankings = user_rankings[1:]\n",
    "            all_positive_rankings = torch.cat((all_positive_rankings, positive_rankings))\n",
    "            all_negative_rankings = torch.cat((all_negative_rankings, negative_rankings))\n",
    "        # compute the loss\n",
    "        loss = model.recommendation_loss(all_positive_rankings, all_negative_rankings)\n",
    "        # for user_id in users_in_batch:\n",
    "        #     # get all the edges specific to this user\n",
    "        #     user_positive_edges = train_positive_edges[:, train_positive_edges[0] == user_id]\n",
    "        #     user_negative_edges = train_negative_edges[:, train_negative_edges[0] == user_id]\n",
    "        #     if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
    "        #         continue\n",
    "        #     # limit the number of positive edges to 5000\n",
    "        #     if (user_positive_edges.shape[1] > 5000):\n",
    "        #         user_positive_edges = user_positive_edges[:, :5000]\n",
    "        #     # Get at most 15000 negative edges\n",
    "        #     if (user_negative_edges.shape[1] > 15000):\n",
    "        #         user_negative_edges = user_negative_edges[:, :15000]\n",
    "        #     # resample the negative edges if we don't have enough\n",
    "        #     user_positive_edges, user_negative_edges = resample_edges_for_user(user_positive_edges, user_negative_edges)\n",
    "        #     # concatenate the positive and negative edges\n",
    "        #     user_edges = torch.cat([user_positive_edges, user_negative_edges], dim=1)\n",
    "        #     # get the rankings for this user\n",
    "        #     user_edges = user_edges.to(device)\n",
    "        #     user_rankings = model(user_edges)\n",
    "        #     # divide the rankings into positive and negative rankings\n",
    "        #     user_positive_rankings = user_rankings[:user_positive_edges.shape[1]]\n",
    "        #     user_negative_rankings = user_rankings[user_positive_edges.shape[1]:]\n",
    "        #     # create all pairs of positive and negative rankings\n",
    "        #     user_positive_rankings = user_positive_rankings.unsqueeze(1).repeat(1, user_negative_rankings.shape[0])\n",
    "        #     user_negative_rankings = user_negative_rankings.unsqueeze(0).repeat(user_positive_rankings.shape[0], 1)\n",
    "        #     # get the user loss\n",
    "        #     user_loss = model.recommendation_loss(user_positive_rankings, user_negative_rankings, 1e-4)\n",
    "        #     # add the user loss to the total loss\n",
    "        #     loss = loss + user_loss\n",
    "        # # divide the loss by the number of users\n",
    "        # loss = loss / BATCH_SIZE\n",
    "        # log the loss\n",
    "        # backprop\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "    if epoch % 50 == 0 and epoch > 0:\n",
    "        # evaluate the model\n",
    "        model.eval()\n",
    "        # iterate over all users in the validation set\n",
    "        validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "        # randomly select 1000 of the users\n",
    "        validation_users = random.sample(validation_users, min(len(validation_users), 500))\n",
    "        mean_ndcg = 0\n",
    "        ndcg_scores = []\n",
    "        for user in tqdm(validation_users):\n",
    "            user_id = id_to_user[user]\n",
    "            relevant_reviews = validation_df[validation_df['user_id'] == user_id]\n",
    "            user_validation_edges = validation_edges[:, validation_edges[0] == user]\n",
    "            user_validation_edges = user_validation_edges.to(device)\n",
    "            user_rankings = model(user_validation_edges)\n",
    "            edges_sorted = list(user_validation_edges[1, user_rankings.argsort(descending=True)])\n",
    "            # use validation_df to get the relevances via the movie_id column and the movie_rating column\n",
    "            relevances = []\n",
    "            for edge in edges_sorted:\n",
    "                movie_id = id_to_movie[int(edge)]\n",
    "                if (movie_id in relevant_reviews['movie_id'].values):\n",
    "                    relevances.append(relevant_reviews[relevant_reviews['movie_id'] == movie_id]['movie_rating'].values[0])\n",
    "                else:\n",
    "                    relevances.append(0)\n",
    "            # calculate the ndcg\n",
    "            if (len(relevances) >= K):\n",
    "                ndcg = compute_ndcg_at_k(relevances, k=K)\n",
    "            if (math.isnan(ndcg)):\n",
    "                print(relevant_reviews)\n",
    "                input()\n",
    "            mean_ndcg += ndcg\n",
    "            ndcg_scores.append(ndcg)\n",
    "        mean_ndcg = mean_ndcg / len(validation_users)\n",
    "        print(\"Standard Deviation: {}\".format(np.std(ndcg_scores)))\n",
    "        # create a histogram of the ndcg scores, make bins for each 0.1\n",
    "        ndcg_scores = np.array(ndcg_scores).squeeze()\n",
    "        writer.add_histogram(\"hist_NDCG/val\", ndcg_scores, epoch)\n",
    "        # also make a histogram in matplotlib and save as png\n",
    "        plt.hist(ndcg_scores, bins=np.arange(0, 1.1, 0.1))\n",
    "        plt.suptitle(\"Validation NDCG Histogram\")\n",
    "        # write information about the model to the histogram\n",
    "        plt.title(f\"Model: LightGCN, Embedding Dim: {EMBEDDING_DIM}, Num Layers: {NUM_LAYERS}, Batch Size: {BATCH_SIZE}, LR: {LR}, Num Train Users: {num_train_users}, Num Train Items: {num_train_items}\", fontsize=8, wrap=True)\n",
    "        plt.xlabel(\"NDCG\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        # save the figure in the hist_NDCG folder, with the title having the model information and the epoch number\n",
    "        plt.savefig(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.png\")\n",
    "        plt.close()\n",
    "        # Also save the raw NDCG scores to a csv file, with the model information in the title, and the epoch number\n",
    "        np.savetxt(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.csv\", ndcg_scores, delimiter=\",\")\n",
    "        print(mean_ndcg)\n",
    "        writer.add_scalar(\"NDCG/val\", mean_ndcg.item(), epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "        recall_at_k = compute_recall_at_k(validation_graph, model, K)\n",
    "        print(recall_at_k)\n",
    "        writer.add_scalar(\"Recall@K/val\", recall_at_k, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "        print(\"Epoch: {}, NDCG: {}, Recall@{}: {}\".format(epoch, mean_ndcg, K, recall_at_k))\n",
    "        average_number_of_matches = 0\n",
    "        for user_id in validation_users:\n",
    "            all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "            recommendations = model.recommend(all_edges.to(device), src_index=torch.tensor([user_id]).to(device), dst_index=torch.tensor([x for x in range(num_train_users + 1, num_train_items)]).to(device), k=10)[0]\n",
    "            movie_names = [movie_id_to_movie_name[id_to_movie[int(recommendation)]] for recommendation in recommendations]\n",
    "            true_user_reviews = user_review_data[id_to_user[user_id]]\n",
    "            matches = 0\n",
    "            for movie_name in movie_names:\n",
    "                if movie_name in true_user_reviews['movie_title'].values:\n",
    "                    matches += 1\n",
    "            average_number_of_matches += matches\n",
    "        average_number_of_matches = average_number_of_matches / len(validation_users)\n",
    "        print(\"Average number of matches: {}\".format(average_number_of_matches))\n",
    "        writer.add_scalar(\"Average number of matches\", average_number_of_matches, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "        print(\"=====================================\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), f\"models/{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.376630903760431e-05\n"
     ]
    }
   ],
   "source": [
    "for param_group in optim.param_groups:\n",
    "    print(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "validation_df[validation_df.user_id == id_to_user[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_edges[:, validation_edges[0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
