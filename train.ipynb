{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from torch_geometric) (1.24.2)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from torch_geometric) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from torch_geometric) (4.64.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from torch_geometric) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from torch_geometric) (1.2.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/site-packages (from torch_geometric) (5.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.14)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
    "!pip install torch\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models.lightgcn import LightGCN\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use the first 10k rows of the data, set to None to use all data\n",
<<<<<<< HEAD
    "AMOUNT_TO_LOAD = None\n",
    "\n",
    "# We also need to mount in a google drive \n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
=======
    "AMOUNT_TO_LOAD = 1000"
>>>>>>> 62c973f (recall at k)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  2%|▏         | 1000/63111 [00:02<02:23, 433.42it/s]\n"
     ]
    }
   ],
   "source": [
    "user_reviews_dir = 'user_reviews'\n",
    "\n",
    "# We are in the google drive setting then\n",
    "if not os.path.exists(user_reviews_dir):\n",
    "  user_reviews_dir = 'gdrive/MyDrive/user_reviews'\n",
    "\n",
    "user_review_data = dict()\n",
    "\n",
    "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
    "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
    "        break\n",
    "    try:\n",
    "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Empty file: {filename}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
    "\n",
    "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "asel82_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(list(user_review_data.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 830.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove all values with nan in the review column\n",
    "for key in tqdm(user_review_data.keys()):\n",
    "    user_review_data[key] = user_review_data[key].dropna(subset=['movie_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 400.98it/s]Train reviews: 420659\n",
      "Validation reviews: 4685\n",
      "Test reviews: 4685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = []\n",
    "validation_reviews = []\n",
    "test_reviews = []\n",
    "for user_id, reviews in tqdm(user_review_data.items()):\n",
    "    if len(reviews) > 15:\n",
    "        validation_review_data = reviews.sample(5, replace=False).to_dict('records')\n",
    "        for review in validation_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        validation_reviews.extend(validation_review_data)\n",
    "        test_review_data = reviews.sample(5, replace=False).to_dict('records')\n",
    "        for review in test_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        test_reviews.extend(test_review_data)\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "    else:\n",
    "        # if the user has less than 5 reviews, we will use all of them for training\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "\n",
    "print(f'Train reviews: {len(train_reviews)}')\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Now that we have the training data, let's construct the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of train users: 1000\nNumber of train items: 44001\nNumber of nodes: 45001\n"
     ]
    }
   ],
   "source": [
    "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
    "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
    "num_nodes = num_train_users + num_train_items\n",
    "print(f'Number of train users: {num_train_users}')\n",
    "print(f'Number of train items: {num_train_items}')\n",
    "print(f'Number of nodes: {num_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val_users = len(set([review['user_id'] for review in validation_reviews]))\n",
    "num_val_items = len(set([review['movie_id'] for review in validation_reviews]))\n",
    "num_val_nodes = num_val_users + num_val_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map users to ids\n",
    "movie_id_to_movie_name = dict()\n",
    "for review in train_reviews:\n",
    "    movie_id_to_movie_name[review['movie_id']] = review['movie_title']\n",
    "\n",
    "user_to_id = dict()\n",
    "for i, user_id in enumerate(set([review['user_id'] for review in train_reviews])):\n",
    "    user_to_id[user_id] = i\n",
    "\n",
    "# Let's map movies to ids\n",
    "movie_to_id = dict()\n",
    "for i, movie_id in enumerate(set([review['movie_id'] for review in train_reviews])):\n",
    "    movie_to_id[movie_id] = i + num_train_users\n",
    "\n",
    "# Let's map ids to users\n",
    "id_to_user = dict()\n",
    "for user_id, index in user_to_id.items():\n",
    "    id_to_user[index] = user_id\n",
    "\n",
    "# Let's map ids to movies\n",
    "id_to_movie = dict()\n",
    "for movie_id, index in movie_to_id.items():\n",
    "    id_to_movie[index] = movie_id\n",
    "\n",
    "# Let's map movie names to movie ids\n",
    "movie_name_to_movie_id = dict()\n",
    "for movie_id, movie_name in movie_id_to_movie_name.items():\n",
    "    movie_name_to_movie_id[movie_name] = movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add nodes that are in our validation and test sets but not in our training set\n",
    "for review in validation_reviews:\n",
    "    if review['user_id'] not in user_to_id:\n",
    "        user_to_id[review['user_id']] = len(user_to_id)\n",
    "    if review['movie_id'] not in movie_to_id:\n",
    "        movie_to_id[review['movie_id']] = len(movie_to_id) + num_train_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert_review_to_edge(review):\n",
    "    user_id = user_to_id[review['user_id']]\n",
    "    movie_id = movie_to_id[review['movie_id']]\n",
    "    edge_weight = review['movie_rating']\n",
    "    if (edge_weight < 3.5 and edge_weight > 2.5):\n",
    "        return None, None\n",
    "    edge = (user_id, movie_id)\n",
    "    edge_weight = review['movie_rating']\n",
    "    return edge, edge_weight\n",
    "\n",
    "def shuffle_edges_and_edge_weights(edges, edge_weights):\n",
    "    c = list(zip(edges, edge_weights))\n",
    "    random.shuffle(c)\n",
    "    return zip(*c)\n",
    "\n",
    "def convert_reviews_to_edges(reviews):\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    for review in tqdm(reviews):\n",
    "        edge, edge_weight = convert_review_to_edge(review)\n",
    "        if edge is not None:\n",
    "            edges.append(edge)\n",
    "            edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Reformat the edges to be a tensor\n",
    "    edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edges, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 420659/420659 [00:00<00:00, 978024.43it/s] \n",
      "100%|██████████| 4685/4685 [00:00<00:00, 577186.50it/s]Train edges: 338642\n",
      "Validation edges: 3881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the edges between users and movies.\n",
    "# The id of the user will be the index of the user in the user_to_id dict\n",
    "# The id of the movie will be the index of the movie in the movie_to_id dict + the number of users\n",
    "\n",
    "train_edges, train_edge_weights = convert_reviews_to_edges(train_reviews)\n",
    "validation_edges, validation_edge_weights = convert_reviews_to_edges(validation_reviews)\n",
    "\n",
    "print(f'Train edges: {train_edges.shape[1]}')\n",
    "print(f'Validation edges: {validation_edges.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as data\n",
    "\n",
    "# create the graph\n",
    "train_graph = data.Data(\n",
    "    edge_index=train_edges,\n",
    "    edge_attr=torch.tensor(train_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "validation_graph = data.Data(\n",
    "    edge_index=validation_edges,\n",
    "    edge_attr=torch.tensor(validation_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "train_graph.validate(raise_on_error=True)\n",
    "validation_graph.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some negative edges\n",
    "def resample_edges_for_user(user_positive_edges, user_negative_edges):\n",
    "    num_negative_edges_to_add = user_positive_edges.shape[1] * 3 - user_negative_edges.shape[1]\n",
    "    if (num_negative_edges_to_add <= 0):\n",
    "        num_negative_edges_to_remove = -num_negative_edges_to_add\n",
    "        # choose the negative edges to keep\n",
    "        negative_edges_to_keep = torch.randint(user_negative_edges.shape[1], (user_negative_edges.shape[1] - num_negative_edges_to_remove,))\n",
    "        # remove all the negative edges for this user\n",
    "        user_negative_edges = user_negative_edges[:, negative_edges_to_keep]\n",
    "    else:\n",
    "        # Create new negative edges\n",
    "        negative_edges_to_add = torch.tensor([[user_id] * num_negative_edges_to_add, torch.randint(num_train_users, num_train_items, (num_negative_edges_to_add,))], dtype=torch.long)\n",
    "        # Add the negative edges to the negative edges for this user\n",
    "        user_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
    "    return user_positive_edges, user_negative_edges\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute ndcg\n",
    "def compute_ndcg_at_k(relevances, k=5):\n",
    "    relevances = relevances[:k]\n",
    "    dcg = 0\n",
    "    for i, relevance in enumerate(relevances):\n",
    "        dcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
    "    idcg = 0\n",
    "    for i, relevance in enumerate(sorted(relevances, reverse=True)):\n",
    "        idcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adapted from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/lightgcn.html\"\"\"\n",
    "class CustomLightGCN(torch.nn.Module):\n",
    "    \"\"\"From the <https://arxiv.org/abs/2002.02126>` paper.\n",
    "\n",
    "    Args:\n",
    "        num_nodes (int): The number of nodes in the graph.\n",
    "        embedding_dim (int): The dimensionality of node embeddings.\n",
    "        num_layers (int): The number of layers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        embedding_dim: int,\n",
    "        num_layers: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "        self.alpha = torch.tensor([1. / (num_layers + 1)] * (num_layers + 1))\n",
    "        self.convs = ModuleList([GATConv(embedding_dim, embedding_dim, heads=8, dropout=0.6) for _ in range(num_layers)])\n",
    "        self.linears = ModuleList([Linear(embedding_dim * 8, embedding_dim) for _ in range(num_layers)])\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def get_embedding(self, edge_index):\n",
    "        x = self.embedding.weight\n",
    "        out = x * self.alpha[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.linears[i](x.view(-1, self.embedding_dim * 8))\n",
    "            out = out + x * self.alpha[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        edge_label_index = edge_index\n",
    "        out = self.get_embedding(edge_index)\n",
    "        user = out[edge_label_index[0]]\n",
    "        movie = out[edge_label_index[1]]\n",
    "        return (user * movie).sum(dim=-1)\n",
    "\n",
    "\n",
    "    def predict_link(self, edge_index, edge_label_index):\n",
    "        \"Predict links between nodes specified in edge_label_index.\"\"\"\n",
    "        pred = self(edge_index, edge_label_index).sigmoid()\n",
    "        return pred.round()\n",
    "\n",
    "\n",
    "    def recommend(self, edge_index, k):\n",
    "        \"\"\"Get top-k recommendations for nodes in src_index.\"\"\"\n",
    "        out_user = self.get_embedding(edge_index)\n",
    "        out_movie = self.get_embedding(edge_index)\n",
    "        pred = out_user @ out_movie.t()\n",
    "        top_index = pred.topk(k, dim=-1).indices\n",
    "        return top_index\n",
    "\n",
    "\n",
    "    def link_pred_loss(self, pred, edge_label):\n",
    "        \"\"\"Computes the model loss for a link prediction using torch.nn.BCEWithLogitsLoss.\n",
    "        \n",
    "        Args:\n",
    "            pred (torch.Tensor): The predictions.\n",
    "            edge_label (torch.Tensor): The ground-truth edge labels.\n",
    "        \"\"\"\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "\n",
    "    def recommendation_loss(self, pos_edge_rank, neg_edge_rank,\n",
    "                            lambda_reg: float = 1e-4):\n",
    "        \"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "        Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "            pos_edge_rank (torch.Tensor): Positive edge rankings.\n",
    "            neg_edge_rank (torch.Tensor): Negative edge rankings.\n",
    "            lambda_reg (int, optional): The L2 regularization strength\n",
    "                of the Bayesian Personalized Ranking (BPR) loss.\n",
    "        \"\"\"\n",
    "        loss_fn = BPRLoss(lambda_reg)\n",
    "        return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is verbatim from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/lightgcn.html. \"\"\"\n",
    "class BPRLoss(_Loss):\n",
    "    \"\"\"The Bayesian Personalized Ranking (BPR) loss.\"\"\"\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = 0\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        \"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).mean()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on device: cpu\n",
      "64\n",
      " 25%|██▌       | 1/4 [00:09<00:28,  9.49s/it]0\n",
      " 50%|█████     | 2/4 [00:18<00:18,  9.05s/it]1\n",
      " 75%|███████▌  | 3/4 [00:27<00:09,  9.31s/it]2\n",
      "100%|██████████| 4/4 [00:36<00:00,  9.23s/it]\n",
      "3\n",
      "100%|██████████| 100/100 [00:01<00:00, 97.94it/s]\n",
      "Standard Deviation: 0.10059097281071949\n",
      " 25%|██▌       | 1/4 [00:11<00:35, 11.78s/it]256\n",
      " 50%|█████     | 2/4 [00:23<00:23, 11.71s/it]257\n",
      " 75%|███████▌  | 3/4 [00:33<00:11, 11.16s/it]258\n",
      "100%|██████████| 4/4 [00:42<00:00, 10.68s/it]\n",
      "259\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.53it/s]\n",
      "Standard Deviation: 0.11194050562219879\n",
      " 25%|██▌       | 1/4 [00:10<00:32, 10.88s/it]512\n",
      " 50%|█████     | 2/4 [00:19<00:19,  9.75s/it]513\n",
      " 75%|███████▌  | 3/4 [00:28<00:09,  9.46s/it]514\n",
      "100%|██████████| 4/4 [00:36<00:00,  9.23s/it]\n",
      "515\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.25it/s]\n",
      "Standard Deviation: 0.11638474372922078\n",
      " 25%|██▌       | 1/4 [00:09<00:29,  9.91s/it]768\n",
      " 50%|█████     | 2/4 [00:18<00:18,  9.23s/it]769\n",
      " 75%|███████▌  | 3/4 [00:27<00:09,  9.04s/it]770\n",
      "100%|██████████| 4/4 [00:34<00:00,  8.67s/it]\n",
      "771\n",
      "100%|██████████| 100/100 [00:00<00:00, 110.44it/s]\n",
      "Standard Deviation: 0.1162094301993785\n",
      " 25%|██▌       | 1/4 [00:10<00:32, 10.97s/it]1024\n",
      " 50%|█████     | 2/4 [00:18<00:17,  8.77s/it]1025\n",
      " 75%|███████▌  | 3/4 [00:27<00:09,  9.01s/it]1026\n",
      "100%|██████████| 4/4 [00:34<00:00,  8.65s/it]\n",
      "1027\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.00it/s]\n",
      "Standard Deviation: 0.101007813576443\n",
      " 25%|██▌       | 1/4 [00:11<00:34, 11.46s/it]1280\n",
      " 50%|█████     | 2/4 [00:21<00:21, 10.77s/it]1281\n",
      " 75%|███████▌  | 3/4 [00:31<00:10, 10.31s/it]1282\n",
      "100%|██████████| 4/4 [00:39<00:00,  9.94s/it]\n",
      "1283\n",
      "100%|██████████| 100/100 [00:00<00:00, 112.81it/s]\n",
      "Standard Deviation: 0.09643754141308872\n",
      " 25%|██▌       | 1/4 [00:11<00:33, 11.06s/it]1536\n",
      " 50%|█████     | 2/4 [00:20<00:19,  9.94s/it]1537\n",
      " 50%|█████     | 2/4 [00:21<00:21, 10.98s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# get the rankings for this user\u001b[39;00m\n\u001b[1;32m     54\u001b[0m user_edges \u001b[38;5;241m=\u001b[39m user_edges\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 55\u001b[0m user_rankings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_edges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# divide the rankings into positive and negative rankings\u001b[39;00m\n\u001b[1;32m     57\u001b[0m user_positive_rankings \u001b[38;5;241m=\u001b[39m user_rankings[:user_positive_edges\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_geometric/nn/models/lightgcn.py:119\u001b[0m, in \u001b[0;36mLightGCN.forward\u001b[0;34m(self, edge_index, edge_label_index)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         edge_label_index \u001b[38;5;241m=\u001b[39m edge_index\n\u001b[0;32m--> 119\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m out_src \u001b[38;5;241m=\u001b[39m out[edge_label_index[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    122\u001b[0m out_dst \u001b[38;5;241m=\u001b[39m out[edge_label_index[\u001b[38;5;241m1\u001b[39m]]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_geometric/nn/models/lightgcn.py:97\u001b[0m, in \u001b[0;36mLightGCN.get_embedding\u001b[0;34m(self, edge_index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m     96\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[i](x, edge_index)\n\u001b[0;32m---> 97\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "LR = 5e-05\n",
    "BATCH_SIZE = num_train_users\n",
    "EMBEDDING_DIM = 64\n",
    "model = LightGCN(num_nodes=num_nodes, embedding_dim=EMBEDDING_DIM, num_layers=NUM_LAYERS)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Running on device: {}\".format(device))\n",
    "print(EMBEDDING_DIM)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[100, 200, 300, 400], gamma=0.5)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=100)\n",
    "\n",
    "train_positive_edges = train_graph.edge_index[:, train_graph.edge_attr >= 3.5]\n",
    "train_negative_edges = train_graph.edge_index[:, train_graph.edge_attr <= 2.5]\n",
    "\n",
    "validation_df = pd.DataFrame.from_dict(validation_reviews)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment=f'LightGCN_{EMBEDDING_DIM}_layers_{NUM_LAYERS}_batch_size_{BATCH_SIZE}_lr_{LR}_num_train_users_{num_train_users}_num_train_items_{num_train_items}')\n",
    "\n",
    "for epoch in range(10001):\n",
    "    # we are using BPR so we go by user\n",
    "    average_loss = 0\n",
    "    # We'll proceed in batches of users\n",
    "    for start_idx in tqdm(range(0, num_train_users, BATCH_SIZE)):\n",
    "        model.train()\n",
    "        loss = torch.tensor(0.0, requires_grad=True)\n",
    "        # randomly select a batch of users\n",
    "        users_in_batch = torch.randperm(num_train_users)[start_idx:start_idx + BATCH_SIZE]\n",
    "        for user_id in users_in_batch:\n",
    "            # get all the edges specific to this user\n",
    "            user_positive_edges = train_positive_edges[:, train_positive_edges[0] == user_id]\n",
    "            user_negative_edges = train_negative_edges[:, train_negative_edges[0] == user_id]\n",
    "            if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
    "                continue\n",
    "            # limit the number of positive edges to 5000\n",
    "            if (user_positive_edges.shape[1] > 5000):\n",
    "                user_positive_edges = user_positive_edges[:, :5000]\n",
    "            # Get at most 15000 negative edges\n",
    "            if (user_negative_edges.shape[1] > 15000):\n",
    "                user_negative_edges = user_negative_edges[:, :15000]\n",
    "            # resample the negative edges if we don't have enough\n",
    "            user_positive_edges, user_negative_edges = resample_edges_for_user(user_positive_edges, user_negative_edges)\n",
    "            # concatenate the positive and negative edges\n",
    "            user_edges = torch.cat([user_positive_edges, user_negative_edges], dim=1)\n",
    "            # get the rankings for this user\n",
    "            user_edges = user_edges.to(device)\n",
    "            user_rankings = model(user_edges)\n",
    "            # divide the rankings into positive and negative rankings\n",
    "            user_positive_rankings = user_rankings[:user_positive_edges.shape[1]]\n",
    "            user_negative_rankings = user_rankings[user_positive_edges.shape[1]:]\n",
    "            # create all pairs of positive and negative rankings\n",
    "            user_positive_rankings = user_positive_rankings.unsqueeze(1).repeat(1, user_negative_rankings.shape[0])\n",
    "            user_negative_rankings = user_negative_rankings.unsqueeze(0).repeat(user_positive_rankings.shape[0], 1)\n",
    "            # get the user loss\n",
    "            user_loss = model.recommendation_loss(user_positive_rankings, user_negative_rankings, 1e-4)\n",
    "            # add the user loss to the total loss\n",
    "            loss = loss + user_loss\n",
    "        # divide the loss by the number of users\n",
    "        loss = loss / BATCH_SIZE\n",
    "        # log the loss\n",
    "        # backprop\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "        print(epoch * BATCH_SIZE + start_idx // BATCH_SIZE)\n",
    "        average_loss = 0\n",
    "    if (epoch % 1 == 0):\n",
    "        # evaluate the model\n",
    "        model.eval()\n",
    "        # iterate over all users in the validation set\n",
    "        validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "        # randomly select 1000 of the users\n",
    "        validation_users = random.sample(validation_users, 100)\n",
    "        mean_ndcg = 0\n",
    "        ndcg_scores = []\n",
    "        for user in tqdm(validation_users):\n",
    "            user_id = id_to_user[user]\n",
    "            relevant_reviews = validation_df[validation_df['user_id'] == user_id]\n",
    "            user_validation_edges = validation_edges[:, validation_edges[0] == user]\n",
    "            user_validation_edges = user_validation_edges.to(device)\n",
    "            user_rankings = model(user_validation_edges)\n",
    "            edges_sorted = list(user_validation_edges[1, user_rankings.argsort(descending=True)])\n",
    "            # use validation_df to get the relevances via the movie_id column and the movie_rating column\n",
    "            relevances = []\n",
    "            for edge in edges_sorted:\n",
    "                movie_id = id_to_movie[int(edge)]\n",
    "                if (movie_id in relevant_reviews['movie_id'].values):\n",
    "                    relevances.append(relevant_reviews[relevant_reviews['movie_id'] == movie_id]['movie_rating'].values[0])\n",
    "                else:\n",
    "                    relevances.append(0)\n",
    "            # calculate the ndcg\n",
    "            ndcg = compute_ndcg_at_k(relevances)\n",
    "            if (math.isnan(ndcg)):\n",
    "                print(relevant_reviews)\n",
    "                input()\n",
    "            mean_ndcg += ndcg\n",
    "            ndcg_scores.append(ndcg)\n",
    "        mean_ndcg = mean_ndcg / len(validation_users)\n",
    "        print(\"Standard Deviation: {}\".format(np.std(ndcg_scores)))\n",
    "        # create a histogram of the ndcg scores, make bins for each 0.1\n",
    "        ndcg_scores = np.array(ndcg_scores).squeeze()\n",
    "        # writer.add_histogram(\"hist_NDCG/val\", ndcg_scores, epoch)\n",
    "        # also make a histogram in matplotlib and save as png\n",
    "        plt.hist(ndcg_scores, bins=np.arange(0, 1.1, 0.1))\n",
    "        plt.suptitle(\"Validation NDCG Histogram\")\n",
    "        # write information about the model to the histogram\n",
    "        plt.title(f\"Model: LightGCN, Embedding Dim: {EMBEDDING_DIM}, Num Layers: {NUM_LAYERS}, Batch Size: {BATCH_SIZE}, LR: {LR}, Num Train Users: {num_train_users}, Num Train Items: {num_train_items}\", fontsize=8, wrap=True)\n",
    "        plt.xlabel(\"NDCG\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        # save the figure in the hist_NDCG folder, with the title having the model information and the epoch number\n",
    "        plt.savefig(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.png\")\n",
    "        plt.close()\n",
    "        # Also save the raw NDCG scores to a csv file, with the model information in the title, and the epoch number\n",
    "        np.savetxt(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.csv\", ndcg_scores, delimiter=\",\")\n",
    "        writer.add_scalar(\"NDCG/val\", mean_ndcg, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           movie_title  movie_rating  movie_id  \\\n",
       "5                        Mars Attacks!           2.0     51975   \n",
       "6                   X-Men: First Class           4.0     16074   \n",
       "7  The Hobbit: The Desolation of Smaug           2.0      8983   \n",
       "8                              Sicario           4.5    197628   \n",
       "9                         White Chicks           4.0     44750   \n",
       "\n",
       "                                   film_slug                    user_id  \n",
       "5                        /film/mars-attacks/  nerevarine808_reviews.csv  \n",
       "6                   /film/x-men-first-class/  nerevarine808_reviews.csv  \n",
       "7  /film/the-hobbit-the-desolation-of-smaug/  nerevarine808_reviews.csv  \n",
       "8                        /film/sicario-2015/  nerevarine808_reviews.csv  \n",
       "9                        /film/white-chicks/  nerevarine808_reviews.csv  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_title</th>\n      <th>movie_rating</th>\n      <th>movie_id</th>\n      <th>film_slug</th>\n      <th>user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>Mars Attacks!</td>\n      <td>2.0</td>\n      <td>51975</td>\n      <td>/film/mars-attacks/</td>\n      <td>nerevarine808_reviews.csv</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>X-Men: First Class</td>\n      <td>4.0</td>\n      <td>16074</td>\n      <td>/film/x-men-first-class/</td>\n      <td>nerevarine808_reviews.csv</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The Hobbit: The Desolation of Smaug</td>\n      <td>2.0</td>\n      <td>8983</td>\n      <td>/film/the-hobbit-the-desolation-of-smaug/</td>\n      <td>nerevarine808_reviews.csv</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Sicario</td>\n      <td>4.5</td>\n      <td>197628</td>\n      <td>/film/sicario-2015/</td>\n      <td>nerevarine808_reviews.csv</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>White Chicks</td>\n      <td>4.0</td>\n      <td>44750</td>\n      <td>/film/white-chicks/</td>\n      <td>nerevarine808_reviews.csv</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "validation_df[validation_df.user_id == id_to_user[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0],\n",
       "        [26802,  6445,  3941, 30679, 21007]])"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "validation_edges[:, validation_edges[0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  415,   415,   415,  ...,   221,   221,   221],\n",
       "        [26767,  8575, 26369,  ..., 36968,  3336, 24881]])"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_recall_at_k(validation_edges, validation_edge_weights, model, num_val_users, num_val_nodes, K):\n",
    "    # get positive edges in validation set\n",
    "    positive_edges = validation_edges[:, torch.tensor(validation_edge_weights).squeeze() >= 3.5]\n",
    "\n",
    "    # map users to positive edges\n",
    "    user_pos_items = get_user_positive_items(positive_edges)\n",
    "\n",
    "    # get users\n",
    "    users = positive_edges[0].unique()\n",
    "\n",
    "    # get movies\n",
    "    movie_indices = torch.LongTensor([_ for _ in range(num_val_users, num_val_nodes)])\n",
    "\n",
    "    # Get positive items for each user in validation set\n",
    "    truth_items = [set(user_pos_items[user.item()]) for user in users]\n",
    "\n",
    "    # Get top-K recommended items for each user in validation set\n",
    "    recommended_items = [set(model.recommend(validation_edges, torch.LongTensor([user]), movie_indices)[:K]) for user in users]\n",
    "\n",
    "    # Compute number of relevant items in top-K recommendations for each user\n",
    "    correctness = [len(truth_items[i].intersection(recommended_items[i])) for i in range(len(users))]\n",
    "\n",
    "    # Compute recall at K\n",
    "    recall = torch.mean(torch.Tensor(correctness) / torch.Tensor([len(truth_items[i]) for i in range(len(users))]))\n",
    "    \n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(compute_recall_at_k(validation_edges, validation_edge_weights, model, num_val_users, num_val_nodes, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3101064bitc77d6772003d4da2b8242c4aef34ae70"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10-final"
  },
  "vscode": {
   "interpreter": {
    "hash": "c42c7180e3fe8f66898b46e6715a19bce1ca98993688e7fbe101188e1cfb25f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
