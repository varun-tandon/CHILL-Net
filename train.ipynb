{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (4.64.1)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.1-cp310-cp310-macosx_12_0_arm64.whl (8.4 MB)\n",
      "Requirement already satisfied: requests in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (2.28.1)\n",
      "Requirement already satisfied: scipy in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (1.10.0)\n",
      "Collecting psutil>=5.8.0\n",
      "  Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB)\n",
      "Requirement already satisfied: numpy in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (1.24.2)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: pyparsing in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.2-cp310-cp310-macosx_10_9_universal2.whl (17 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, psutil, MarkupSafe, joblib, scikit-learn, jinja2, torch_geometric\n",
      "Successfully installed MarkupSafe-2.1.2 jinja2-3.1.2 joblib-1.2.0 psutil-5.9.4 scikit-learn-1.2.1 threadpoolctl-3.1.0 torch_geometric-2.2.0\n",
      "Collecting torch\n",
      "  Using cached torch-1.13.1-cp310-none-macosx_11_0_arm64.whl (53.2 MB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "Successfully installed torch-1.13.1 typing-extensions-4.5.0\n",
      "Collecting torch_sparse\n",
      "  Using cached torch_sparse-0.6.16-cp310-cp310-macosx_11_0_arm64.whl\n",
      "Requirement already satisfied: scipy in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_sparse) (1.10.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/varun/miniconda3/lib/python3.10/site-packages (from scipy->torch_sparse) (1.24.2)\n",
      "Installing collected packages: torch_sparse\n",
      "Successfully installed torch_sparse-0.6.16\n",
      "Collecting torch_scatter\n",
      "  Using cached torch_scatter-2.1.0.tar.gz (106 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: torch_scatter\n",
      "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l-^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install torch\n",
    "!pip install torch_sparse\n",
    "!pip install torch_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.models.lightgcn import LightGCN\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use the first 10k rows of the data, set to None to use all data\n",
    "AMOUNT_TO_LOAD = 100\n",
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/63111 [00:00<01:17, 810.19it/s]\n"
     ]
    }
   ],
   "source": [
    "user_reviews_dir = 'user_reviews'\n",
    "user_review_data = dict()\n",
    "\n",
    "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
    "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
    "        break\n",
    "    try:\n",
    "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Empty file: {filename}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
    "\n",
    "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asel82_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(list(user_review_data.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 948.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518 2\n",
      "Train reviews: 45290\n",
      "Validation reviews: 96\n",
      "Test reviews: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = []\n",
    "validation_reviews = []\n",
    "test_reviews = []\n",
    "for user_id, reviews in tqdm(user_review_data.items()):\n",
    "    if len(reviews) > 5:\n",
    "        # randomly remove one review from the user's reviews for the test set and one for the validation set\n",
    "        reviews_to_remove = reviews.sample(2)\n",
    "        # test data\n",
    "        test_review_data = reviews_to_remove.iloc[0].to_dict()\n",
    "        test_review_data['user_id'] = user_id\n",
    "        test_reviews.append(test_review_data)\n",
    "        # validation data\n",
    "        validation_review_data = reviews_to_remove.iloc[1].to_dict()\n",
    "        validation_review_data['user_id'] = user_id\n",
    "        validation_reviews.append(validation_review_data)\n",
    "        # train data\n",
    "        train_review_data = reviews.drop(reviews_to_remove.index).to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        \n",
    "        if (user_id == \"ahmad97_reviews.csv\"):\n",
    "          print(len(train_review_data), len(reviews_to_remove))\n",
    "        train_reviews.extend(train_review_data)\n",
    "    else:\n",
    "        # if the user has less than 5 reviews, we will use all of them for training\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "\n",
    "print(f'Train reviews: {len(train_reviews)}')\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_title': 'All Too Well: The Short Film',\n",
       " 'movie_rating': 4.0,\n",
       " 'movie_id': 807762,\n",
       " 'film_slug': '/film/all-too-well-the-short-film/',\n",
       " 'user_id': 'asel82_reviews.csv'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Now that we have the training data, let's construct the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train users: 100\n",
      "Number of train items: 12618\n",
      "Number of nodes: 12718\n"
     ]
    }
   ],
   "source": [
    "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
    "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
    "num_nodes = num_train_users + num_train_items\n",
    "print(f'Number of train users: {num_train_users}')\n",
    "print(f'Number of train items: {num_train_items}')\n",
    "print(f'Number of nodes: {num_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map users to ids\n",
    "movie_id_to_movie_name = dict()\n",
    "for review in train_reviews:\n",
    "    movie_id_to_movie_name[review['movie_id']] = review['movie_title']\n",
    "\n",
    "user_to_id = dict()\n",
    "for i, user_id in enumerate(set([review['user_id'] for review in train_reviews])):\n",
    "    user_to_id[user_id] = i\n",
    "\n",
    "# Let's map movies to ids\n",
    "movie_to_id = dict()\n",
    "for i, movie_id in enumerate(set([review['movie_id'] for review in train_reviews])):\n",
    "    movie_to_id[movie_id] = i + num_train_users\n",
    "\n",
    "# Let's map ids to users\n",
    "id_to_user = dict()\n",
    "for user_id, index in user_to_id.items():\n",
    "    id_to_user[index] = user_id\n",
    "\n",
    "# Let's map ids to movies\n",
    "id_to_movie = dict()\n",
    "for movie_id, index in movie_to_id.items():\n",
    "    id_to_movie[index] = movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation reviews: 96\n",
      "Test reviews: 96\n",
      "Validation reviews: 88\n",
      "Test reviews: 83\n"
     ]
    }
   ],
   "source": [
    "# Let's remove any data in our validation and test sets that have ids that are not in our training set\n",
    "# Before removal:\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')\n",
    "\n",
    "# Removal\n",
    "validation_reviews = [review for review in validation_reviews if review['user_id'] in user_to_id and review['movie_id'] in movie_to_id]\n",
    "test_reviews = [review for review in test_reviews if review['user_id'] in user_to_id and review['movie_id'] in movie_to_id]\n",
    "\n",
    "# After removal:\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert_review_to_edge(review):\n",
    "    user_id = user_to_id[review['user_id']]\n",
    "    movie_id = movie_to_id[review['movie_id']]\n",
    "    edge_weight = review['movie_rating']\n",
    "    if (edge_weight < 3.5 and edge_weight > 2.5):\n",
    "        return None, None\n",
    "    edge = (user_id, movie_id)\n",
    "    edge_weight = review['movie_rating']\n",
    "    return edge, edge_weight\n",
    "\n",
    "def shuffle_edges_and_edge_weights(edges, edge_weights):\n",
    "    c = list(zip(edges, edge_weights))\n",
    "    random.shuffle(c)\n",
    "    return zip(*c)\n",
    "\n",
    "def convert_reviews_to_edges(reviews):\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    for review in tqdm(reviews):\n",
    "        edge, edge_weight = convert_review_to_edge(review)\n",
    "        if edge is not None:\n",
    "            edges.append(edge)\n",
    "            edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Reformat the edges to be a tensor\n",
    "    edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edges, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45290/45290 [00:00<00:00, 2479442.77it/s]\n",
      "100%|██████████| 88/88 [00:00<00:00, 792057.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 38675\n",
      "Validation edges: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the edges between users and movies.\n",
    "# The id of the user will be the index of the user in the user_to_id dict\n",
    "# The id of the movie will be the index of the movie in the movie_to_id dict + the number of users\n",
    "\n",
    "train_edges, train_edge_weights = convert_reviews_to_edges(train_reviews)\n",
    "validation_edges, validation_edge_weights = convert_reviews_to_edges(validation_reviews)\n",
    "\n",
    "print(f'Train edges: {train_edges.shape[1]}')\n",
    "print(f'Validation edges: {validation_edges.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as data\n",
    "\n",
    "# create the graph\n",
    "train_graph = data.Data(\n",
    "    edge_index=train_edges,\n",
    "    edge_attr=torch.tensor(train_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "validation_graph = data.Data(\n",
    "    edge_index=validation_edges,\n",
    "    edge_attr=torch.tensor(validation_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.validate(raise_on_error=True)\n",
    "validation_graph.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_edges(positive_edges, negative_edges):\n",
    "    # For every user, determine add fake negative edges until we have the same number of positive edges\n",
    "    # We will do this by randomly creating negative edges for each user\n",
    "    additional_negative_edges = []\n",
    "    for user_id in range(num_train_users):\n",
    "        # Get the positive edges for this user\n",
    "        user_positive_edges = positive_edges[:, positive_edges[0] == user_id]\n",
    "        # Get the negative edges for this user\n",
    "        user_negative_edges = negative_edges[:, negative_edges[0] == user_id]\n",
    "        # Determine how many negative edges we need to add\n",
    "        num_negative_edges_to_add = user_positive_edges.shape[1] - user_negative_edges.shape[1]\n",
    "        if (num_negative_edges_to_add <= 0):\n",
    "            num_negative_edges_to_remove = -num_negative_edges_to_add\n",
    "            # choose the negative edges to keep\n",
    "            negative_edges_to_keep = torch.randint(user_negative_edges.shape[1], (user_negative_edges.shape[1] - num_negative_edges_to_remove,))\n",
    "            # remove all the negative edges for this user\n",
    "            negative_edges = negative_edges[:, negative_edges[0] != user_id]\n",
    "            # add the negative edges to keep back to the negative edges\n",
    "            negative_edges = torch.cat([negative_edges, user_negative_edges[:, negative_edges_to_keep]], dim=1)\n",
    "        else:\n",
    "            # Create the negative edges\n",
    "            negative_edges_to_add = torch.tensor([[user_id] * num_negative_edges_to_add, torch.randint(num_train_users, num_train_items, (num_negative_edges_to_add,))], dtype=torch.long)\n",
    "            # Add the negative edges to the list of additional negative edges\n",
    "            additional_negative_edges.append(negative_edges_to_add)\n",
    "    # Concatenate the additional negative edges\n",
    "    additional_negative_edges = torch.cat(additional_negative_edges, dim=1)\n",
    "    # Concatenate the additional negative edges with the existing negative edges\n",
    "    negative_edges = torch.cat([negative_edges, additional_negative_edges], dim=1)\n",
    "    return positive_edges, negative_edges\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_at_k_memory_efficient(model, num_items, num_users, positive_edges, k=5):\n",
    "    model.eval()\n",
    "    # select a random subset of 1000 users\n",
    "    users = torch.randperm(num_users)[:1000]\n",
    "    # for each user, use a heapq to keep track of the top k items\n",
    "    top_k_items = [list() for _ in range(1000)]\n",
    "    print(\"Created top k items\")\n",
    "    with torch.no_grad():\n",
    "        # we're going to go over all possible item, user pairs, but we're going to do it in batches\n",
    "        for user_id in tqdm(users):\n",
    "            all_edges_for_user = torch.tensor([(user_id, item_id) for item_id in range(num_items)], dtype=torch.long).t().contiguous()\n",
    "            top_k_items[user_id] = model(all_edges_for_user).topk(k=k, dim=0)[1].tolist()\n",
    "        # Check how many of the top k items are in the positive edges\n",
    "        num_correct = 0\n",
    "        for user_id, positive_items in enumerate(positive_edges):\n",
    "            # increment num_correct if the edge (user_id, item_id) is in the positive edges\n",
    "            num_correct += len(set(top_k_items[user_id]) & set(positive_items.tolist()))\n",
    "        precision_at_k = num_correct / (num_users * k)\n",
    "    return precision_at_k\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put this on tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, model, train_positive_edges, train_negative_edges, num_users):\n",
    "        loss = torch.tensor(0.0, requires_grad=True)\n",
    "        for user_id in range(num_users):\n",
    "            # get all the positive and negative edges for this user\n",
    "            user_positive_edges = train_positive_edges[:, train_positive_edges[0] == user_id]\n",
    "            user_negative_edges = train_negative_edges[:, train_negative_edges[0] == user_id]\n",
    "            if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
    "                continue\n",
    "            # compute the embeddings for all the positive and negative edges\n",
    "            positive_edge_embeddings = model.get_embedding(user_positive_edges)\n",
    "            negative_edge_embeddings = model.get_embedding(user_negative_edges)\n",
    "            # compute the pairwise differences\n",
    "            pairwise_differences = positive_edge_embeddings.unsqueeze(1) - negative_edge_embeddings\n",
    "            # for each pairwise difference we want -log(sigmoid(x))\n",
    "            # we can do this by doing log(1 + exp(-x))\n",
    "            user_loss = torch.log(1 + torch.exp(-pairwise_differences)).sum() / (user_positive_edges.shape[1] * user_negative_edges.shape[1])\n",
    "            loss = loss + user_loss\n",
    "        loss = loss / num_users\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 5294.31494140625\n",
      "Epoch: 0, Batch: 1, Loss: 0.0\n",
      "Epoch: 0, Batch: 2, Loss: 0.0\n",
      "Epoch: 0, Batch: 3, Loss: 0.0\n",
      "Epoch: 0, Batch: 4, Loss: 0.0\n",
      "Epoch: 0, Batch: 5, Loss: 0.0\n",
      "Epoch: 0, Batch: 6, Loss: 0.0\n",
      "Epoch: 0, Batch: 7, Loss: 0.0\n",
      "Epoch: 0, Batch: 8, Loss: 0.0\n",
      "Epoch: 0, Batch: 9, Loss: 0.0\n",
      "Epoch: 0, Batch: 10, Loss: 0.0\n",
      "Epoch: 0, Batch: 11, Loss: 0.0\n",
      "Epoch: 0, Batch: 12, Loss: 0.0\n",
      "Epoch: 0, Batch: 13, Loss: 0.0\n",
      "Epoch: 0, Batch: 14, Loss: 0.0\n",
      "Epoch: 0, Batch: 15, Loss: 0.0\n",
      "Epoch: 0, Batch: 16, Loss: 0.0\n",
      "Epoch: 0, Batch: 17, Loss: 0.0\n",
      "Epoch: 0, Batch: 18, Loss: 0.0\n",
      "Epoch: 0, Batch: 19, Loss: 0.0\n",
      "Epoch: 0, Batch: 20, Loss: 0.0\n",
      "Epoch: 0, Batch: 21, Loss: 0.0\n",
      "Epoch: 0, Batch: 22, Loss: 0.0\n",
      "Epoch: 0, Batch: 23, Loss: 0.0\n",
      "Epoch: 0, Batch: 24, Loss: 0.0\n",
      "Epoch: 0, Batch: 25, Loss: 0.0\n",
      "Epoch: 0, Batch: 26, Loss: 0.0\n",
      "Epoch: 0, Batch: 27, Loss: 0.0\n",
      "Epoch: 0, Batch: 28, Loss: 0.0\n",
      "Epoch: 0, Batch: 29, Loss: 0.0\n",
      "Epoch: 0, Batch: 30, Loss: 0.0\n",
      "Epoch: 0, Batch: 31, Loss: 0.0\n",
      "Epoch: 0, Batch: 32, Loss: 0.0\n",
      "Epoch: 0, Batch: 33, Loss: 0.0\n",
      "Epoch: 0, Batch: 34, Loss: 0.0\n",
      "Epoch: 0, Batch: 35, Loss: 0.0\n",
      "Epoch: 0, Batch: 36, Loss: 0.0\n",
      "Epoch: 0, Batch: 37, Loss: 0.0\n",
      "Epoch: 0, Batch: 38, Loss: 0.0\n",
      "Epoch: 0, Batch: 39, Loss: 0.0\n",
      "Epoch: 0, Batch: 40, Loss: 0.0\n",
      "Epoch: 0, Batch: 41, Loss: 0.0\n",
      "Epoch: 0, Batch: 42, Loss: 0.0\n",
      "Epoch: 0, Batch: 43, Loss: 0.0\n",
      "Epoch: 0, Batch: 44, Loss: 0.0\n",
      "Epoch: 0, Batch: 45, Loss: 0.0\n",
      "Epoch: 0, Batch: 46, Loss: 0.0\n",
      "Epoch: 0, Batch: 47, Loss: 0.0\n",
      "Epoch: 0, Batch: 48, Loss: 0.0\n",
      "Epoch: 0, Batch: 49, Loss: 0.0\n",
      "Epoch: 0, Batch: 50, Loss: 0.0\n",
      "Epoch: 0, Batch: 51, Loss: 0.0\n",
      "Epoch: 0, Batch: 52, Loss: 0.0\n",
      "Epoch: 0, Batch: 53, Loss: 0.0\n",
      "Epoch: 0, Batch: 54, Loss: 0.0\n",
      "Epoch: 0, Batch: 55, Loss: 0.0\n",
      "Epoch: 0, Batch: 56, Loss: 0.0\n",
      "Epoch: 0, Batch: 57, Loss: 0.0\n",
      "Epoch: 0, Batch: 58, Loss: 0.0\n",
      "Epoch: 0, Batch: 59, Loss: 0.0\n",
      "Epoch: 0, Batch: 60, Loss: 0.0\n",
      "Epoch: 0, Batch: 61, Loss: 0.0\n",
      "Epoch: 0, Batch: 62, Loss: 0.0\n",
      "Epoch: 0, Batch: 63, Loss: 0.0\n",
      "Epoch: 0, Batch: 64, Loss: 0.0\n",
      "Epoch: 0, Batch: 65, Loss: 0.0\n",
      "Epoch: 0, Batch: 66, Loss: 0.0\n",
      "Epoch: 0, Batch: 67, Loss: 0.0\n",
      "Epoch: 0, Batch: 68, Loss: 0.0\n",
      "Epoch: 0, Batch: 69, Loss: 0.0\n",
      "Epoch: 0, Batch: 70, Loss: 0.0\n",
      "Epoch: 0, Batch: 71, Loss: 0.0\n",
      "Epoch: 0, Batch: 72, Loss: 0.0\n",
      "Epoch: 0, Batch: 73, Loss: 0.0\n",
      "Epoch: 0, Batch: 74, Loss: 0.0\n",
      "Epoch: 0, Batch: 75, Loss: 0.0\n",
      "Epoch: 0, Batch: 76, Loss: 0.0\n",
      "Epoch: 0, Batch: 77, Loss: 0.0\n",
      "Epoch: 0, Batch: 78, Loss: 0.0\n",
      "Epoch: 0, Batch: 79, Loss: 0.0\n",
      "Epoch: 0, Batch: 80, Loss: 0.0\n",
      "Epoch: 0, Batch: 81, Loss: 0.0\n",
      "Epoch: 0, Batch: 82, Loss: 0.0\n",
      "Epoch: 0, Batch: 83, Loss: 0.0\n",
      "Epoch: 0, Batch: 84, Loss: 0.0\n",
      "Epoch: 0, Batch: 85, Loss: 0.0\n",
      "Epoch: 0, Batch: 86, Loss: 0.0\n",
      "Epoch: 0, Batch: 87, Loss: 0.0\n",
      "Epoch: 0, Batch: 88, Loss: 0.0\n",
      "Epoch: 0, Batch: 89, Loss: 0.0\n",
      "Epoch: 0, Batch: 90, Loss: 0.0\n",
      "Epoch: 0, Batch: 91, Loss: 0.0\n",
      "Epoch: 0, Batch: 92, Loss: 0.0\n",
      "Epoch: 0, Batch: 93, Loss: 0.0\n",
      "Epoch: 0, Batch: 94, Loss: 0.0\n",
      "Epoch: 0, Batch: 95, Loss: 0.0\n",
      "Epoch: 0, Batch: 96, Loss: 0.0\n",
      "Epoch: 0, Batch: 97, Loss: 0.0\n",
      "Epoch: 0, Batch: 98, Loss: 0.0\n",
      "Epoch: 0, Batch: 99, Loss: 0.0\n",
      "Epoch: 0, Batch: 100, Loss: 0.0\n",
      "Epoch: 0, Batch: 101, Loss: 0.0\n",
      "Epoch: 0, Batch: 102, Loss: 0.0\n",
      "Epoch: 0, Batch: 103, Loss: 0.0\n",
      "Epoch: 0, Batch: 104, Loss: 0.0\n",
      "Epoch: 0, Batch: 105, Loss: 0.0\n",
      "Epoch: 0, Batch: 106, Loss: 0.0\n",
      "Epoch: 0, Batch: 107, Loss: 0.0\n",
      "Epoch: 0, Batch: 108, Loss: 0.0\n",
      "Epoch: 0, Batch: 109, Loss: 8759.228515625\n",
      "Epoch: 0, Batch: 110, Loss: 273.7230529785156\n",
      "Epoch: 0, Batch: 111, Loss: 273.7209167480469\n",
      "Epoch: 0, Batch: 112, Loss: 273.719482421875\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model = LightGCN(num_nodes=num_nodes, embedding_dim=EMBEDDING_DIM, num_layers=3)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = BPRLoss()\n",
    "\n",
    "train_positive_edges = train_graph.edge_index[:, train_graph.edge_attr >= 3.5]\n",
    "train_negative_edges = train_graph.edge_index[:, train_graph.edge_attr <= 2.5]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_positive_edges, train_negative_edges = resample_edges(train_positive_edges, train_negative_edges)\n",
    "\n",
    "for epoch in tqdm(range(1000)):\n",
    "    model.train()\n",
    "\n",
    "    num_batches = train_positive_edges.shape[1] // BATCH_SIZE\n",
    "    for i in range(0, train_positive_edges.shape[1], BATCH_SIZE):\n",
    "        positive_edges = train_positive_edges[:, i:i+BATCH_SIZE]\n",
    "        negative_edges = train_negative_edges[:, i:i+BATCH_SIZE]\n",
    "        positive_ranks = model(positive_edges)\n",
    "        negative_ranks = model(negative_edges)\n",
    "        train_loss = loss(model, positive_edges, negative_edges, num_train_users)\n",
    "        print(\"Epoch: {}, Batch: {}, Loss: {}\".format(epoch, i // BATCH_SIZE, train_loss))\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch * num_batches + i // BATCH_SIZE)\n",
    "        optim.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    if (epoch % 100 == 0):\n",
    "        # Let's print the top ranked movies for a few users\n",
    "        model.eval()\n",
    "        for user_id in range(10):\n",
    "            all_edges_for_user = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_nodes)], dtype=torch.long).t().contiguous()\n",
    "            top_k_items_and_scores = model(all_edges_for_user).topk(k=5, dim=0)\n",
    "            top_k_items = top_k_items_and_scores[1].tolist()\n",
    "            top_k_scores = top_k_items_and_scores[0].tolist()\n",
    "            top_k_movies = [id_to_movie[item_id] for item_id in top_k_items]\n",
    "            top_k_movie_names = [movie_id_to_movie_name[movie_id] for movie_id in top_k_movies]\n",
    "            print(f'User: {id_to_user[user_id]}, Top 5 items: {top_k_movie_names}, Top 5 scores: {top_k_scores}')\n",
    "\n",
    "        # precision_at_k = compute_precision_at_k_memory_efficient(model, num_train_items, num_train_users, train_positive_edges)\n",
    "        # print(f'Epoch: {epoch}, Precision at k: {precision_at_k}')\n",
    "        # writer.add_scalar('Precision at k', precision_at_k, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
