{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (4.64.1)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.1-cp310-cp310-macosx_12_0_arm64.whl (8.4 MB)\n",
      "Requirement already satisfied: requests in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (2.28.1)\n",
      "Requirement already satisfied: scipy in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (1.10.0)\n",
      "Collecting psutil>=5.8.0\n",
      "  Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB)\n",
      "Requirement already satisfied: numpy in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (1.24.2)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: pyparsing in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.2-cp310-cp310-macosx_10_9_universal2.whl (17 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/varun/miniconda3/lib/python3.10/site-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, psutil, MarkupSafe, joblib, scikit-learn, jinja2, torch_geometric\n",
      "Successfully installed MarkupSafe-2.1.2 jinja2-3.1.2 joblib-1.2.0 psutil-5.9.4 scikit-learn-1.2.1 threadpoolctl-3.1.0 torch_geometric-2.2.0\n",
      "Collecting torch\n",
      "  Using cached torch-1.13.1-cp310-none-macosx_11_0_arm64.whl (53.2 MB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "Successfully installed torch-1.13.1 typing-extensions-4.5.0\n",
      "Collecting torch_sparse\n",
      "  Using cached torch_sparse-0.6.16-cp310-cp310-macosx_11_0_arm64.whl\n",
      "Requirement already satisfied: scipy in /Users/varun/miniconda3/lib/python3.10/site-packages (from torch_sparse) (1.10.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/varun/miniconda3/lib/python3.10/site-packages (from scipy->torch_sparse) (1.24.2)\n",
      "Installing collected packages: torch_sparse\n",
      "Successfully installed torch_sparse-0.6.16\n",
      "Collecting torch_scatter\n",
      "  Using cached torch_scatter-2.1.0.tar.gz (106 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: torch_scatter\n",
      "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l-^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install torch\n",
    "!pip install torch_sparse\n",
    "!pip install torch_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.models.lightgcn import LightGCN\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use the first 10k rows of the data, set to None to use all data\n",
    "AMOUNT_TO_LOAD = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: sebastian823_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "user_reviews_dir = 'user_reviews'\n",
    "user_review_data = dict()\n",
    "\n",
    "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
    "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
    "        break\n",
    "    try:\n",
    "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Empty file: {filename}')\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
    "\n",
    "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asel82_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(list(user_review_data.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 884.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 5322200\n",
      "Validation reviews: 9857\n",
      "Test reviews: 9857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = []\n",
    "validation_reviews = []\n",
    "test_reviews = []\n",
    "for user_id, reviews in tqdm(user_review_data.items()):\n",
    "    if len(reviews) > 5:\n",
    "        # randomly remove one review from the user's reviews for the test set and one for the validation set\n",
    "        reviews_to_remove = reviews.sample(2)\n",
    "        # test data\n",
    "        test_review_data = reviews_to_remove.iloc[0].to_dict()\n",
    "        test_review_data['user_id'] = user_id\n",
    "        test_reviews.append(test_review_data)\n",
    "        # validation data\n",
    "        validation_review_data = reviews_to_remove.iloc[1].to_dict()\n",
    "        validation_review_data['user_id'] = user_id\n",
    "        validation_reviews.append(validation_review_data)\n",
    "        # train data\n",
    "        train_review_data = reviews.drop(reviews_to_remove.index).to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "    else:\n",
    "        # if the user has less than 5 reviews, we will use all of them for training\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "\n",
    "print(f'Train reviews: {len(train_reviews)}')\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_title': 'All Too Well: The Short Film',\n",
       " 'movie_rating': 4.0,\n",
       " 'movie_id': 807762,\n",
       " 'film_slug': '/film/all-too-well-the-short-film/',\n",
       " 'user_id': 'asel82_reviews.csv'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Now that we have the training data, let's construct the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train users: 10000\n",
      "Number of train items: 159981\n",
      "Number of nodes: 169981\n"
     ]
    }
   ],
   "source": [
    "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
    "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
    "num_nodes = num_train_users + num_train_items\n",
    "print(f'Number of train users: {num_train_users}')\n",
    "print(f'Number of train items: {num_train_items}')\n",
    "print(f'Number of nodes: {num_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightGCN(\n",
    "    num_nodes=num_nodes,\n",
    "    embedding_dim=64,\n",
    "    num_layers=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding\n",
    "# Let's create two embedding models, one for users and one for items\n",
    "user_embedding = Embedding(num_embeddings=num_train_users, embedding_dim=64)\n",
    "item_embedding = Embedding(num_embeddings=num_train_items, embedding_dim=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
