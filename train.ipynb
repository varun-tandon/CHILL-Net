{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.0.0%2Bcu117.html\n",
      "Requirement already satisfied: torch-sparse in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (0.6.17+pt20cu117)\n",
      "Collecting torch-scatter\n",
      "  Using cached https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_scatter-2.1.1%2Bpt20cu117-cp38-cp38-win_amd64.whl (3.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from torch-sparse) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from scipy->torch-sparse) (1.23.5)\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.1+pt20cu117\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-sparse torch-scatter -f https://data.pyg.org/whl/torch-2.0.0%2Bcu117.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (3.7.1)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (4.39.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from tensorboard) (0.38.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from tensorboard) (65.6.3)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.51.3-cp38-cp38-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 12.0 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "     ---------------------------------------- 177.2/177.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from tensorboard) (2.28.1)\n",
      "Collecting protobuf>=3.19.6\n",
      "  Downloading protobuf-4.22.1-cp38-cp38-win_amd64.whl (420 kB)\n",
      "     ------------------------------------- 420.6/420.6 kB 13.2 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from markdown>=2.6.8->tensorboard) (6.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\varun\\anaconda3\\envs\\letter\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, werkzeug, tensorboard-data-server, rsa, pyasn1-modules, protobuf, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard\n",
      "Successfully installed absl-py-1.4.0 cachetools-5.3.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 grpcio-1.51.3 markdown-3.4.1 oauthlib-3.2.2 protobuf-4.22.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 werkzeug-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models.lightgcn import LightGCN\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We can begin by loading in the user review data. For each user, we have a subset of the movies that they reviewed. We'll load each of the CSVs as dataframes, and store a dict of user IDs corresponding to their dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use the first 10k rows of the data, set to None to use all data\n",
    "AMOUNT_TO_LOAD = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 334/63111 [00:02<05:57, 175.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: 468889434_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2654/63111 [00:15<05:25, 185.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file: alinetta_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5002/63111 [00:27<05:21, 180.96it/s]\n"
     ]
    }
   ],
   "source": [
    "user_reviews_dir = 'user_reviews'\n",
    "user_review_data = dict()\n",
    "\n",
    "for filename in tqdm(os.listdir(user_reviews_dir)):\n",
    "    if AMOUNT_TO_LOAD is not None and len(user_review_data) >= AMOUNT_TO_LOAD:\n",
    "        break\n",
    "    try:\n",
    "        user_review_data[filename] = pd.read_csv(os.path.join(user_reviews_dir, filename), encoding='unicode_escape')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Empty file: {filename}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation, and test sets. Since this is a recommender, we're gonna split by removing some of the user's reviews.\n",
    "\n",
    "For every user, so long as the user has more than 5 reviews, remove one review for the validation set and one review for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001kidd_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(list(user_review_data.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:04<00:00, 1179.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove all values with nan in the review column\n",
    "for key in tqdm(user_review_data.keys()):\n",
    "    user_review_data[key] = user_review_data[key].dropna(subset=['movie_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:13<00:00, 378.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 1748693\n",
      "Validation reviews: 141560\n",
      "Test reviews: 70780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = []\n",
    "validation_reviews = []\n",
    "test_reviews = []\n",
    "for user_id, reviews in tqdm(user_review_data.items()):\n",
    "    if len(reviews) > 80:\n",
    "        validation_review_data_df = reviews.sample(40, replace=False)\n",
    "        validation_review_data = validation_review_data_df.to_dict('records')\n",
    "        for review in validation_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        validation_reviews.extend(validation_review_data)\n",
    "        # remove the validation reviews from the training data\n",
    "        reviews = reviews.drop(validation_review_data_df.index)\n",
    "        test_review_data_df = reviews.sample(20, replace=False)\n",
    "        test_review_data = test_review_data_df.to_dict('records')\n",
    "        for review in test_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        test_reviews.extend(test_review_data)\n",
    "        # remove the test reviews from the training data\n",
    "        reviews = reviews.drop(test_review_data_df.index)\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "    else:\n",
    "        # if the user has less than 5 reviews, we will use all of them for training\n",
    "        train_review_data = reviews.to_dict('records')\n",
    "        for review in train_review_data:\n",
    "            review['user_id'] = user_id\n",
    "        train_reviews.extend(train_review_data)\n",
    "\n",
    "print(f'Train reviews: {len(train_reviews)}')\n",
    "print(f'Validation reviews: {len(validation_reviews)}')\n",
    "print(f'Test reviews: {len(test_reviews)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Now that we have the training data, let's construct the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train users: 5000\n",
      "Number of train items: 87735\n",
      "Number of nodes: 95299\n"
     ]
    }
   ],
   "source": [
    "num_train_users = len(set([review['user_id'] for review in train_reviews]))\n",
    "num_train_items = len(set([review['movie_id'] for review in train_reviews]))\n",
    "num_total_items = len(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews]))\n",
    "num_nodes = num_train_users + num_total_items\n",
    "print(f'Number of train users: {num_train_users}')\n",
    "print(f'Number of train items: {num_train_items}')\n",
    "print(f'Number of nodes: {num_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val_users = len(set([review['user_id'] for review in validation_reviews]))\n",
    "num_val_items = len(set([review['movie_id'] for review in validation_reviews]))\n",
    "num_val_nodes = num_val_users + num_val_items\n",
    "num_test_users = len(set([review['user_id'] for review in test_reviews]))\n",
    "num_test_items = len(set([review['movie_id'] for review in test_reviews]))\n",
    "num_test_nodes = num_test_users + num_test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map users to ids\n",
    "movie_id_to_movie_name = dict()\n",
    "for review in train_reviews + validation_reviews + test_reviews:\n",
    "    movie_id_to_movie_name[review['movie_id']] = review['movie_title']\n",
    "\n",
    "user_to_id = dict()\n",
    "for i, user_id in enumerate(set([review['user_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
    "    user_to_id[user_id] = i\n",
    "\n",
    "# Let's map movies to ids\n",
    "movie_to_id = dict()\n",
    "for i, movie_id in enumerate(set([review['movie_id'] for review in train_reviews + validation_reviews + test_reviews])):\n",
    "    movie_to_id[movie_id] = i + num_train_users\n",
    "\n",
    "# Let's map ids to users\n",
    "id_to_user = dict()\n",
    "for user_id, index in user_to_id.items():\n",
    "    id_to_user[index] = user_id\n",
    "\n",
    "# Let's map ids to movies\n",
    "id_to_movie = dict()\n",
    "for movie_id, index in movie_to_id.items():\n",
    "    id_to_movie[index] = movie_id\n",
    "\n",
    "# Let's map movie names to movie ids\n",
    "movie_name_to_movie_id = dict()\n",
    "for movie_id, movie_name in movie_id_to_movie_name.items():\n",
    "    movie_name_to_movie_id[movie_name] = movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert_review_to_edge(review):\n",
    "    user_id = user_to_id[review['user_id']]\n",
    "    movie_id = movie_to_id[review['movie_id']]\n",
    "    edge_weight = review['movie_rating']\n",
    "    if (edge_weight < 3.5 and edge_weight > 2.5):\n",
    "        return None, None\n",
    "    edge = (user_id, movie_id)\n",
    "    edge_weight = review['movie_rating']\n",
    "    return edge, edge_weight\n",
    "\n",
    "def convert_reviews_to_edges(reviews):\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    for review in tqdm(reviews):\n",
    "        edge, edge_weight = convert_review_to_edge(review)\n",
    "        if edge is not None:\n",
    "            edges.append(edge)\n",
    "            edge_weights.append(edge_weight)\n",
    "    \n",
    "    # Reformat the edges to be a tensor\n",
    "    edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edges, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1748693/1748693 [00:01<00:00, 1106817.88it/s]\n",
      "100%|██████████| 141560/141560 [00:00<00:00, 1047565.88it/s]\n",
      "100%|██████████| 70780/70780 [00:00<00:00, 994955.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 1407492\n",
      "Validation edges: 116433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the edges between users and movies.\n",
    "# The id of the user will be the index of the user in the user_to_id dict\n",
    "# The id of the movie will be the index of the movie in the movie_to_id dict + the number of users\n",
    "\n",
    "train_edges, train_edge_weights = convert_reviews_to_edges(train_reviews)\n",
    "validation_edges, validation_edge_weights = convert_reviews_to_edges(validation_reviews)\n",
    "test_edges, test_edge_weights = convert_reviews_to_edges(test_reviews)\n",
    "\n",
    "print(f'Train edges: {train_edges.shape[1]}')\n",
    "print(f'Validation edges: {validation_edges.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as data\n",
    "\n",
    "# create the graph\n",
    "train_graph = data.Data(\n",
    "    edge_index=train_edges,\n",
    "    edge_attr=torch.tensor(train_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "validation_graph = data.Data(\n",
    "    edge_index=validation_edges,\n",
    "    edge_attr=torch.tensor(validation_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "test_graph = data.Data(\n",
    "    edge_index=test_edges,\n",
    "    edge_attr=torch.tensor(test_edge_weights),\n",
    "    num_nodes=num_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.validate(raise_on_error=True)\n",
    "validation_graph.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some negative edges\n",
    "def resample_edges_for_user(user_positive_edges, user_negative_edges):\n",
    "    num_negative_edges_to_add = user_positive_edges.shape[1] * 3 - user_negative_edges.shape[1]\n",
    "    if (num_negative_edges_to_add <= 0):\n",
    "        num_negative_edges_to_remove = -num_negative_edges_to_add\n",
    "        # choose the negative edges to keep\n",
    "        negative_edges_to_keep = torch.randint(user_negative_edges.shape[1], (user_negative_edges.shape[1] - num_negative_edges_to_remove,))\n",
    "        # remove all the negative edges for this user\n",
    "        user_negative_edges = user_negative_edges[:, negative_edges_to_keep]\n",
    "    else:\n",
    "        # Create new negative edges\n",
    "        negative_edges_to_add = torch.tensor([[user_id] * num_negative_edges_to_add, torch.randint(num_train_users, num_train_items, (num_negative_edges_to_add,))], dtype=torch.long)\n",
    "        # Add the negative edges to the negative edges for this user\n",
    "        user_negative_edges = torch.cat([user_negative_edges, negative_edges_to_add], dim=1)\n",
    "    return user_positive_edges, user_negative_edges\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute ndcg\n",
    "def compute_ndcg_at_k(relevances, k=5):\n",
    "    dcg = 0\n",
    "    for i, relevance in enumerate(relevances):\n",
    "        if i == k:\n",
    "            break\n",
    "        dcg += (relevance) / np.log2(i + 2)\n",
    "    idcg = 0\n",
    "    for i, relevance in enumerate(sorted(relevances, reverse=True)):\n",
    "        if i == k:\n",
    "            break\n",
    "        idcg += (relevance) / np.log2(i + 2)\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def compute_recall_at_k(validation_graph, model, K):\n",
    "    # get positive edges in validation set\n",
    "    positive_edges = validation_graph.edge_index[:, validation_graph.edge_attr > 3.5]\n",
    "\n",
    "    # map users to positive edges\n",
    "    user_pos_items = get_user_positive_items(positive_edges)\n",
    "\n",
    "    # get users\n",
    "    users = positive_edges[0].unique()\n",
    "\n",
    "    users = users[torch.randint(users.shape[0], (min(200, len(users)),))]\n",
    "    # filter the validation edges to only the users we want to evaluate\n",
    "    user_validation_edges = []\n",
    "    for user in users:\n",
    "        user_validation_edges.append(validation_graph.edge_index[:, validation_graph.edge_index[0] == user])\n",
    "    user_validation_edges = torch.cat(user_validation_edges, dim=1)\n",
    "    print(user_validation_edges.shape)\n",
    "\n",
    "    first_user_id = users[0].item()\n",
    "    user_name = id_to_user[first_user_id]\n",
    "    print(f'User: {user_name}')\n",
    "\n",
    "    # get movies\n",
    "    movie_indices = torch.LongTensor([_ for _ in range(len(users) + 1, validation_graph.num_nodes)]).to(device)\n",
    "\n",
    "    # Get positive items for each user in validation set\n",
    "    truth_items = [set(user_pos_items[user.item()]) for user in users]\n",
    "\n",
    "    first_user_truth_items = truth_items[0]\n",
    "    first_user_truth_items = [id_to_movie[item] for item in first_user_truth_items]\n",
    "    first_user_truth_items = [movie_id_to_movie_name[item] for item in first_user_truth_items]\n",
    "    print(first_user_truth_items)\n",
    "\n",
    "    training_edges = train_graph.edge_index\n",
    "\n",
    "    # Get top-K recommended items for each user in validation set\n",
    "    total_recall = 0\n",
    "    print(\"Computing recommendations for {} users\".format(len(users)))\n",
    "    for user_index, user_id in tqdm(enumerate(users), total=len(users)):\n",
    "        tick = time.time()\n",
    "        all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "        recommendations = model.recommend(all_edges.to(device), src_index=torch.tensor([user_id]).to(device), dst_index=torch.tensor([x for x in range(num_train_users + 1, num_train_items)]).to(device), k=10 * K)[0]\n",
    "        tock = time.time()\n",
    "        train_edges_for_user = training_edges[:, training_edges[0] == user_id].to(device)\n",
    "        # remove all the recommendations that are in the training set\n",
    "        recommendations = recommendations[~torch.isin(recommendations, train_edges_for_user[1])][:K]\n",
    "        if (len(recommendations) < K):\n",
    "            print(\"Not enough recommendations for user {}\".format(user_id))\n",
    "            continue\n",
    "        if (user_id == first_user_id):\n",
    "            first_user_recommended_items = recommendations\n",
    "            first_user_recommended_items = [id_to_movie[item.item()] for item in first_user_recommended_items if item.item() > num_train_users]\n",
    "            first_user_recommended_items = [movie_id_to_movie_name[item] for item in first_user_recommended_items if item in movie_id_to_movie_name]\n",
    "            print(first_user_recommended_items)\n",
    "        # num_intersect = 0\n",
    "        truth_items_for_user = truth_items[user_index]\n",
    "        # for item in recommendations:\n",
    "        #     item = item.item()\n",
    "        #     if item in truth_items_for_user:\n",
    "        #         num_intersect += 1\n",
    "        # print(num_intersect)\n",
    "        num_intersect = len(set([item.item() for item in recommendations]).intersection(truth_items[user_index]))\n",
    "        recall = num_intersect / len(truth_items_for_user)\n",
    "        total_recall += recall\n",
    "    return total_recall / len(users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList, Linear\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv, GATv2Conv\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLightGCN(torch.nn.Module):\n",
    "    r\"\"\"The LightGCN model from the `\"LightGCN: Simplifying and Powering\n",
    "    Graph Convolution Network for Recommendation\"\n",
    "    <https://arxiv.org/abs/2002.02126>`_ paper.\n",
    "    :class:`~torch_geometric.nn.models.LightGCN` learns embeddings by linearly\n",
    "    propagating them on the underlying graph, and uses the weighted sum of the\n",
    "    embeddings learned at all layers as the final embedding\n",
    "    .. math::\n",
    "        \\textbf{x}_i = \\sum_{l=0}^{L} \\alpha_l \\textbf{x}^{(l)}_i,\n",
    "    where each layer's embedding is computed as\n",
    "    .. math::\n",
    "        \\mathbf{x}^{(l+1)}_i = \\sum_{j \\in \\mathcal{N}(i)}\n",
    "        \\frac{1}{\\sqrt{\\deg(i)\\deg(j)}}\\mathbf{x}^{(l)}_j.\n",
    "    Two prediction heads and training objectives are provided:\n",
    "    **link prediction** (via\n",
    "    :meth:`~torch_geometric.nn.models.LightGCN.link_pred_loss` and\n",
    "    :meth:`~torch_geometric.nn.models.LightGCN.predict_link`) and\n",
    "    **recommendation** (via\n",
    "    :meth:`~torch_geometric.nn.models.LightGCN.recommendation_loss` and\n",
    "    :meth:`~torch_geometric.nn.models.LightGCN.recommend`).\n",
    "    .. note::\n",
    "        Embeddings are propagated according to the graph connectivity specified\n",
    "        by :obj:`edge_index` while rankings or link probabilities are computed\n",
    "        according to the edges specified by :obj:`edge_label_index`.\n",
    "    Args:\n",
    "        num_nodes (int): The number of nodes in the graph.\n",
    "        embedding_dim (int): The dimensionality of node embeddings.\n",
    "        num_layers (int): The number of\n",
    "            :class:`~torch_geometric.nn.conv.LGConv` layers.\n",
    "        alpha (float or torch.Tensor, optional): The scalar or vector\n",
    "            specifying the re-weighting coefficients for aggregating the final\n",
    "            embedding. If set to :obj:`None`, the uniform initialization of\n",
    "            :obj:`1 / (num_layers + 1)` is used. (default: :obj:`None`)\n",
    "        **kwargs (optional): Additional arguments of the underlying\n",
    "            :class:`~torch_geometric.nn.conv.LGConv` layers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        embedding_dim: int,\n",
    "        num_layers: int,\n",
    "        alpha: Optional[Union[float, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if alpha is None:\n",
    "            alpha = 1. / (num_layers + 1)\n",
    "\n",
    "        if isinstance(alpha, Tensor):\n",
    "            assert alpha.size(0) == num_layers + 1\n",
    "        else:\n",
    "            alpha = torch.tensor([alpha] * (num_layers + 1))\n",
    "        self.register_buffer('alpha', alpha)\n",
    "\n",
    "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "        self.num_heads = 2\n",
    "        self.convs = ModuleList([GATv2Conv(embedding_dim, embedding_dim, heads=self.num_heads, dropout=0.5) for _ in range(num_layers)])\n",
    "        self.linears = ModuleList([Linear(embedding_dim * self.num_heads, embedding_dim) for _ in range(num_layers)])\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def get_embedding(self, edge_index: Adj) -> Tensor:\n",
    "        r\"\"\"Returns the embedding of nodes in the graph.\"\"\"\n",
    "        x = self.embedding.weight\n",
    "        out = x * self.alpha[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.linears[i](x.view(-1, self.embedding_dim * self.num_heads))\n",
    "            out = out + x * self.alpha[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, edge_index: Adj,\n",
    "                edge_label_index: OptTensor = None) -> Tensor:\n",
    "        r\"\"\"Computes rankings for pairs of nodes.\n",
    "        Args:\n",
    "            edge_index (torch.Tensor or SparseTensor): Edge tensor specifying\n",
    "                the connectivity of the graph.\n",
    "            edge_label_index (torch.Tensor, optional): Edge tensor specifying\n",
    "                the node pairs for which to compute rankings or probabilities.\n",
    "                If :obj:`edge_label_index` is set to :obj:`None`, all edges in\n",
    "                :obj:`edge_index` will be used instead. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        if edge_label_index is None:\n",
    "            if isinstance(edge_index, SparseTensor):\n",
    "                edge_label_index = torch.stack(edge_index.coo()[:2], dim=0)\n",
    "            else:\n",
    "                edge_label_index = edge_index\n",
    "\n",
    "        out = self.get_embedding(edge_index)\n",
    "\n",
    "        out_src = out[edge_label_index[0]]\n",
    "        out_dst = out[edge_label_index[1]]\n",
    "        return (out_src * out_dst).sum(dim=-1)\n",
    "\n",
    "    def predict_link(self, edge_index: Adj, edge_label_index: OptTensor = None,\n",
    "                     prob: bool = False) -> Tensor:\n",
    "        r\"\"\"Predict links between nodes specified in :obj:`edge_label_index`.\n",
    "        Args:\n",
    "            prob (bool, optional): Whether probabilities should be returned.\n",
    "                (default: :obj:`False`)\n",
    "        \"\"\"\n",
    "        pred = self(edge_index, edge_label_index).sigmoid()\n",
    "        return pred if prob else pred.round()\n",
    "\n",
    "    def recommend(self, edge_index: Adj, src_index: OptTensor = None,\n",
    "                  dst_index: OptTensor = None, k: int = 1) -> Tensor:\n",
    "        r\"\"\"Get top-:math:`k` recommendations for nodes in :obj:`src_index`.\n",
    "        Args:\n",
    "            src_index (torch.Tensor, optional): Node indices for which\n",
    "                recommendations should be generated.\n",
    "                If set to :obj:`None`, all nodes will be used.\n",
    "                (default: :obj:`None`)\n",
    "            dst_index (torch.Tensor, optional): Node indices which represent\n",
    "                the possible recommendation choices.\n",
    "                If set to :obj:`None`, all nodes will be used.\n",
    "                (default: :obj:`None`)\n",
    "            k (int, optional): Number of recommendations. (default: :obj:`1`)\n",
    "        \"\"\"\n",
    "        out_src = out_dst = self.get_embedding(edge_index)\n",
    "\n",
    "        if src_index is not None:\n",
    "            out_src = out_src[src_index]\n",
    "\n",
    "        if dst_index is not None:\n",
    "            out_dst = out_dst[dst_index]\n",
    "\n",
    "        pred = out_src @ out_dst.t()\n",
    "        top_index = pred.topk(k, dim=-1).indices\n",
    "\n",
    "        if dst_index is not None:  # Map local top-indices to original indices.\n",
    "            top_index = dst_index[top_index.view(-1)].view(*top_index.size())\n",
    "\n",
    "        return top_index\n",
    "\n",
    "    def link_pred_loss(self, pred: Tensor, edge_label: Tensor,\n",
    "                       **kwargs) -> Tensor:\n",
    "        r\"\"\"Computes the model loss for a link prediction objective via the\n",
    "        :class:`torch.nn.BCEWithLogitsLoss`.\n",
    "        Args:\n",
    "            pred (torch.Tensor): The predictions.\n",
    "            edge_label (torch.Tensor): The ground-truth edge labels.\n",
    "            **kwargs (optional): Additional arguments of the underlying\n",
    "                :class:`torch.nn.BCEWithLogitsLoss` loss function.\n",
    "        \"\"\"\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "    def recommendation_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
    "                            lambda_reg: float = 1e-4, **kwargs) -> Tensor:\n",
    "        r\"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "        Personalized Ranking (BPR) loss.\n",
    "        .. note::\n",
    "            The i-th entry in the :obj:`pos_edge_rank` vector and i-th entry\n",
    "            in the :obj:`neg_edge_rank` entry must correspond to ranks of\n",
    "            positive and negative edges of the same entity (*e.g.*, user).\n",
    "        Args:\n",
    "            pos_edge_rank (torch.Tensor): Positive edge rankings.\n",
    "            neg_edge_rank (torch.Tensor): Negative edge rankings.\n",
    "            lambda_reg (int, optional): The :math:`L_2` regularization strength\n",
    "                of the Bayesian Personalized Ranking (BPR) loss.\n",
    "                (default: :obj:`1e-4`)\n",
    "            **kwargs (optional): Additional arguments of the underlying\n",
    "                :class:`torch_geometric.nn.models.lightgcn.BPRLoss` loss\n",
    "                function.\n",
    "        \"\"\"\n",
    "        loss_fn = BPRLoss(lambda_reg, **kwargs)\n",
    "        return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.num_nodes}, '\n",
    "                f'{self.embedding_dim}, num_layers={self.num_layers})')\n",
    "\n",
    "\n",
    "class BPRLoss(_Loss):\n",
    "    r\"\"\"The Bayesian Personalized Ranking (BPR) loss.\n",
    "    The BPR loss is a pairwise loss that encourages the prediction of an\n",
    "    observed entry to be higher than its unobserved counterparts\n",
    "    (see `here <https://arxiv.org/abs/2002.02126>`__).\n",
    "    .. math::\n",
    "        L_{\\text{BPR}} = - \\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_u}\n",
    "        \\sum_{j \\not\\in \\mathcal{N}_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\n",
    "        + \\lambda \\vert\\vert \\textbf{x}^{(0)} \\vert\\vert^2\n",
    "    where :math:`lambda` controls the :math:`L_2` regularization strength.\n",
    "    We compute the mean BPR loss for simplicity.\n",
    "    Args:\n",
    "        lambda_reg (float, optional): The :math:`L_2` regularization strength\n",
    "            (default: 0).\n",
    "        **kwargs (optional): Additional arguments of the underlying\n",
    "            :class:`torch.nn.modules.loss._Loss` class.\n",
    "    \"\"\"\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        r\"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "        .. note::\n",
    "            The i-th entry in the :obj:`positives` vector and i-th entry\n",
    "            in the :obj:`negatives` entry should correspond to the same\n",
    "            entity (*.e.g*, user), as the BPR is a personalized ranking loss.\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).mean()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is verbatim from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/lightgcn.html. \"\"\"\n",
    "class BPRLoss(_Loss):\n",
    "    \"\"\"The Bayesian Personalized Ranking (BPR) loss.\"\"\"\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = 0\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        \"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).mean()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_hard_negative_edges_for_user(user_positive_edges, user_negative_edges, model, num_train_items, k):\n",
    "    device = 'cuda'\n",
    "    # Select hard negative edges based on current model parameters\n",
    "    user_positive_items = user_positive_edges[1, :]\n",
    "    # randomly select a positive edge\n",
    "    positive_edge = user_positive_edges[:, torch.randint(0, user_positive_edges.shape[1], (1,))]\n",
    "    # get the rankings for this user\n",
    "    all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous().to(device)\n",
    "    with torch.no_grad():\n",
    "        user_rankings = model.forward(all_edges) # this is of shape (42263) -- each index is the prediction for that index's movie\n",
    "    mask = torch.ones(num_train_items - num_train_users, dtype=torch.bool).to(device) # gets indices of all the movies\n",
    "    pos_items_mask = user_positive_items < num_train_items - num_train_users\n",
    "    filtered_pos_items = user_positive_items[pos_items_mask]\n",
    "    mask[filtered_pos_items] = True\n",
    "\n",
    "    # get the rankings for negative items\n",
    "    negative_rankings = user_rankings[mask]\n",
    "\n",
    "    _, topk_items = torch.topk(negative_rankings, k)\n",
    "    negative_items = torch.nonzero(mask).flatten()[topk_items]\n",
    "    # create the new negative edges\n",
    "    negative_edges_to_add = torch.tensor([[user_id] * k, negative_items], dtype=torch.long).to(device)\n",
    "    return positive_edge, negative_edges_to_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/313 [00:15<20:32,  3.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     50\u001b[0m first_negative_edges \u001b[39m=\u001b[39m user_negative_edges[:, torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, user_negative_edges\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], (\u001b[39m5\u001b[39m,))]\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> 51\u001b[0m positive_edge, negative_edges \u001b[39m=\u001b[39m resample_hard_negative_edges_for_user(user_positive_edges, user_negative_edges, model, num_train_items, epoch)\n\u001b[0;32m     52\u001b[0m negative_edges \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((negative_edges, first_negative_edges), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     53\u001b[0m positive_edge \u001b[39m=\u001b[39m positive_edge\u001b[39m.\u001b[39mcuda()\n",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m, in \u001b[0;36mresample_hard_negative_edges_for_user\u001b[1;34m(user_positive_edges, user_negative_edges, model, num_train_items, k)\u001b[0m\n\u001b[0;32m      6\u001b[0m positive_edge \u001b[39m=\u001b[39m user_positive_edges[:, torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, user_positive_edges\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], (\u001b[39m1\u001b[39m,))]\n\u001b[0;32m      7\u001b[0m \u001b[39m# get the rankings for this user\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m all_edges \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([(user_id, item_id) \u001b[39mfor\u001b[39;00m item_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_train_users, num_train_items)], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39mt()\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     10\u001b[0m     user_rankings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(all_edges) \u001b[39m# this is of shape (42263) -- each index is the prediction for that index's movie\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m positive_edge \u001b[39m=\u001b[39m user_positive_edges[:, torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, user_positive_edges\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], (\u001b[39m1\u001b[39m,))]\n\u001b[0;32m      7\u001b[0m \u001b[39m# get the rankings for this user\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m all_edges \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([(user_id, item_id) \u001b[39mfor\u001b[39;00m item_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_train_users, num_train_items)], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39mt()\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     10\u001b[0m     user_rankings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(all_edges) \u001b[39m# this is of shape (42263) -- each index is the prediction for that index's movie\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    import numpy as np\n",
    "    import math\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    NUM_LAYERS = 1\n",
    "    LR = 5e-4\n",
    "    BATCH_SIZE = 16\n",
    "    EMBEDDING_DIM = 32\n",
    "    LOAD_CHECKPOINT = False\n",
    "    K = 20\n",
    "    REG = 1e-3\n",
    "    model = CustomLightGCN(num_nodes=num_nodes, embedding_dim=EMBEDDING_DIM, num_layers=NUM_LAYERS, normalize=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    if LOAD_CHECKPOINT:\n",
    "        model.load_state_dict(torch.load(f'models/{EMBEDDING_DIM}_{NUM_LAYERS}_{1024}_{1e-3}_{num_train_users}_{143295}.pt', map_location=device))\n",
    "\n",
    "    print(\"Running on device: {}\".format(device))\n",
    "    print(EMBEDDING_DIM)\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95)\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[100, 200, 300, 400], gamma=0.5)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=100)\n",
    "\n",
    "    train_positive_edges = train_graph.edge_index[:, train_graph.edge_attr >= 3.5]\n",
    "    train_negative_edges = train_graph.edge_index[:, train_graph.edge_attr <= 2.5]\n",
    "\n",
    "    validation_df = pd.DataFrame.from_dict(validation_reviews)\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(comment=f'LightGCN_{EMBEDDING_DIM}_layers_{NUM_LAYERS}_batch_size_{BATCH_SIZE}_lr_{LR}_num_train_users_{num_train_users}_num_train_items_{num_train_items}_recall_{K}')\n",
    "\n",
    "    for epoch in range(10001):\n",
    "        # we are using BPR so we go by user\n",
    "        # We'll proceed in batches of users\n",
    "        for start_idx in tqdm(range(0, num_train_users, BATCH_SIZE)):\n",
    "            model.train()\n",
    "            all_positive_rankings = torch.tensor([]).cuda()\n",
    "            all_negative_rankings = torch.tensor([]).cuda()\n",
    "            # randomly select a batch of users\n",
    "            users_in_batch = torch.randperm(num_train_users)[:BATCH_SIZE]\n",
    "            # for each user randomly select a positive edge and 5 negative edges\n",
    "            # use torch to do this efficiently\n",
    "            for user_id in users_in_batch:\n",
    "                # get one random positive edge\n",
    "                user_positive_edges = train_positive_edges[:, train_positive_edges[0] == user_id]\n",
    "                user_negative_edges = train_negative_edges[:, train_negative_edges[0] == user_id]\n",
    "                if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
    "                    continue\n",
    "                first_negative_edges = user_negative_edges[:, torch.randint(0, user_negative_edges.shape[1], (5,))].cuda()\n",
    "                positive_edge, negative_edges = resample_hard_negative_edges_for_user(user_positive_edges, user_negative_edges, model, num_train_items, epoch)\n",
    "                negative_edges = torch.cat((negative_edges, first_negative_edges), dim=1)\n",
    "                positive_edge = positive_edge.cuda()\n",
    "                user_edges = torch.cat((positive_edge, negative_edges), dim=1)\n",
    "                # get the rankings of the positive and negative edges\n",
    "                user_rankings = model(user_edges.cuda())\n",
    "                del user_edges\n",
    "                torch.cuda.empty_cache()\n",
    "                # compute the loss\n",
    "                positive_rankings = user_rankings[0].unsqueeze(0).repeat(negative_edges.shape[1])\n",
    "                negative_rankings = user_rankings[1:]\n",
    "                all_positive_rankings = torch.cat((all_positive_rankings, positive_rankings))\n",
    "                all_negative_rankings = torch.cat((all_negative_rankings, negative_rankings))\n",
    "            # compute the loss\n",
    "            loss = model.recommendation_loss(all_positive_rankings, all_negative_rankings, REG)\n",
    "            del all_positive_rankings\n",
    "            del all_negative_rankings\n",
    "            torch.cuda.empty_cache()\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "            if (start_idx / BATCH_SIZE) % 100 == 0:\n",
    "                # evaluate the model\n",
    "                model.eval()\n",
    "                # iterate over all users in the validation set\n",
    "                validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "                # randomly select 1000 of the users\n",
    "                average_val_loss = 0\n",
    "                for i in range(3):\n",
    "                    validation_users = random.sample(validation_users, min(len(validation_users), 16))\n",
    "                    mean_ndcg = 0\n",
    "                    ndcg_scores = []\n",
    "                    validation_positive_edges = validation_graph.edge_index[:, validation_graph.edge_attr >= 3.5]\n",
    "                    validation_negative_edges = validation_graph.edge_index[:, validation_graph.edge_attr <= 2.5]\n",
    "                    val_positive_rankings = torch.tensor([]).cuda()\n",
    "                    val_negative_rankings = torch.tensor([]).cuda()\n",
    "                    for user in tqdm(validation_users):\n",
    "                        user_id = id_to_user[user]\n",
    "                        relevant_reviews = validation_df[validation_df['user_id'] == user_id]\n",
    "                        user_validation_edges = validation_edges[:, validation_edges[0] == user]\n",
    "                        user_validation_edges = user_validation_edges.to(device)\n",
    "                        user_rankings = model(user_validation_edges).cpu()\n",
    "                        user_validation_edges = user_validation_edges.cpu()\n",
    "                        edges_sorted = list(user_validation_edges[1, user_rankings.argsort(descending=True)])\n",
    "                        # use validation_df to get the relevances via the movie_id column and the movie_rating column\n",
    "                        relevances = []\n",
    "                        for edge in edges_sorted:\n",
    "                            movie_id = id_to_movie[int(edge)]\n",
    "                            if (movie_id in relevant_reviews['movie_id'].values):\n",
    "                                relevances.append(relevant_reviews[relevant_reviews['movie_id'] == movie_id]['movie_rating'].values[0])\n",
    "                            else:\n",
    "                                relevances.append(0)\n",
    "                        # calculate the ndcg\n",
    "                        if (len(relevances) >= K):\n",
    "                            ndcg = compute_ndcg_at_k(relevances, k=K)\n",
    "                        if (math.isnan(ndcg)):\n",
    "                            print(relevant_reviews)\n",
    "                            input()\n",
    "                        mean_ndcg += ndcg\n",
    "                        ndcg_scores.append(ndcg)\n",
    "                        user_positive_edges = validation_positive_edges[:, validation_positive_edges[0] == user]\n",
    "                        user_negative_edges = validation_negative_edges[:, validation_negative_edges[0] == user]\n",
    "                        if (user_positive_edges.shape[1] == 0 or user_negative_edges.shape[1] == 0):\n",
    "                            continue\n",
    "                        positive_edge = user_positive_edges[:, torch.randint(0, user_positive_edges.shape[1], (1,))]\n",
    "                        negative_edges = user_negative_edges[:, torch.randint(0, user_negative_edges.shape[1], (5,))]\n",
    "                        all_edges = torch.cat([positive_edge, negative_edges], dim=1)\n",
    "                        all_rankings = model(all_edges.cuda())\n",
    "                        del all_edges\n",
    "                        torch.cuda.empty_cache()\n",
    "                        positive_rankings = all_rankings[0].unsqueeze(0).repeat(5)\n",
    "                        negative_rankings = all_rankings[1:]\n",
    "                        val_positive_rankings = torch.cat([val_positive_rankings, positive_rankings])\n",
    "                        val_negative_rankings = torch.cat([val_negative_rankings, negative_rankings])\n",
    "                        # calculate the validation loss\n",
    "                    with torch.no_grad():\n",
    "                        val_loss = model.recommendation_loss(val_positive_rankings, val_negative_rankings, REG)\n",
    "                    del val_positive_rankings\n",
    "                    del val_negative_rankings\n",
    "                    torch.cuda.empty_cache()\n",
    "                    average_val_loss += val_loss\n",
    "                writer.add_scalar(\"Loss/val\", val_loss / 3, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "                mean_ndcg = mean_ndcg / len(validation_users)\n",
    "                print(\"Standard Deviation: {}\".format(np.std(ndcg_scores)))\n",
    "                # create a histogram of the ndcg scores, make bins for each 0.1\n",
    "                ndcg_scores = np.array(ndcg_scores).squeeze()\n",
    "                writer.add_histogram(\"hist_NDCG/val\", ndcg_scores, epoch)\n",
    "                # also make a histogram in matplotlib and save as png\n",
    "                plt.hist(ndcg_scores, bins=np.arange(0, 1.1, 0.1))\n",
    "                plt.suptitle(\"Validation NDCG Histogram\")\n",
    "                # write information about the model to the histogram\n",
    "                plt.title(f\"Model: LightGCN, Embedding Dim: {EMBEDDING_DIM}, Num Layers: {NUM_LAYERS}, Batch Size: {BATCH_SIZE}, LR: {LR}, Num Train Users: {num_train_users}, Num Train Items: {num_train_items}\", fontsize=8, wrap=True)\n",
    "                plt.xlabel(\"NDCG\")\n",
    "                plt.ylabel(\"Frequency\")\n",
    "                # save the figure in the hist_NDCG folder, with the title having the model information and the epoch number\n",
    "                plt.savefig(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.png\")\n",
    "                plt.close()\n",
    "                # Also save the raw NDCG scores to a csv file, with the model information in the title, and the epoch number\n",
    "                np.savetxt(f\"hist_NDCG/val_{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}_{epoch}.csv\", ndcg_scores, delimiter=\",\")\n",
    "                print(mean_ndcg)\n",
    "                writer.add_scalar(\"NDCG\", mean_ndcg.item(), epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "                recall_at_k = compute_recall_at_k(validation_graph, model, K)\n",
    "                print(recall_at_k)\n",
    "                writer.add_scalar(\"Recall@K/val\", recall_at_k, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "                print(\"Epoch: {}, NDCG: {}, Recall@{}: {}\".format(epoch, mean_ndcg, K, recall_at_k))\n",
    "                average_number_of_matches = 0\n",
    "                for user_id in validation_users:\n",
    "                    all_edges = torch.tensor([(user_id, item_id) for item_id in range(num_train_users, num_train_items)], dtype=torch.long).t().contiguous()\n",
    "                    dst_index = torch.tensor([x for x in range(num_train_users + 1, num_train_items)]).to(device)\n",
    "                    recommendations = model.recommend(all_edges.to(device), src_index=torch.tensor([user_id]).to(device), dst_index=dst_index, k=10)[0].cpu()\n",
    "                    del all_edges\n",
    "                    del dst_index\n",
    "                    torch.cuda.empty_cache()\n",
    "                    movie_names = [movie_id_to_movie_name[id_to_movie[int(recommendation)]] for recommendation in recommendations]\n",
    "                    true_user_reviews = user_review_data[id_to_user[user_id]]\n",
    "                    matches = 0\n",
    "                    for movie_name in movie_names:\n",
    "                        if movie_name in true_user_reviews['movie_title'].values:\n",
    "                            matches += 1\n",
    "                    average_number_of_matches += matches\n",
    "                average_number_of_matches = average_number_of_matches / len(validation_users)\n",
    "                print(\"Average number of matches: {}\".format(average_number_of_matches))\n",
    "                writer.add_scalar(\"Average number of matches\", average_number_of_matches, epoch * (num_train_users // BATCH_SIZE) + start_idx // BATCH_SIZE)\n",
    "                print(\"=====================================\")\n",
    "        \n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), f\"models/{EMBEDDING_DIM}_{NUM_LAYERS}_{BATCH_SIZE}_{LR}_{num_train_users}_{num_train_items}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00475\n"
     ]
    }
   ],
   "source": [
    "for param_group in optim.param_groups:\n",
    "    print(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_users = list(set([int(x) for x in validation_edges[0, :]]))\n",
    "validation_df[validation_df.user_id == id_to_user[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_edges[:, validation_edges[0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
